{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from brat_parser import get_entities_relations_attributes_groups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from text_mining_func import normalize\n",
    "\n",
    "# entities, relations, attributes, groups = get_entities_relations_attributes_groups(\"/Users/aelitta/Downloads/export/clinical/main/0/2.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "txt = []\n",
    "tst = []\n",
    "apply = []\n",
    "n=0\n",
    "with open(\"Documents/Myeloma/mm_v3.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        word = line[2:3]\n",
    "        print(\"word\", word)\n",
    "        ann = line[4:5]\n",
    "        print(\"ann\", ann)\n",
    "        ann2 = line[8:9]\n",
    "        print(\"ann2\", ann2)\n",
    "#        print(line)\n",
    "        if len(word) == 0 or len(ann) == 0:\n",
    "            continue\n",
    "        if word[0].lower() == 'эпикриз':\n",
    "            n+=1\n",
    "        if n<5:\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                txt.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0])>0:\n",
    "                txt.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            else:\n",
    "                continue\n",
    "        if n==4:\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                tst.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0])>0:\n",
    "                tst.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            apply.append(word[0])\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "txt = []\n",
    "tst = []\n",
    "apply = []\n",
    "txt_groups = []\n",
    "tst_groups = []\n",
    "txt_subgroups = []\n",
    "tst_subgroups = []\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "with open(\"Documents/Myeloma/mm_v3.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        word = line[2:3]\n",
    "        #print(\"word\", word)\n",
    "        ann = line[4:5]\n",
    "        #print(\"ann\", ann)\n",
    "        ann2 = line[8:9]\n",
    "        #print(\"ann2\", ann2)\n",
    "#        print(line)\n",
    "        if len(word) == 0 or len(ann) == 0 or word[0][:1] == '\\t' or len(word[0]+ ' '+'O')==2 :\n",
    "            continue\n",
    "        if word[0].lower() == 'эпикриз':\n",
    "            n+=1\n",
    "            ng=0\n",
    "            nsg=0\n",
    "        if len(word[0]+ ' '+'O')==2:\n",
    "            print(2, word[0])\n",
    "        if n<6:\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                txt.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0]):\n",
    "                txt.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            if ann2[0].split('[')[0] == 'group':\n",
    "                if ng==0:\n",
    "                    txt_groups.append(word[0] + ' ' + 'B-group')\n",
    "                    ng+=1\n",
    "                else:\n",
    "                    txt_groups.append(word[0] + ' ' + 'I-group')\n",
    "            else:\n",
    "                txt_groups.append(word[0] + ' ' + 'O')\n",
    "                ng=0\n",
    "            if len(ann2[0].split('|')) == 2:\n",
    "                    if ann2[0].split('|')[1].split('[')[0] == 'subgroup':\n",
    "                        if nsg==0:\n",
    "                            txt_subgroups.append(word[0] + ' ' + 'B-subgroup')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            txt_subgroups.append(word[0] + ' ' + 'I-subgroup')\n",
    "            else:\n",
    "                txt_subgroups.append(word[0] + ' ' + 'O')\n",
    "                nsg=0\n",
    "            \n",
    "        if n==6:\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                tst.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0])>0:\n",
    "                tst.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            if ann2[0].split('[')[0] == 'group' and len(word[0])>0:\n",
    "                if ng==0:\n",
    "                    tst_groups.append(word[0] + ' ' + 'B-group')\n",
    "                    ng+=1\n",
    "                else:\n",
    "                    tst_groups.append(word[0] + ' ' + 'I-group')\n",
    "            else:\n",
    "                tst_groups.append(word[0] + ' ' + 'O')\n",
    "            if len(ann2[0].split('|')) == 2 and len(word[0])>0:\n",
    "                    if ann2[0].split('|')[1].split('[')[0] == 'subgroup':\n",
    "                        if nsg==0:\n",
    "                            tst_subgroups.append(word[0] + ' ' + 'B-subgroup')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            tst_subgroups.append(word[0] + ' ' + 'I-subgroup')\n",
    "            elif len(word[0])>0:\n",
    "                    tst_subgroups.append(word[0] + ' ' + 'O')\n",
    "        else:\n",
    "            apply.append(word[0])\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups_oak, and subgroup1 (with date), subgroup2 (no date)\n",
    "\n",
    "import csv\n",
    "txt = []\n",
    "tst = []\n",
    "apply = []\n",
    "txt_groups = []\n",
    "tst_groups = []\n",
    "txt_subgroups = []\n",
    "tst_subgroups = []\n",
    "txt_subgroups1 = []\n",
    "tst_subgroups1 = []\n",
    "set_of_ann = []\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "with open(\"Documents/Myeloma/mm_kazan.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        word = line[2:3]\n",
    "        #print(\"word\", word)\n",
    "        ann = line[4:5]\n",
    "        #print(\"ann\", ann)\n",
    "        ann2 = line[8:9]\n",
    "        #print(\"ann2\", ann2)\n",
    "#        print(line)\n",
    "        if len(word) == 0 or len(ann) == 0 or word[0][:1] == '\\t' or len(word[0]+ ' '+'O')==2 :\n",
    "            continue\n",
    "        if word[0].lower() == 'эпикриз':\n",
    "            n+=1\n",
    "            ng=0\n",
    "            nsg=0\n",
    "        if len(word[0]+ ' '+'O')==2:\n",
    "            print(2, word[0])\n",
    "        if n<6:\n",
    "            set_of_ann.append(ann2[0])\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                txt.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0]):\n",
    "                txt.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            if ann2[0].split('[')[0] == 'group\\_oak':\n",
    "                if ng==0:\n",
    "                    txt_groups.append(word[0] + ' ' + 'B-group_oak')\n",
    "                    ng+=1\n",
    "                else:\n",
    "                    txt_groups.append(word[0] + ' ' + 'I-group_oak')\n",
    "            else:\n",
    "                txt_groups.append(word[0] + ' ' + 'O')\n",
    "                ng=0\n",
    "            if len(ann2[0].split('|')) == 2:\n",
    "                    if ann2[0].split('|')[0].split('[')[0] == 'subgroup1':\n",
    "                        if nsg==0:\n",
    "                            txt_subgroups1.append(word[0] + ' ' + 'B-subgroup1')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            txt_subgroups1.append(word[0] + ' ' + 'I-subgroup1')\n",
    "                    if ann2[0].split('|')[1].split('[')[0] == 'subgroup2':\n",
    "                        if nsg==0:\n",
    "                            txt_subgroups.append(word[0] + ' ' + 'B-subgroup2')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            txt_subgroups.append(word[0] + ' ' + 'I-subgroup2')\n",
    "            else:\n",
    "                txt_subgroups.append(word[0] + ' ' + 'O')\n",
    "                nsg=0\n",
    "            \n",
    "        if n==6:\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                tst.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0])>0:\n",
    "                tst.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            if ann2[0].split('[')[0] == 'group\\_oak' and len(word[0])>0:\n",
    "                if ng==0:\n",
    "                    tst_groups.append(word[0] + ' ' + 'B-group_oak')\n",
    "                    ng+=1\n",
    "                else:\n",
    "                    tst_groups.append(word[0] + ' ' + 'I-group_oak')\n",
    "            else:\n",
    "                tst_groups.append(word[0] + ' ' + 'O')\n",
    "            if len(ann2[0].split('|')) == 2 and len(word[0])>0:\n",
    "                    if ann2[0].split('|')[0].split('[')[0] == 'subgroup1':\n",
    "                        if nsg==0:\n",
    "                            tst_subgroups1.append(word[0] + ' ' + 'B-subgroup1')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            tst_subgroups1.append(word[0] + ' ' + 'I-subgroup1')\n",
    "                    if ann2[0].split('|')[1].split('[')[0] == 'subgroup2':\n",
    "                        if nsg==0:\n",
    "                            tst_subgroups.append(word[0] + ' ' + 'B-subgroup2')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            tst_subgroups.append(word[0] + ' ' + 'I-subgroup2')\n",
    "            elif len(word[0])>0:\n",
    "                    tst_subgroups.append(word[0] + ' ' + 'O')\n",
    "                    txt_subgroups1.append(word[0] + ' ' + 'O')\n",
    "        else:\n",
    "            apply.append(word[0])\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "with open(\"Documents/Cancer/РМЖ/Разметка/02.04.1964__12_023641.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        word = line[2:3]\n",
    "        ann = line[4:7]\n",
    "        #print(\"ann\", ann)\n",
    "        ann2 = line[8:9]\n",
    "        #print(\"ann2\", ann2)\n",
    "        print(\"word, ann\", word, ann, ann2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "with open(\"Documents/Myeloma/m1t.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        word = line[2:3]\n",
    "        #print(\"word\", word)\n",
    "        ann = line[4:5]\n",
    "        #print(\"ann\", ann)\n",
    "        ann2 = line[8:9]\n",
    "        #print(\"ann2\", ann2)\n",
    "#        print(line)\n",
    "        if len(word) == 0 or len(ann) == 0 or word[0][:1] == '\\t' or len(word[0]+ ' '+'O')==2 :\n",
    "            continue\n",
    "        if word[0].lower() == 'эпикриз':\n",
    "            n+=1\n",
    "            ng=0\n",
    "            nsg=0\n",
    "        if len(word[0]+ ' '+'O')==2:\n",
    "            print(2, word[0])\n",
    "        if n<6:\n",
    "            set_of_ann.append(ann2[0])\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                txt.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0]):\n",
    "                txt.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            if ann2[0].split('[')[0] == 'group\\_oak':\n",
    "                if ng==0:\n",
    "                    txt_groups.append(word[0] + ' ' + 'B-group_oak')\n",
    "                    ng+=1\n",
    "                else:\n",
    "                    txt_groups.append(word[0] + ' ' + 'I-group_oak')\n",
    "            if ann2[0].split('[')[0] == 'group\\_bak':\n",
    "                if ng==0:\n",
    "                    txt_groups.append(word[0] + ' ' + 'B-group_bak')\n",
    "                    ng+=1\n",
    "                else:\n",
    "                    txt_groups.append(word[0] + ' ' + 'I-group_bak')\n",
    "            else:\n",
    "                txt_groups.append(word[0] + ' ' + 'O')\n",
    "                ng=0\n",
    "            if len(ann2[0].split('|')) == 2:\n",
    "                    if ann2[0].split('|')[0].split('[')[0] == 'subgroup1':\n",
    "                        if nsg==0:\n",
    "                            txt_subgroups1.append(word[0] + ' ' + 'B-subgroup1')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            txt_subgroups1.append(word[0] + ' ' + 'I-subgroup1')\n",
    "                    if ann2[0].split('|')[1].split('[')[0] == 'subgroup2':\n",
    "                        if nsg==0:\n",
    "                            txt_subgroups.append(word[0] + ' ' + 'B-subgroup2')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            txt_subgroups.append(word[0] + ' ' + 'I-subgroup2')\n",
    "            else:\n",
    "                txt_subgroups.append(word[0] + ' ' + 'O')\n",
    "                txt_subgroups1.append(word[0] + ' ' + 'O')\n",
    "                nsg=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(set_of_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(txt_subgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/train.txt','w')\n",
    "cnt = 0\n",
    "for ele in txt:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/groups/train.txt','w')\n",
    "cnt = 0\n",
    "for ele in txt_groups:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/subgroups/train.txt','w')\n",
    "cnt = 0\n",
    "for ele in txt_subgroups:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/subgroups1/train.txt','w')\n",
    "cnt = 0\n",
    "for ele in txt_subgroups1:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/test.txt','w')\n",
    "cnt = 0\n",
    "for ele in tst:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/groups/test.txt','w')\n",
    "cnt = 0\n",
    "for ele in tst_groups:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/subgroups/test.txt','w')\n",
    "cnt = 0\n",
    "for ele in tst_subgroups:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/subgroups1/test.txt','w')\n",
    "cnt = 0\n",
    "for ele in tst_subgroups1:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/valid.txt','w')\n",
    "cnt = 0\n",
    "for ele in txt:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/groups/valid.txt','w')\n",
    "cnt = 0\n",
    "for ele in txt_groups:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/subgroups/valid.txt','w')\n",
    "cnt = 0\n",
    "for ele in txt_subgroups:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Documents/Myeloma/ner_few_shot_data/subgroups1/valid.txt','w')\n",
    "cnt = 0\n",
    "for ele in txt_subgroups1:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/Users/aelitta/Downloads/export/clinical/main/0/2.txt', 'r')\n",
    "txt = file.read()\n",
    "file.close()\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^а-яА-Я0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = []\n",
    "target_tokens = []\n",
    "for entity in entities:\n",
    "    if entities[entity].type == 'Disease':\n",
    "        spans.append(entities[entity].span[0])\n",
    "        target_tokens.append(entities[entity].text.lower())\n",
    "\n",
    "simple = []\n",
    "target = []\n",
    "\n",
    "for word in txt.split():\n",
    "    match = 0\n",
    "    for span in spans:\n",
    "        if txt.find(word) >= span[0] and txt.find(word) <= span[1]:\n",
    "            match+=1\n",
    "    if match == 0 and word!='':\n",
    "        simple.append(word)\n",
    "    if match >0 and word!='':\n",
    "        target.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [token for token in normalize(target) if token !='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontarget = [token for token in normalize(simple) if token !='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_ngrams(txt, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_revised = []\n",
    "\n",
    "txt_changed = re.sub(r'[^а-яА-Я0-9\\s]', ' ', txt)\n",
    "\n",
    "pos0 = 0\n",
    "for word in re.sub(r'[^а-яА-Я0-9\\s]', ' ', txt).split():\n",
    "    pos = txt_changed.find(word)+pos0\n",
    "    match = 0\n",
    "    for entity in entities:\n",
    "        if pos >= entities[entity].span[0][0] and pos <= entities[entity].span[0][1]:\n",
    "            if pos == entities[entity].span[0][0]:\n",
    "                text_revised.append(word + ' ' + 'B-' + entities[entity].type)\n",
    "                match+=1\n",
    "            else:\n",
    "                text_revised.append(word + ' ' + 'I-' + entities[entity].type)\n",
    "                match+=1\n",
    "    if match == 0:\n",
    "        text_revised.append(word + ' ' + 'O')\n",
    "    txt_changed = txt[pos+len(word)+1:]\n",
    "    pos0 = pos+len(word)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('MeSH/f2.txt','w')\n",
    "for ele in text_revised:\n",
    "    f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_revised = []\n",
    "n = 0\n",
    "for k in range(5):\n",
    "    k = n*10+k+3\n",
    "    entities, relations, attributes, groups = get_entities_relations_attributes_groups(\"/Users/aelitta/Downloads/export/clinical/test/\"+str(k)+\".ann\")\n",
    "    file = open('/Users/aelitta/Downloads/export/clinical/test/'+str(k)+'.txt', 'r')\n",
    "    txt = file.read()\n",
    "    file.close()\n",
    "#    text_revised = []\n",
    "\n",
    "    txt_changed = re.sub(r'[^а-яА-Я0-9\\s]', ' ', txt)\n",
    "\n",
    "    pos0 = 0\n",
    "    cnt = 0\n",
    "    for word in re.sub(r'[^а-яА-Я0-9\\s]', ' ', txt).split():\n",
    "        pos = txt_changed.find(word)+pos0\n",
    "        match = 0\n",
    "        for entity in entities:\n",
    "            if pos >= entities[entity].span[0][0] and pos <= entities[entity].span[0][1]:\n",
    "                if pos == entities[entity].span[0][0]:\n",
    "                    text_revised.append(word + ' ' + 'B-' + entities[entity].type)\n",
    "                    match+=1\n",
    "                else:\n",
    "                    text_revised.append(word + ' ' + 'I-' + entities[entity].type)\n",
    "                    match+=1\n",
    "        if match == 0:\n",
    "            text_revised.append(word + ' ' + 'O')\n",
    "        txt_changed = txt[pos+len(word)+1:]\n",
    "        pos0 = pos+len(word)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('MeSH/test.txt','w')\n",
    "cnt = 0\n",
    "for ele in text_revised:\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt+=1\n",
    "        t = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('MeSH/train.txt', 'r')\n",
    "txt = file.read()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(txt.split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разметка статей BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#предобработка для поиска таблиц\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "\n",
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "prev = ''\n",
    "\n",
    "files_stat = {}\n",
    "\n",
    "for file in listfiles:\n",
    "    sents = {}\n",
    "    anns = {}\n",
    "    ref = {}\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv' and file[:3]!='PMC':\n",
    "\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            snmax = []\n",
    "            for line in tsvreader:\n",
    "                if len(line)>5:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if sn not in snmax:\n",
    "                            snmax.append(sn)\n",
    "                    except:\n",
    "                        continue\n",
    "            #собираем предложения и на уровне их ищем таблицы\n",
    "            sntable = []\n",
    "#             print(snmax)   \n",
    "            for sn in snmax:\n",
    "                s = []  \n",
    "                cnt_ann = 0\n",
    "                cnt_refs = 0\n",
    "                with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "                    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                    for line in tsvreader:\n",
    "                        if len(line)>0 and line[0].split('-')[0]==sn:\n",
    "                            s.append(line[2])\n",
    "                            if line[6] !='_':\n",
    "                                cnt_ann+=1\n",
    "                            if line[2].lower().find('references')>=0:\n",
    "                                cnt_refs+=1\n",
    "    #                         if sn=='80':\n",
    "    #                             print(s)\n",
    "                        else:\n",
    "                            s.append('')\n",
    "                            cnt_ann=cnt_ann\n",
    "                            cnt_refs = cnt_refs\n",
    "                        sents[sn]=' '.join(s)\n",
    "                        anns[sn] = cnt_ann\n",
    "                        ref[sn] = cnt_refs\n",
    "                    cnt=0\n",
    "                    for t in s:\n",
    "                        if t.find('(')>=0 or t.find(')')>=0:\n",
    "                            cnt+=1\n",
    "                        if cnt/(len(s)+1)>0.15:\n",
    "                            if sn not in sntable:\n",
    "                                sntable.append(sn)\n",
    "                    files_stat[file] = [sents, anns, ref, snmax, sntable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_stat['eichhorst2016.tsv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предобработка для поиска таблиц по новой разметке (PMC)\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "\n",
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/new_articles\")\n",
    "prev = ''\n",
    "\n",
    "# files_stat = {}\n",
    "\n",
    "for file in listfiles:\n",
    "    sents = {}\n",
    "    anns = {}\n",
    "    ref = {}\n",
    "    \n",
    "    if file[-3:]=='tsv' and file[:3]=='PMC' or file=='16611308.tsv' or file=='1503098.tsv' \\\n",
    "        and file not in ['PMC4722809.tsv','PMC4643417.tsv','PMC4552311.tsv','PMC4535917.tsv']:\n",
    "        print(file)\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/new_articles/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            snmax = []\n",
    "            for line in tsvreader:\n",
    "                if len(line)>=5:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if sn not in snmax:\n",
    "                            snmax.append(sn)\n",
    "                    except:\n",
    "                        continue\n",
    "            #собираем предложения и на уровне их ищем таблицы\n",
    "            sntable = []\n",
    "#             print(snmax)   \n",
    "            for sn in snmax:\n",
    "                s = []  \n",
    "                cnt_ann = 0\n",
    "                cnt_refs = 0\n",
    "                with open(\"Documents/Cancer/CLL/Разметка/new_articles/\"+file) as tsvfile:\n",
    "                    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                    for line in tsvreader:\n",
    "                        if len(line)>=5 and line[0].split('-')[0]==sn:\n",
    "                            s.append(line[2])\n",
    "                            if line[4] !='_':\n",
    "                                cnt_ann+=1\n",
    "                            if line[2].lower().find('references')>=0:\n",
    "                                cnt_refs+=1\n",
    "    #                         if sn=='80':\n",
    "    #                             print(s)\n",
    "                        else:\n",
    "                            s.append('')\n",
    "                            cnt_ann=cnt_ann\n",
    "                            cnt_refs = cnt_refs\n",
    "                        sents[sn]=' '.join(s)\n",
    "                        anns[sn] = cnt_ann\n",
    "                        ref[sn] = cnt_refs\n",
    "                    cnt=0\n",
    "                    for t in s:\n",
    "                        if t.find('(')>=0 or t.find(')')>=0:\n",
    "                            cnt+=1\n",
    "                        if cnt/(len(s)+1)>0.15:\n",
    "                            if sn not in sntable:\n",
    "                                sntable.append(sn)\n",
    "                    files_stat[file] = [sents, anns, ref, snmax, sntable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_stat['PMC2861142.tsv'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sn in snmax:\n",
    "    if ref[sn]>0:\n",
    "        print(sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sn in sntable:\n",
    "    if anns[sn]==0:\n",
    "        print(sents[sn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "\n",
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.lower().find(sub.lower(), start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub)\n",
    "\n",
    "def extract_numbers(str_obsledS):        \n",
    "    nums = re.findall('\\d+\\W\\d+', str_obsledS)+ re.findall('\\d+\\W\\d+\\W\\d+', str_obsledS) +       re.findall('\\d{1,2}', str_obsledS) + re.findall('\\d{1,2}\\W\\d+\\W\\d+', str_obsledS)+       re.findall('\\d+\\W\\d+\\W\\d+\\W\\d+', str_obsledS)+re.findall('\\d+\\W\\d+\\w\\d+\\W\\d+', str_obsledS) +       re.findall('\\d+\\w\\d+', str_obsledS)+re.findall('\\d+\\W\\d+\\w\\d+', str_obsledS) +       re.findall('\\d+\\W+\\d+\\W\\d+', str_obsledS) + re.findall('\\d+\\W\\d+\\W+\\d+', str_obsledS) +       re.findall('\\d+\\w\\d+\\w\\d+', str_obsledS)\n",
    "    num_long = []\n",
    "    str_obsledS_new = str_obsledS\n",
    "    if len(nums)>0:\n",
    "        max_length = max([len(x) for x in nums])\n",
    "        for num in nums:\n",
    "            if len(num)==max_length:\n",
    "                num_long.append(num)\n",
    "    if len(str_obsledS_new)<len(str_obsledS):\n",
    "        num_long = extract_numbers(str_obsledS_new)\n",
    "    return np.unique(num_long) #max(nums, key=len)\n",
    "\n",
    "# def add_string(filename, bio_annotation):\n",
    "#     ns = 4\n",
    "#     with open(\"Documents/Cancer/CLL/Разметка/\"+filename) as tsvfile:\n",
    "#         tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "#         prev = ''\n",
    "#         for line in tsvreader:\n",
    "# #             print(line)\n",
    "#             if len(line)<5:\n",
    "#                 continue\n",
    "#             if line[ns] !='_' and len(line[2])>0:\n",
    "#                 if prev == '':\n",
    "#                     num_long = extract_numbers(line[2])\n",
    "#                     if len(num_long)==0:\n",
    "#                          bio_annotation.append(line[2] + '\\t' + 'B-' + line[ns].split('[')[0])\n",
    "#                     else:\n",
    "#                          bio_annotation.append(line[2].replace(str(num_long[0]), '[') + '\\t' + 'B-'+ line[ns].split('[')[0])\n",
    "#                          bio_annotation.append('num' + '\\t' + 'I-'+ line[ns].split('[')[0])\n",
    "#                          bio_annotation.append(']' + '\\t' + 'I-'+ line[ns].split('[')[0])\n",
    "#                     prev = line[ns].split('[')[0]\n",
    "#                 elif prev == line[ns].split('[')[0]:\n",
    "#                     num_long = extract_numbers(line[2])\n",
    "#                     if len(num_long)==0:\n",
    "#                          bio_annotation.append(line[2] + '\\t' + 'I-'+ line[ns].split('[')[0])\n",
    "#                     else:\n",
    "#                          bio_annotation.append(line[2].replace(str(num_long[0]), '[') + '\\t' + 'I-'+ line[ns].split('[')[0])\n",
    "#                          bio_annotation.append('num' + '\\t' + 'I-'+ line[ns].split('[')[0])\n",
    "#                          bio_annotation.append(']' + '\\t' + 'I-'+ line[ns].split('[')[0])\n",
    "#                     prev = line[ns].split('[')[0]\n",
    "#             else:\n",
    "#                 if len(line[2].strip())>0:\n",
    "                    \n",
    "#                     try:\n",
    "#                         sn = line[0].split('-')[0]\n",
    "#                         num_long = extract_numbers(line[2])\n",
    "#                         if anns[sn]>0: #or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "# #                             print(sn)\n",
    "#                             if len(num_long)==0:\n",
    "#                                  bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "#                             else:\n",
    "#                                  bio_annotation.append(line[2].replace(str(num_long[0]), '[') + '\\t' + 'O')\n",
    "#                                  bio_annotation.append('num' + '\\t' + 'O')\n",
    "#                                  bio_annotation.append(']' + '\\t' + 'O')\n",
    "#                             prev = ''\n",
    "#                     except:\n",
    "#                         continue   \n",
    "#     return bio_annotation\n",
    "\n",
    "def add_string(filename, bio_annotation):\n",
    "    ns = 4\n",
    "    with open(\"Documents/Cancer/CLL/Разметка/\"+filename) as tsvfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        prev = ''\n",
    "        for line in tsvreader:\n",
    "#             print(line)\n",
    "            if len(line)<5:\n",
    "                continue\n",
    "            if line[ns] !='_' and len(line[2])>0:\n",
    "                if prev == '':\n",
    "                    bio_annotation.append(line[2] + '\\t' + 'B-' + line[ns].split('[')[0])\n",
    "                    prev = line[ns].split('[')[0]\n",
    "                elif prev == line[ns].split('[')[0]:\n",
    "                    bio_annotation.append(line[2] + '\\t' + 'I-'+ line[ns].split('[')[0])\n",
    "                    prev = line[ns].split('[')[0]\n",
    "            else:\n",
    "                if len(line[2].strip())>0:                    \n",
    "                    try:\n",
    "#                         sn = line[0].split('-')[0]\n",
    "#                         num_long = extract_numbers(line[2])\n",
    "#                         if anns[sn]>0: #or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "#                             print(sn)\n",
    "                        bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "                        prev = ''\n",
    "                    except:\n",
    "                        continue   \n",
    "    return bio_annotation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "listfiles = [file for file in listfiles if file[:3]=='PMC']\n",
    "prev = ''\n",
    "filestat = files_stat\n",
    "\n",
    "for file in listfiles[0:5]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        anns = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        bio_annotation = add_string(file, bio_annotation)        \n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/valid_new.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[5:7]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        anns = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        bio_annotation = add_string(file, bio_annotation)           \n",
    "\n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/test_new.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[7:]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        anns = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        bio_annotation = add_string(file, bio_annotation)\n",
    "\n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/train_new.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "\n",
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "prev = ''\n",
    "filestat = files_stat\n",
    "\n",
    "for file in listfiles[0:5]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            counter = 0\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + '\\t'+ str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) + '\\t' + 'B-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + '\\t' + str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) +'\\t' + 'I-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                elif len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            pos = counter+len(line[2].strip())\n",
    "                            bio_annotation.append(line[2].strip() +'\\t' + str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) + '\\t' + 'O')\n",
    "                            counter = counter+len(line[2].strip())+1\n",
    "                            prev = ''\n",
    "                            if line[2].strip()=='.':\n",
    "                                counter = 0\n",
    "                    except:\n",
    "                        continue           \n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/conll/valid.txt','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[5:7]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            counter = 0\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + '\\t'+ str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) + '\\t' + 'B-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + '\\t' + str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) +'\\t' + 'I-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                elif len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            pos = counter+len(line[2].strip())\n",
    "                            bio_annotation.append(line[2].strip() +'\\t' + str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) + '\\t' + 'O')\n",
    "                            counter = counter+len(line[2].strip())+1\n",
    "                            prev = ''\n",
    "                            if line[2].strip()=='.':\n",
    "                                counter = 0\n",
    "                    except:\n",
    "                        continue            \n",
    "\n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/conll/test.txt','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[7:]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            counter = 0\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + '\\t'+ str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) + '\\t' + 'B-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + '\\t' + str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) +'\\t' + 'I-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                elif len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            pos = counter+len(line[2].strip())\n",
    "                            bio_annotation.append(line[2].strip() +'\\t' + str(counter) + '\\t'+ str(pos) +'\\t' + str(counter+1) + '\\t'+ str(pos+1) + '\\t' + 'O')\n",
    "                            counter = counter+len(line[2].strip())+1\n",
    "                            prev = ''\n",
    "                            if line[2].strip()=='.':\n",
    "                                counter = 0\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/conll/train.txt','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "\n",
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "prev = ''\n",
    "filestat = files_stat\n",
    "\n",
    "for file in listfiles[0:5]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            counter = 0\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + ' '+ 'NN' + ' '+ 'B-NP'  + ' ' + 'B-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + ' ' + 'NN' + ' '+ 'B-NP' +' ' + 'I-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                elif len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            pos = counter+len(line[2].strip())\n",
    "                            bio_annotation.append(line[2].strip() +' ' + 'NN' + ' '+ 'B-NP' + ' ' + 'O')\n",
    "                            counter = counter+len(line[2].strip())+1\n",
    "                            prev = ''\n",
    "                            if line[2].strip()=='.':\n",
    "                                counter = 0\n",
    "                    except:\n",
    "                        continue           \n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/conll/valid.txt','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[5:7]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            counter = 0\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + ' '+ 'NN' + ' '+ 'B-NP'  + ' ' + 'B-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + ' ' + 'NN' + ' '+ 'B-NP' +' ' + 'I-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                elif len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            pos = counter+len(line[2].strip())\n",
    "                            bio_annotation.append(line[2].strip() +' ' + 'NN' + ' '+ 'B-NP' + ' ' + 'O')\n",
    "                            counter = counter+len(line[2].strip())+1\n",
    "                            prev = ''\n",
    "                            if line[2].strip()=='.':\n",
    "                                counter = 0\n",
    "                    except:\n",
    "                        continue              \n",
    "\n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/conll/test.txt','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[7:]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            counter = 0\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + ' '+ 'NN' + ' '+ 'B-NP'  + ' ' + 'B-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        pos = counter+len(line[2].strip())\n",
    "                        bio_annotation.append(line[2].strip() + ' ' + 'NN' + ' '+ 'B-NP' +' ' + 'I-'+ line[6].split('[')[0])\n",
    "                        prev = line[6].split('[')[0]\n",
    "                        counter = counter+len(line[2].strip())+1\n",
    "                        if line[2].strip()=='.':\n",
    "                            counter = 0\n",
    "                elif len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            pos = counter+len(line[2].strip())\n",
    "                            bio_annotation.append(line[2].strip() +'\\t' + 'NN' + ' '+ 'B-NP' + ' ' + 'O')\n",
    "                            counter = counter+len(line[2].strip())+1\n",
    "                            prev = ''\n",
    "                            if line[2].strip()=='.':\n",
    "                                counter = 0\n",
    "                    except:\n",
    "                        continue  \n",
    "\n",
    "\n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/NER/conll/train.txt','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_stat['PMC3134602.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filestat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.lower().find(sub.lower(), start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub)\n",
    "\n",
    "def extract_numbers(str_obsledS):        \n",
    "    nums = re.findall('\\d+\\W\\d+', str_obsledS)+ re.findall('\\d+\\W\\d+\\W\\d+', str_obsledS) +       re.findall('\\d{1,2}', str_obsledS) + re.findall('\\d{1,2}\\W\\d+\\W\\d+', str_obsledS)+       re.findall('\\d+\\W\\d+\\W\\d+\\W\\d+', str_obsledS)+re.findall('\\d+\\W\\d+\\w\\d+\\W\\d+', str_obsledS) +       re.findall('\\d+\\w\\d+', str_obsledS)+re.findall('\\d+\\W\\d+\\w\\d+', str_obsledS) +       re.findall('\\d+\\W+\\d+\\W\\d+', str_obsledS) + re.findall('\\d+\\W\\d+\\W+\\d+', str_obsledS) +       re.findall('\\d+\\w\\d+\\w\\d+', str_obsledS)\n",
    "    num_long = []\n",
    "    str_obsledS_new = str_obsledS\n",
    "    if len(nums)>0:\n",
    "        max_length = max([len(x) for x in nums])\n",
    "        for num in nums:\n",
    "            if len(num)==max_length:\n",
    "                num_long.append(num)\n",
    "    if len(str_obsledS_new)<len(str_obsledS):\n",
    "        num_long = extract_numbers(str_obsledS_new)\n",
    "    return np.unique(num_long) #max(nums, key=len)\n",
    "\n",
    "def add_string(filename, bio_annotation):\n",
    "    try:\n",
    "        sents = filestat[filename][0]\n",
    "        anns = filestat[filename][1]\n",
    "        ref = filestat[filename][2]\n",
    "        snmax = filestat[filename][3]\n",
    "        sntable = filestat[filename][4]\n",
    "    except:\n",
    "        return bio_annotation\n",
    "    with open(\"Documents/Cancer/CLL/Разметка/\"+filename) as tsvfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        prev = ''\n",
    "        for line in tsvreader:\n",
    "#             print(line)\n",
    "            if len(line)<ns+1:\n",
    "                continue\n",
    "            if line[ns] !='_' and len(line[2])>0 and line[ns].split('[')[0].find(code)>=0:\n",
    "                if prev == '':\n",
    "                    num_long = extract_numbers(line[2])\n",
    "                    if len(num_long)==0:\n",
    "                         bio_annotation.append(line[2] + '\\t' + 'B')\n",
    "                    else:\n",
    "                         bio_annotation.append(line[2].replace(str(num_long[0]), '[') + '\\t' + 'B')\n",
    "                         bio_annotation.append('num' + '\\t' + 'I')\n",
    "                         bio_annotation.append(']' + '\\t' + 'I')\n",
    "                    prev = line[ns].split('[')[0]\n",
    "                elif prev == line[ns].split('[')[0]:\n",
    "                    num_long = extract_numbers(line[2])\n",
    "                    if len(num_long)==0:\n",
    "                         bio_annotation.append(line[2] + '\\t' + 'I')\n",
    "                    else:\n",
    "                         bio_annotation.append(line[2].replace(str(num_long[0]), '[') + '\\t' + 'I')\n",
    "                         bio_annotation.append('num' + '\\t' + 'I')\n",
    "                         bio_annotation.append(']' + '\\t' + 'I')\n",
    "                    prev = line[ns].split('[')[0]\n",
    "            else:\n",
    "                if len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        num_long = extract_numbers(line[2])\n",
    "                        if anns[sn]>0: #or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            if len(num_long)==0:\n",
    "                                 bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "                            else:\n",
    "                                 bio_annotation.append(line[2].replace(str(num_long[0]), '[') + '\\t' + 'O')\n",
    "                                 bio_annotation.append('num' + '\\t' + 'O')\n",
    "                                 bio_annotation.append(']' + '\\t' + 'O')\n",
    "                            prev = ''\n",
    "                    except:\n",
    "                        prev = ''\n",
    "                        continue  \n",
    "                \n",
    "    return bio_annotation\n",
    "\n",
    "# def add_string(filename, bio_annotation):\n",
    "#     try:\n",
    "#         sents = filestat[filename][0]\n",
    "#         anns = filestat[filename][1]\n",
    "#         ref = filestat[filename][2]\n",
    "#         snmax = filestat[filename][3]\n",
    "#         sntable = filestat[filename][4]\n",
    "#     except:\n",
    "#         return bio_annotation\n",
    "#     with open(\"Documents/Cancer/CLL/Разметка/\"+filename) as tsvfile:\n",
    "#         tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "#         prev = ''\n",
    "#         for line in tsvreader:\n",
    "# #             print(line)\n",
    "#             if len(line)<ns+1:\n",
    "#                 continue\n",
    "#             if line[ns] !='_' and len(line[2])>0 and line[ns].split('[')[0].find(code)>=0:\n",
    "#                 if prev == '':\n",
    "#                     bio_annotation.append(line[2] + '\\t' + 'B')\n",
    "#                     prev = line[ns].split('[')[0]\n",
    "#                 elif prev == line[ns].split('[')[0]:\n",
    "#                     bio_annotation.append(line[2] + '\\t' + 'I')\n",
    "#                     prev = line[ns].split('[')[0]\n",
    "#             else:\n",
    "#                 if len(line[2].strip())>0:\n",
    "#                     try:\n",
    "#                         sn = line[0].split('-')[0]\n",
    "#                         if anns[sn]>0: #or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "#                             bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "#                             prev = ''\n",
    "#                     except:\n",
    "#                         prev = ''\n",
    "#                         continue  \n",
    "#     with open(\"Documents/Cancer/CLL/Разметка/\"+filename) as tsvfile:\n",
    "#         tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "#         prev = ''\n",
    "#         for line in tsvreader:\n",
    "# #             print(line)\n",
    "#             if len(line)<ns+1:\n",
    "#                 continue\n",
    "#             if line[ns] !='_' and len(line[2])>0 and line[ns].split('[')[0].find(code)>=0:\n",
    "#                 if prev == '':\n",
    "#                     bio_annotation.append(line[2] + '\\t' + 'B')\n",
    "#                     prev = line[ns].split('[')[0]\n",
    "#                 elif prev == line[ns].split('[')[0]:\n",
    "#                     bio_annotation.append(line[2] + '\\t' + 'I')\n",
    "#                     prev = line[ns].split('[')[0]\n",
    "#             else:\n",
    "#                 if len(line[2].strip())>0:\n",
    "#                     try:\n",
    "#                         sn = line[0].split('-')[0]\n",
    "#                         if anns[sn]>0: #or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "#                             bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "#                             prev = ''\n",
    "#                     except:\n",
    "#                         prev = ''\n",
    "#                         continue  \n",
    "#     return bio_annotation\n",
    "\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "\n",
    "code = 'demography'\n",
    "code_ = 'demography'\n",
    "filestat = files_stat\n",
    "\n",
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\") # + os.listdir(\"Documents/Cancer/CLL/Разметка/new_articles\") \n",
    "# listfiles = [file for file in listfiles if file[:3]=='PMC']\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[:10]:\n",
    "    print(file)\n",
    "    if file[:3]=='PMC' or file=='16611308.tsv' or file=='1503098.tsv':\n",
    "        ns = 4\n",
    "    else:\n",
    "        ns = 6\n",
    "    if file[-3:]=='tsv':\n",
    "        bio_annotation = add_string(file, bio_annotation)\n",
    "#         print(bio_annotation)\n",
    "            \n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/'+code_+'/devel.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[11:15]:\n",
    "    print(file)\n",
    "    if file[:3]=='PMC' or file=='16611308.tsv' or file=='1503098.tsv':\n",
    "        ns = 4\n",
    "    else:\n",
    "        ns = 6    \n",
    "    if file[-3:]=='tsv':\n",
    "        bio_annotation = add_string(file, bio_annotation)\n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/'+code_+'/test.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[15:]:\n",
    "    print(file)\n",
    "    if file[:3]=='PMC' or file=='16611308.tsv' or file=='1503098.tsv':\n",
    "        ns = 4\n",
    "    else:\n",
    "        ns = 6\n",
    "    if file[-3:]=='tsv':\n",
    "        bio_annotation = add_string(file, bio_annotation)\n",
    "            \n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/'+code_+'/train_dev.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\") + os.listdir(\"Documents/Cancer/CLL/Разметка/new_articles\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filestat['PMC2827854.tsv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение параграфов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests  as req\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = pd.read_excel('Documents/Cancer/CLL/ХЛЛ список публикаций КИ_28.12.2021.xlsx')\n",
    "pmids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url):\n",
    "    dom = req.get(url).text\n",
    "    jsn = json.loads(dom)\n",
    "    return jsn['documents'][0]['passages'][1]['text']\n",
    "\n",
    "url = 'https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pubmed.cgi/BioC_json/27711974/unicode'\n",
    "parse(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Documents/Cancer/CLL/ABstracts/'+str(27711974)+'.csv', 'w') as f:\n",
    "        f.write(parse(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url):\n",
    "    dom = req.get(url).text\n",
    "    jsn = json.loads(dom)\n",
    "    return jsn['documents'][0]['passages'][1]['text']\n",
    "\n",
    "#abstracts\n",
    "dct_texts = {}\n",
    "for pmid in pmids['PMID']:\n",
    "    url = 'https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pubmed.cgi/BioC_json/'+str(pmid)+'/unicode'\n",
    "    try:\n",
    "        dct_texts[pmid] = parse(url)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pmid in pmids['PMID']:\n",
    "    try:\n",
    "        with open('Documents/Cancer/CLL/ABstracts/'+str(pmid)+'.csv', 'w') as f:\n",
    "            f.write(dct_texts[pmid])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(dct_fulltexts['PMC1785084'], 'html.parser')\n",
    "# txts = []\n",
    "# for passage in soup.find_all('passage'):\n",
    "#     flag = 0\n",
    "#     for child in passage.descendants:        \n",
    "#         if child.name == 'infon' and (child.text == 'abstract' or child.text == 'paragraph'):            \n",
    "#             txts.append(passage.find_all('text')[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fulltexts\n",
    "dct_fulltexts = {}\n",
    "for pmcid in pmids[pmids['PMCID'].notnull()]['PMCID']:\n",
    "    url = 'https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmcids='+pmcid\n",
    "    dct_fulltexts[pmcid] = req.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pmcid in pmids[pmids['PMCID'].notnull()]['PMCID']:\n",
    "    soup = BeautifulSoup(dct_fulltexts[pmcid], 'html.parser')\n",
    "    txts = []\n",
    "    for passage in soup.find_all('passage'):\n",
    "        for child in passage.descendants:        \n",
    "            if child.name == 'infon' and (child.text == 'abstract' or child.text == 'paragraph'):            \n",
    "                txts.append(passage.find_all('text')[0].text)\n",
    "    with open('Documents/Cancer/CLL/Fulltexts/'+str(pmcid)+'.txt', 'w') as f:\n",
    "        f.write('\\n'.join(txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для Лии\n",
    "articleids = []\n",
    "txts = []\n",
    "for pmcid in pmids[pmids['PMCID'].notnull()]['PMCID']:\n",
    "    soup = BeautifulSoup(dct_fulltexts[pmcid], 'html.parser')\n",
    "    for passage in soup.find_all('passage'):\n",
    "        for child in passage.descendants:        \n",
    "            if child.name == 'infon' and (child.text == 'abstract' or child.text == 'paragraph'):   \n",
    "                articleids.append(pmcid)\n",
    "                txts.append(passage.find_all('text')[0].text)\n",
    "                \n",
    "with open('CLL/paragraphs.txt', 'w') as f:\n",
    "    f.write('\\n'.join(txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids[pmids['PMCID'].notnull()]['PMCID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "\n",
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "prev = ''\n",
    "filestat = files_stat\n",
    "code = 'OS'\n",
    "folder = 'OS'\n",
    "\n",
    "for file in listfiles[0:5]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            counter = 0\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0 and line[6].split('[')[0].find(code)>=0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        bio_annotation.append(line[2] + '\\t' + 'B')\n",
    "                        prev = line[6].split('[')[0]\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        bio_annotation.append(line[2] + '\\t' + 'I')\n",
    "                        prev = line[6].split('[')[0]\n",
    "                elif len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "                            prev = ''\n",
    "                    except:\n",
    "                        continue           \n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/'+folder+'/valid.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[5:7]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0 and line[6].split('[')[0].find(code)>=0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        bio_annotation.append(line[2] + '\\t' + 'B')\n",
    "                        prev = line[6].split('[')[0]\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        bio_annotation.append(line[2] + '\\t' + 'I')\n",
    "                        prev = line[6].split('[')[0]\n",
    "                elif len(line[2].strip())>0:\n",
    "                    try:\n",
    "                        sn = line[0].split('-')[0]\n",
    "                        if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                            bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "                            prev = ''\n",
    "                    except:\n",
    "                        continue            \n",
    "\n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/'+folder+'/test.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "bio_annotation = []\n",
    "prev = ''\n",
    "\n",
    "for file in listfiles[7:]:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv':\n",
    "        sents = filestat[file][0]\n",
    "        ants = filestat[file][1]\n",
    "        ref = filestat[file][2]\n",
    "        snmax = filestat[file][3]\n",
    "        sntable = filestat[file][4]\n",
    "        for sn in snmax:\n",
    "            if ref[sn]>0:\n",
    "                refnum=sn\n",
    "        with open(\"Documents/Cancer/CLL/Разметка/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2].strip())>0 and line[6].split('[')[0].find(code)>=0:\n",
    "        #             print(line[2], line[6])\n",
    "                    if prev == '':\n",
    "                        bio_annotation.append(line[2] + '\\t' + 'B')\n",
    "                        prev = line[6].split('[')[0]\n",
    "                    elif prev == line[6].split('[')[0]:\n",
    "                        bio_annotation.append(line[2] + '\\t' + 'I')\n",
    "                        prev = line[6].split('[')[0]\n",
    "                else:\n",
    "                    if len(line[2].strip())>0:\n",
    "                        try:\n",
    "                            sn = line[0].split('-')[0]\n",
    "                            if anns[sn]>0 or sn not in sntable and pd.to_numeric(sn)<pd.to_numeric(refnum):\n",
    "                                bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "                                prev = ''\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "            \n",
    "f=open('Documents/Cancer/CLL/Разметка/TRAIN/'+folder+'/train.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разметка RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "entity = []\n",
    "index = []\n",
    "sentence = []\n",
    "\n",
    "dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/Marked_articles/'\n",
    "#listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "listfiles = os.listdir(dir_)\n",
    "\n",
    "prev = ''\n",
    "jsons = []\n",
    "jsons_neg = []\n",
    "\n",
    "for file in listfiles:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv' and file[:3]!='cre':\n",
    "        bio_annotation = []\n",
    "        entity = []\n",
    "        index = []\n",
    "        sentence = []\n",
    "        \n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "                print(line)\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2])>0:\n",
    "#                     print(line[2], line[6])\n",
    "#                     if prev == '':\n",
    "                    bio_annotation.append(line[2])\n",
    "                    prev = line[4].split('[')[0]\n",
    "                    index.append(line[6].split('[')[1])\n",
    "                    entity.append(prev)\n",
    "                    sentence.append(line[0])\n",
    "#                     elif prev == line[6].split('[')[0]:\n",
    "#                         bio_annotation.append(line[2])\n",
    "#                         prev = line[6].split('[')[0]\n",
    "#                         index.append(line[6].split('[')[1])\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(len(bio_annotation)):\n",
    "            for j in range(len(bio_annotation)):\n",
    "                if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] and index[i]==index[j]:\n",
    "                    print(i,j)\n",
    "                    #извлекаем полную запись\n",
    "                    k=0\n",
    "                    while i+k< len(entity) and entity[i+k]==entity[i] and index[i+k]==index[i]:\n",
    "                        k+=1\n",
    "                    p=0\n",
    "                    while j+p< len(entity) and  entity[j+p]==entity[j] and index[j+p]==index[j]:\n",
    "                        p+=1\n",
    "                    s=[]\n",
    "                    with open(dir_+file) as tsvfile:\n",
    "                        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                        for line in tsvreader:\n",
    "                            if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "                                s.append(line[2])\n",
    "                    jsons.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                  't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                  'tokens':s\n",
    "                                 })\n",
    "                    i = i+k\n",
    "                    j = j+p\n",
    "                if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] and index[i]!=index[j] \\\n",
    "                    and sentence[i].split('-')[0]==sentence[j].split('-')[0]:\n",
    "                        #извлекаем полную запись\n",
    "                        k=0\n",
    "                        while i+k< len(entity) and entity[i+k]==entity[i] and index[i+k]==index[i]:\n",
    "                            k+=1\n",
    "                        p=0\n",
    "                        while j+p< len(entity) and  entity[j+p]==entity[j] and index[j+p]==index[j]:\n",
    "                            p+=1\n",
    "                        s=[]\n",
    "                        with open(dir_+file) as tsvfile:\n",
    "                            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                            for line in tsvreader:\n",
    "                                if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "                                    s.append(line[2])\n",
    "                            jsons_neg.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                          't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                          'tokens':s\n",
    "                                         })\n",
    "            \n",
    "# f=open('Documents/Cancer/CLL/Разметка/TRAIN/train.tsv','w')\n",
    "# cnt = 0\n",
    "# for ele in bio_annotation:\n",
    "# #     print(ele)\n",
    "#     if cnt<=50:\n",
    "#         f.write(ele+'\\n')\n",
    "#         cnt+=1\n",
    "#     else:\n",
    "#         f.write('\\n')\n",
    "#         cnt = 0\n",
    "#         f.write(ele+'\\n')\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "nfiles = 0\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "entity = []\n",
    "index = []\n",
    "sentence = []\n",
    "entity_unique = []\n",
    "\n",
    "dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/Marked_articles/'\n",
    "#listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "listfiles = os.listdir(dir_)\n",
    "\n",
    "prev = ''\n",
    "jsons = []\n",
    "jsons_neg_train = []\n",
    "ent_ids = []\n",
    "\n",
    "for file in listfiles:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv' and file[:3]!='cre' and file.find('ann_382')<0:\n",
    "        nfiles+=1\n",
    "        bio_annotation = []\n",
    "        entity = []\n",
    "        index = []\n",
    "        sentence = []\n",
    "        nconn = []\n",
    "        ent_ids = []\n",
    "        ent_dict = []\n",
    "        dk=0\n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[3]!= '_':\n",
    "                    ent_dict.append([line[3],line[4]])\n",
    "        df = pd.DataFrame(ent_dict)\n",
    "        df1 = df[df[0]=='*'].reset_index().reset_index()\n",
    "        nzs = df1.shape[0]+2\n",
    "        df2 = df[df[0]!='*'].drop_duplicates().reset_index().reset_index()\n",
    "        df2['level_0'] = df2['level_0']+df1.shape[0]+1\n",
    "        dict_df2={}\n",
    "        for t, b in zip(df2[1], df2['level_0']):\n",
    "            dict_df2[t] = b\n",
    "        nz = 0\n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "#                 print(line)\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2])>0:\n",
    "#                     print(line[2], line[6])\n",
    "#                     if prev == '':\n",
    "                    nc = len(line[6].split('|'))                      \n",
    "                    prev = line[4].split('[')[0]\n",
    "                    if prev not in entity_unique:\n",
    "                        entity_unique.append(prev)\n",
    "                    if line[4].find(']')>0:\n",
    "                        ent_id = dict_df2[line[4]]\n",
    "                    else:\n",
    "                        ent_id = nz\n",
    "                        nz+=1\n",
    "#                     if ent_id>0:\n",
    "#                         print(ent_id)\n",
    "                    bio_annotation.append(line[2])\n",
    "                    index.append([line[6].split('|')[r].split('[')[1].split(']')[0] for r in range(nc)])\n",
    "                    entity.append(prev)\n",
    "                    sentence.append(line[0])\n",
    "                    ent_ids.append(ent_id)\n",
    "                elif line[4] != '_' and len(line[2])>0:\n",
    "                    prev = line[4].split('[')[0]\n",
    "                    if prev not in entity_unique:\n",
    "                        entity_unique.append(prev)\n",
    "                    if line[4].find(']')>0:\n",
    "                        ent_id = dict_df2[line[4]]\n",
    "                    else:\n",
    "                        ent_id = nz\n",
    "                        nz+=1\n",
    "                    if nz>nzs+1:\n",
    "                        print('nz problem:',file, nz, nzs)\n",
    "                    bio_annotation.append(line[2])\n",
    "                    index.append([''])\n",
    "                    entity.append(prev)\n",
    "                    sentence.append(line[0])\n",
    "                    ent_ids.append(ent_id)\n",
    "#                     elif prev == line[6].split('[')[0]:\n",
    "#                         bio_annotation.append(line[2])\n",
    "#                         prev = line[6].split('[')[0]\n",
    "#                         index.append(line[6].split('[')[1])\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        k=1\n",
    "        p=1\n",
    "        while i<len(bio_annotation):\n",
    "            k=1\n",
    "            while i+k< len(entity) and entity[i+k]==entity[i] \\\n",
    "                and abs(int(sentence[i+k].split('-')[1])-int(sentence[i+(k-1)].split('-')[1]))==1:\n",
    "                k+=1 ##весь токен\n",
    "            j = i+k\n",
    "            while j<len(bio_annotation):\n",
    "#                 print(i,j)\n",
    "                if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] \\\n",
    "                    and sentence[i].split('-')[0]==sentence[j].split('-')[0]: \n",
    "                        #извлекаем полную запись\n",
    "                        p=1\n",
    "                        while j+p< len(entity) and  entity[j+p]==entity[j] \\\n",
    "                            and abs(int(sentence[j+p].split('-')[1])-int(sentence[j+p-1].split('-')[1]))==1:\n",
    "                            p+=1\n",
    "                        s=[]\n",
    "                        with open(dir_+file) as tsvfile:\n",
    "                            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                            for line in tsvreader:\n",
    "                                if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "                                    s.append(line[2])\n",
    "                            ni = 0 #flag of conn\n",
    "                            for nii in index[i]:\n",
    "                                for nij in index[j]:\n",
    "                                    if nii == nij and nii !='' and entity[i]!='_' and entity[j]!='_':\n",
    "                                        jsons.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                                      't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                                      'tokens':s,\n",
    "                                                      'h_id': ent_ids[i],\n",
    "                                                      't_id': ent_ids[j],\n",
    "                                                      'filename': file\n",
    "                                                     })\n",
    "                                        ni+=1\n",
    "                            if ni==0 and entity[i]!='_' and entity[j]!='_':\n",
    "                                jsons_neg_train.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                          't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                          'tokens':s,\n",
    "                                          'h_id': ent_ids[i],\n",
    "                                          't_id': ent_ids[j],\n",
    "                                          'filename': file\n",
    "                                                       })\n",
    "                        j = j+p\n",
    "                else:\n",
    "                    j+=1\n",
    "            i = i+k    \n",
    "\n",
    "\n",
    "#         for i in range(len(bio_annotation)):\n",
    "#             for j in range(len(bio_annotation)):\n",
    "#                 if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] and index[i]==index[j]:\n",
    "#         #             print(i,j)\n",
    "#                     #извлекаем полную запись\n",
    "#                     k=0\n",
    "#                     while i+k< len(entity) and entity[i+k]==entity[i] and index[i+k]==index[i]:\n",
    "#                         k+=1\n",
    "#                     p=0\n",
    "#                     while j+p< len(entity) and  entity[j+p]==entity[j] and index[j+p]==index[j]:\n",
    "#                         p+=1\n",
    "#                     s=[]\n",
    "#                     with open(dir_+file) as tsvfile:\n",
    "#                         tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "#                         for line in tsvreader:\n",
    "#                             if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "#                                 s.append(line[2])\n",
    "#                     jsons.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "#                                   't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "#                                   'tokens':s\n",
    "#                                  })\n",
    "#                     i = i+k\n",
    "#                     j = j+p\n",
    "#                 if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] and index[i]!=index[j] \\\n",
    "#                     and sentence[i].split('-')[0]==sentence[j].split('-')[0]:\n",
    "#                         #извлекаем полную запись\n",
    "#                         k=0\n",
    "#                         while i+k< len(entity) and entity[i+k]==entity[i] and index[i+k]==index[i]:\n",
    "#                             k+=1\n",
    "#                         p=0\n",
    "#                         while j+p< len(entity) and  entity[j+p]==entity[j] and index[j+p]==index[j]:\n",
    "#                             p+=1\n",
    "#                         s=[]\n",
    "#                         with open(dir_+file) as tsvfile:\n",
    "#                             tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "#                             for line in tsvreader:\n",
    "#                                 if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "#                                     s.append(line[2])\n",
    "#                             jsons_neg_train.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "#                                           't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "#                                           'tokens':s\n",
    "#                                          })\n",
    "#очистка отрицательных примеров\n",
    "jsons_neg_clear = []\n",
    "for k in range(len(jsons_neg_train)):\n",
    "    if jsons_neg_train[k] in jsons:\n",
    "        continue\n",
    "    else:\n",
    "        jsons_neg_clear.append(jsons_neg_train[k])\n",
    "jsons_neg_train = jsons_neg_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons_neg_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jsons_neg_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jsons[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inference для RE\n",
    "import os\n",
    "\n",
    "nfiles = 0\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "entity = []\n",
    "index = []\n",
    "sentence = []\n",
    "\n",
    "dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/DownloadToInception/'\n",
    "#listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "listfiles = os.listdir(dir_)\n",
    "\n",
    "prev = ''\n",
    "# jsons = []\n",
    "jsons_neg = []\n",
    "\n",
    "for file in listfiles:\n",
    "    print(file)\n",
    "    if file[-3:]=='txt' :\n",
    "        nfiles+=1\n",
    "        bio_annotation = []\n",
    "        entity = []\n",
    "        index = []\n",
    "        sentence = []\n",
    "        ent_ids = []\n",
    "        ent_dict = []\n",
    "        dk=0\n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[3]!= '_':\n",
    "                    ent_dict.append([line[3],line[4]])\n",
    "        df = pd.DataFrame(ent_dict)\n",
    "        df1 = df[df[0]=='*'].reset_index().reset_index()\n",
    "        nzs = df1.shape[0]+2\n",
    "        df2 = df[df[0]!='*'].drop_duplicates().reset_index().reset_index()\n",
    "        df2['level_0'] = df2['level_0']+df1.shape[0]+1\n",
    "        dict_df2={}\n",
    "        for t, b in zip(df2[1], df2['level_0']):\n",
    "            dict_df2[t] = b\n",
    "        nz = 0\n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "#                 print(line)\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[4] !='_' and len(line[2])>0:\n",
    "#                     print(line[2], line[6])\n",
    "#                     if prev == '':\n",
    "                    bio_annotation.append(line[2])\n",
    "                    prev = line[4].split('[')[0]\n",
    "                    if line[4].find(']')>0:\n",
    "                        ent_id = dict_df2[line[4]]\n",
    "                    else:\n",
    "                        ent_id = nz\n",
    "                        nz+=1\n",
    "                    if nz>nzs+1:\n",
    "                        print('nz problem:',file, nz, nzs)\n",
    "                    entity.append(prev)\n",
    "                    sentence.append(line[0])\n",
    "                    ent_ids.append(ent_id)\n",
    "#                     elif prev == line[6].split('[')[0]:\n",
    "#                         bio_annotation.append(line[2])\n",
    "#                         prev = line[6].split('[')[0]\n",
    "#                         index.append(line[6].split('[')[1])\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        k=1\n",
    "        p=1\n",
    "        while i<len(bio_annotation):\n",
    "            k=1\n",
    "            while i+k< len(entity) and entity[i+k]==entity[i] \\\n",
    "                and abs(int(sentence[i+k].split('-')[1])-int(sentence[i+k-1].split('-')[1]))==1:\n",
    "                k+=1 ##весь токен\n",
    "            j = i+k\n",
    "            while j<len(bio_annotation):\n",
    "#                 print(i,j)\n",
    "                if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] \\\n",
    "                    and sentence[i].split('-')[0]==sentence[j].split('-')[0]: \n",
    "                        #извлекаем полную запись\n",
    "                        p=1\n",
    "                        while j+p< len(entity) and  entity[j+p]==entity[j] \\\n",
    "                            and abs(int(sentence[j+p].split('-')[1])-int(sentence[j+p-1].split('-')[1]))==1:\n",
    "                            p+=1\n",
    "                        s=[]\n",
    "                        with open(dir_+file) as tsvfile:\n",
    "                            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                            for line in tsvreader:\n",
    "                                if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "                                    s.append(line[2])\n",
    "                            jsons_neg.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                          't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                          'tokens':s,\n",
    "                                          'h_id': ent_ids[i],\n",
    "                                          't_id': ent_ids[j]\n",
    "                                         })                        \n",
    "                        j = j+p\n",
    "                else:\n",
    "                    j+=1\n",
    "            i = i+k    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62844"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jsons_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_chest.13-1766.tsv\n",
      "ann_j.ijcard.2017.02.094.tsv\n",
      "ann_s00246-007-9139-2.tsv\n",
      "ann_circulationaha.111.081125.tsv\n",
      "ann_CIRCHEARTFAILURE.108.796789.tsv\n",
      "ann_s10157-016-1344-y.tsv\n",
      "ann_NEJMoa012212.tsv\n",
      "ann_rccm.200308-1142OC.tsv\n",
      "ann_s13318-017-0424-z.tsv\n",
      "ann_113.full.tsv\n",
      "ann_1343.full.tsv\n",
      "ann_446.full.tsv\n",
      "ann_s00392-005-0266-6.tsv\n",
      "ann_11534940-000000000-00000.tsv\n",
      "ann_j.pupt.2018.02.005.tsv\n",
      "ann_j.healun.2012.04.005.tsv\n",
      "ann_ajh_2Fhps057.tsv\n",
      "ann_jcph.152.tsv\n",
      "ann_rccm.201506-1091le.tsv\n",
      "ann_bf02044111.tsv\n",
      "ann_j.ijcard.2015.08.080.tsv\n",
      "ann_pedsPAH.tsv\n",
      "ann_chest.129.6.1636.tsv\n",
      "ann_j.healun.2018.07.001.tsv\n",
      "ann_j.pupt.2014.04.007.tsv\n",
      "ann_ajrccm.163.4.2007116.tsv\n",
      "ann_09031936.05.00075305.tsv\n",
      "ann_11539110-000000000-00000.tsv\n",
      "ann_200330.full.tsv\n",
      "ann_s0100-879x2008000800003.tsv\n",
      "ann_chest.14-0193.tsv\n",
      "ann_1477-7525-11-31.tsv\n",
      "ann_s00277-018-3518-z.tsv\n",
      "ann_j.jacc.2006.12.037.tsv\n",
      "ann_0091270011398241.tsv\n",
      "ann_fjc.0b013e31827e0fa9.tsv\n",
      "ann_heartjnl-2016-309621.tsv\n",
      "ann_j.jpeds.2012.05.050.tsv\n",
      "ann_journal.pone.0114309.tsv\n",
      "ann_hrt.79.2.175.tsv\n",
      "ann_j.pupt.2008.11.009.tsv\n",
      "ann_circj.cj-11-0473.tsv\n",
      "ann_ham.2010.1075.tsv\n",
      "ann_jcph.193.tsv\n",
      "ann_j.echo.2005.07.019.tsv\n",
      "ann_13993003.01886-2017.tsv\n",
      "ann_405.full.tsv\n",
      "ann_09031936.00105914.tsv\n",
      "ann_j.hlc.2018.12.005.tsv\n",
      "ann_hrt.2004.051961.tsv\n",
      "ann_s00228-007-0408-z.tsv\n",
      "ann_ppul.23032.tsv\n",
      "ann_0091270004270833.tsv\n",
      "ann_j.jacc.2004.06.060.tsv\n",
      "ann_rccm.200603-358OC.tsv\n",
      "ann_000446583.tsv\n",
      "ann_j.1540-8175.2012.01809.x.tsv\n",
      "ann_chest.11-2212.tsv\n",
      "ann_s40256-014-0081-4.tsv\n",
      "ann_s0009-9236_2803_2900005-5.tsv\n",
      "ann_0091270007309708.tsv\n",
      "ann_1354.full.tsv\n",
      "ann_CIRCHEARTFAILURE.115.003011.tsv\n",
      "ann_j.healun.2017.09.024.tsv\n",
      "ann_1471-2466-11-25.tsv\n",
      "ann_222.full.tsv\n",
      "ann_j.healun.2006.10.019.tsv\n",
      "ann_CIRCULATIONAHA.105.605527.tsv\n",
      "ann_0091270009341182.tsv\n",
      "ann_j.jjcc.2016.07.002.tsv\n",
      "ann_03007995.2012.685930.tsv\n",
      "ann_j.healun.2016.12.012.tsv\n",
      "ann_j.healun.2008.04.009.tsv\n",
      "ann_03007995.2011.605440.tsv\n",
      "ann_chest.130.5.1471.tsv\n",
      "ann_j.healun.2006.11.009.tsv\n",
      "ann_j.jacc.2005.04.050.tsv\n",
      "ann_chest.11-0404.tsv\n",
      "ann_1410.full.tsv\n",
      "ann_030079905x30680.tsv\n",
      "ann_j.jvca.2007.10.015.tsv\n",
      "ann_09031936.00182909.tsv\n",
      "ann_09031936.04.00111904.tsv\n",
      "ann_s40256-017-0262-z.tsv\n",
      "ann_s12872-016-0361-9.tsv\n",
      "ann_rccm.200605-694OC.tsv\n",
      "ann_jcm-06-00043-v2.tsv\n",
      "ann_j.healun.2013.06.008.tsv\n",
      "ann_72.full.tsv\n",
      "ann_1602493.full.tsv\n",
      "ann_NEJMoa1209655.tsv\n",
      "ann_CIRCRESAHA.114.305951.tsv\n",
      "ann_2000513.full.tsv\n",
      "ann_Phosphodisesterase inhibitors in Pulmonary hypertension.tsv\n",
      "ann_s0140-6736_2895_2991504-4.tsv\n",
      "ann_j.ijcard.2010.10.051.tsv\n",
      "ann_hjh.0b013e3282f382ff.tsv\n",
      "ann_bcp.14039.tsv\n",
      "ann_jac_2Fdkw125.tsv\n",
      "ann_j.jacc.2006.05.070.tsv\n",
      "ann_09031936.04.00028404.tsv\n",
      "ann_s0735-1097_2899_2900312-5.tsv\n",
      "ann_circulationaha.106.618397.tsv\n",
      "ann_691.full.tsv\n",
      "ann_j.clinthera.2013.02.013.tsv\n",
      "ann_j.ijcard.2017.04.016.tsv\n",
      "ann_13993003.02493-2016.tsv\n",
      "ann_630.full.tsv\n",
      "ann_j.ahj.2007.03.005.tsv\n",
      "ann_s00421-011-2085-y.tsv\n",
      "ann_ejhf.1375.tsv\n",
      "ann_s00421-010-1799-6.tsv\n",
      "ann_112972980600700304.tsv\n",
      "ann_v10007-012-0027-9.tsv\n",
      "ann_13993003.02449-2016.tsv\n",
      "ann_770.full.tsv\n",
      "ann_rccm.200406-804oc.tsv\n",
      "ann_000090555.tsv\n",
      "ann_469.full.tsv\n",
      "ann_j.rmed.2006.12.007.tsv\n",
      "ann_CIRCULATIONAHA.107.716373.tsv\n",
      "ann_j.healun.2010.11.009.tsv\n",
      "ann_art.33460.tsv\n",
      "ann_j.jchf.2014.07.013.tsv\n",
      "ann_chest.121.6.1860.tsv\n",
      "ann_j.ijcard.2014.09.101.tsv\n",
      "ann_j.jacc.2006.06.062.tsv\n",
      "ann_clpt.2010.120.tsv\n",
      "ann_chest.07-0275.tsv\n",
      "ann_ehjci_2Fjeaa285.tsv\n",
      "ann_s-0029-1239496.tsv\n",
      "ann_rccm.201605-1024OC.tsv\n",
      "ann_1471-2261-9-15.tsv\n",
      "ann_fjc.0b013e31828685da.tsv\n",
      "ann_CIRCIMAGING.108.780247.tsv\n",
      "ann_j.cardfail.2012.02.004.tsv\n",
      "ann_j.cct.2011.04.001.tsv\n",
      "ann_CIRCULATIONAHA.104.473371.tsv\n",
      "ann_326.full.tsv\n",
      "ann_ard.2005.048967.tsv\n",
      "ann_j.1365-2362.2009.02116.x.tsv\n",
      "ann_267.full.tsv\n",
      "ann_j.vph.2006.01.013.tsv\n",
      "ann_1514.full.tsv\n",
      "ann_chest.128.4.2599.tsv\n",
      "ann_09031936.06.00057906.tsv\n",
      "ann_s41598-021-97396-z.tsv\n",
      "ann_NEJMoa1503184.tsv\n",
      "ann_01.pdr.0000198772.26417.66.tsv\n",
      "ann_s1047951117000981.tsv\n",
      "ann_journal.pone.0215146.tsv\n",
      "ann_j.healun.2009.09.020.tsv\n",
      "ann_09031936.00011308.tsv\n",
      "ann_00003088-200847090-00004.tsv\n",
      "ann_chest.121.5.1561.tsv\n",
      "ann_j.chest.2015.11.005.tsv\n",
      "ann_s00392-007-0490-3.tsv\n",
      "ann_circj.72.1147.tsv\n",
      "ann_j.ihj.2021.07.007.tsv\n",
      "ann_00498254.2012.664665.tsv\n",
      "ann_heart.83.4.406.tsv\n",
      "ann_s00408-013-9542-9.tsv\n",
      "ann_s2213-2600_2820_2930532-4.tsv\n",
      "ann_500.full.tsv\n",
      "ann_s40256-015-0117-4.tsv\n",
      "ann_Becker-Gruenig_et_al_Efficacy_of_exercise_training.tsv\n",
      "ann_chest.07-3035.tsv\n",
      "ann_art.22634.tsv\n",
      "ann_j.chest.2018.04.027.tsv\n",
      "ann_0253-7613.132158.tsv\n",
      "ann_rccm.201704-0789LE.tsv\n",
      "ann_j.1755-5922.2010.00213.x.tsv\n",
      "ann_jcph.888.tsv\n",
      "ann_114.full.tsv\n",
      "ann_ard.2007.079921.tsv\n",
      "ann_hcr.0000000000000117.tsv\n",
      "ann_j.pupt.2008.07.003.tsv\n",
      "ann_162.full.tsv\n",
      "ann_eurheartj_2Feht540.tsv\n",
      "ann_09031936.06.00044406.tsv\n",
      "ann_j.ijcard.2011.07.009.tsv\n",
      "ann_ptr.6714.tsv\n",
      "ann_j.healun.2014.01.853.tsv\n",
      "ann_ham.2013.1137.tsv\n",
      "ann_jrheum.100358.tsv\n",
      "ann_s40262-013-0063-8.tsv\n",
      "ann_09031936.06.00095705.tsv\n",
      "ann_rccm.201101-0093OC.tsv\n",
      "ann_icvts_2Fivv325.tsv\n",
      "ann_j.1743-6109.2005.20359.x.tsv\n",
      "ann_PHTN2007.tsv\n",
      "ann_thoraxjnl-2013-204150.tsv\n",
      "ann_chd.12764.tsv\n",
      "ann_circinterventions.115.002837.tsv\n",
      "ann_0091270008315315.tsv\n",
      "ann_1745-6215-15-486.tsv\n",
      "ann_j.tvjl.2014.08.009.tsv\n",
      "ann_cts.12382.tsv\n",
      "ann_0967-3334_2F28_2F1_2F001.tsv\n",
      "ann_jcpt.12085.tsv\n",
      "ann_hc3601.096826.tsv\n",
      "ann_ppul.23241.tsv\n",
      "ann_j.healun.2011.03.011.tsv\n",
      "ann_j.hlc.2017.02.015.tsv\n",
      "ann_japplphysiol.00087.2016.tsv\n",
      "ann_09031936.00149011.tsv\n",
      "ann_eurjhf_2Fhft017.tsv\n",
      "ann_rccm.200307-957OC.tsv\n",
      "ann_73_1731.tsv\n",
      "ann_nejmoa020204.tsv\n",
      "ann_168.full.tsv\n",
      "ann_5584_2016_18.tsv\n",
      "ann_j.amjcard.2005.07.129.tsv\n",
      "ann_0091270011418656.tsv\n",
      "ann_j.healun.2007.07.040.tsv\n",
      "ann_2016-10-revue_litt.tsv\n",
      "ann_j.cardfail.2010.10.004.tsv\n",
      "ann_circulationaha.110.016667.tsv\n",
      "ann_chest.07-2324.tsv\n",
      "ann_13993003.01030-2019.tsv\n",
      "ann_chest.128.2.709.tsv\n",
      "ann_s00296-008-0789-z.tsv\n",
      "ann_5-MVD_PAHT_Stepien_JSAP2009.tsv\n",
      "ann_s00380-009-1176-8.tsv\n",
      "ann_s00330-012-2606-z.tsv\n",
      "ann_j.transproceed.2007.08.069.tsv\n",
      "ann_s40261-014-0207-0.tsv\n",
      "ann_j.ejim.2008.03.008.tsv\n",
      "ann_000355875.tsv\n",
      "ann_j.healun.2015.05.025.tsv\n",
      "ann_fjc.0b013e31802b3184.tsv\n",
      "ann_7.full.tsv\n",
      "ann_j.amjcard.2005.06.083.tsv\n",
      "ann_Sutendra-2013-Pulmonary arterial h.tsv\n",
      "ann_0003319711411704.tsv\n",
      "ann_chest.129.4.1009.tsv\n",
      "ann_bcp.12584.tsv\n",
      "ann_s00134-003-2016-4.tsv\n",
      "ann_chest.09-2099.tsv\n",
      "ann_09031936.00107012.tsv\n",
      "ann_190023.full.tsv\n",
      "ann_030079906x167390.tsv\n",
      "ann_0003-4819-141-3-200408030-00005.tsv\n",
      "ann_chest.128.4.2368.tsv\n",
      "ann_s00380-014-0544-1.tsv\n",
      "ann_j.ejcts.2005.09.007.tsv\n",
      "ann_000450759.tsv\n",
      "ann_aca.aca_231_16.tsv\n",
      "ann_s12098-020-03643-y.tsv\n",
      "ann_j.ijcard.2012.02.007.tsv\n",
      "ann_rheumatology_2Fkep398.tsv\n",
      "ann_s00134-011-2254-9.tsv\n",
      "ann_chest.10-1166.tsv\n",
      "ann_09031936.01.17100080.tsv\n",
      "ann_1007.full.tsv\n",
      "ann_j.healun.2007.04.013.tsv\n",
      "ann_bf03021608.tsv\n",
      "ann_circj.69.131.tsv\n",
      "ann_j.1747-0803.2009.00297.x.tsv\n",
      "ann_bja_2Faet308.tsv\n",
      "ann_00000539-200112000-00018.tsv\n",
      "ann_13993003.002762019.tsv\n",
      "ann_annalsats.201410-463oc.tsv\n",
      "ann_chest.06-2118.tsv\n",
      "ann_copd.s260917.tsv\n",
      "ann_j.healun.2018.09.003.tsv\n",
      "ann_circj.cj-15-0599.tsv\n",
      "ann_1465-9921-10-129.tsv\n",
      "ann_s00296-004-0439-z.tsv\n",
      "ann_67.full.tsv\n",
      "ann_j.jclinane.2005.08.018.tsv\n",
      "ann_944.full.tsv\n",
      "ann_j.healun.2009.09.005.tsv\n",
      "ann_j.hrtlng.2020.04.006.tsv\n",
      "ann_bmjopen-2016-011028.tsv\n",
      "ann_S2213-2600_2816_2930307-1.tsv\n",
      "ann_13993003.00090-2016.tsv\n",
      "ann_170134.full.tsv\n",
      "ann_j.cardfail.2009.09.008.tsv\n",
      "ann_000242498.tsv\n",
      "ann_j.hlc.2012.06.013.tsv\n",
      "ann_1303.full.tsv\n",
      "ann_1753465808103499.tsv\n",
      "ann_1074248411429966.tsv\n",
      "ann_gut.2005.077453.tsv\n",
      "ann_bcp.13267.tsv\n",
      "ann_rccm.201001-0123oc.tsv\n",
      "ann_03007990903210066.tsv\n",
      "ann_030079905x46232.tsv\n",
      "ann_chest.06-2690.tsv\n",
      "ann_j.healun.2014.02.019.tsv\n",
      "ann_s00296-006-0222-4.tsv\n",
      "ann_circj.cj-16-0254.tsv\n",
      "ann_j.ijcard.2019.07.004.tsv\n",
      "ann_s0735-1097_2802_2901786-2.tsv\n",
      "ann_09031936.98.12040932.tsv\n",
      "ann_j.rmed.2017.03.025.tsv\n",
      "ann_ajh_2Fhpt018.tsv\n",
      "ann_j.ijcard.2004.09.002.tsv\n",
      "ann_j.1939-1676.2010.0517.x.tsv\n",
      "ann_S2213-2600_2816_2930019-4.tsv\n",
      "ann_CIRCULATIONAHA.107.742510.tsv\n",
      "ann_S2213-2600_2820_2930394-5.tsv\n",
      "ann_chest.06-2903.tsv\n",
      "ann_circj.69.335.tsv\n",
      "ann_09031936.06.00030206.tsv\n",
      "ann_j.ejcts.2010.01.045.tsv\n",
      "ann_s11748-016-0724-2.tsv\n",
      "ann_787.full.tsv\n",
      "ann_13993003.02044-2014.tsv\n",
      "ann_circj.cj-10-1310.tsv\n",
      "ann_j.amjcard.2013.04.051.tsv\n",
      "ann_ar3883.tsv\n",
      "ann_01.jcm.0000203850.97890.fe.tsv\n",
      "ann_000107977.tsv\n",
      "ann_heart.86.6.661.tsv\n",
      "ann_CIRCHEARTFAILURE.111.963314.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_j.ahj.2005.07.005.tsv\n",
      "ann_j.1440-1754.1996.tb00942.x.tsv\n",
      "ann_rccm.201507-1456OC.tsv\n",
      "ann_art.34614.tsv\n",
      "ann_asheducation-2011.1.419.tsv\n",
      "ann_j.pupt.2005.09.006.tsv\n",
      "ann_667.full.tsv\n",
      "ann_154.full.tsv\n",
      "ann_ham.2006.7.54.tsv\n",
      "ann_cc7005.tsv\n",
      "ann_eurheartj_2Fehu035.tsv\n",
      "ann_1745-6215-14-188.tsv\n",
      "ann_j.healun.2011.08.019.tsv\n",
      "ann_ehm074.tsv\n",
      "ann_1755-5922.12008.tsv\n",
      "ann_ejhf.177.tsv\n",
      "ann_s12325-013-0029-0.tsv\n",
      "ann_j.chest.2017.04.188.tsv\n",
      "ann_eurjhf_2Fhfs071.tsv\n",
      "ann_clpt.2009.217.tsv\n",
      "ann_asheducation-2017.1.423.tsv\n",
      "ann_chest.07-0592.tsv\n",
      "ann_j.jjcc.2014.01.003.tsv\n",
      "ann_mtc.2003.107.tsv\n",
      "ann_thx.2005.041954.tsv\n",
      "ann_hr.2009.113.tsv\n",
      "ann_498.full.tsv\n",
      "ann_503.full.tsv\n",
      "ann_j.healun.2006.09.016.tsv\n",
      "ann_1465-9921-6-92.tsv\n",
      "ann_s002469900463.tsv\n",
      "ann_j.1365-2125.2009.03384.x.tsv\n",
      "ann_biomolecules-10-01261-v3.tsv\n",
      "ann_j.jtcvs.2003.11.035.tsv\n",
      "ann_j.echo.2005.07.010.tsv\n",
      "ann_pah-novinky-bwv.tsv\n",
      "ann_09031936.00176312.tsv\n",
      "ann_rccm.200410-1411OC.tsv\n",
      "ann_rccm.201809-1631LE.tsv\n",
      "ann_psp4.12202.tsv\n",
      "ann_09031936.02.02572001.tsv\n",
      "ann_j.vph.2005.03.003.tsv\n",
      "ann_s00408-014-9667-5.tsv\n",
      "ann_S2213-2600_2814_2970013-X.tsv\n",
      "ann_v058p00797.tsv\n",
      "ann_0961203307076509.tsv\n",
      "ann_0091270008319793.tsv\n",
      "ann_ehf2.12630.tsv\n",
      "ann_j.rmed.2016.06.018.tsv\n",
      "ann_1701214.full.tsv\n",
      "ann_1471-2466-12-54.tsv\n",
      "ann_chest.12-3023.tsv\n",
      "ann_j.hlc.2018.04.299.tsv\n",
      "ann_COPD.S106480.tsv\n",
      "ann_bph.14621.tsv\n",
      "ann_respcare.05280.tsv\n",
      "ann_j.1399-3046.2007.00863.x.tsv\n",
      "ann_s0735-1097_2899_2900494-5.tsv\n",
      "ann_blood-2015-08-618561.tsv\n",
      "ann_s10456-010-9192-y.tsv\n",
      "ann_ard.2007.069609.tsv\n",
      "ann_PCC.0000000000002410.tsv\n",
      "ann_13993003.02004-2018.tsv\n",
      "ann_circj.cj-12-0203.tsv\n",
      "ann_s0735-1097_2803_2900555-2.tsv\n",
      "ann_j.chest.2021.01.066.tsv\n",
      "ann_smw_147_w14462.tsv\n",
      "ann_148.full.tsv\n",
      "ann_j.hlc.2019.01.013.tsv\n",
      "ann_jcph.639.tsv\n",
      "ann_hrt.2006.100388.tsv\n",
      "ann_CIRCHEARTFAILURE.115.002729.tsv\n",
      "ann_s00408-020-00402-w.tsv\n",
      "ann_1702638.full.tsv\n",
      "ann_0091270009351173.tsv\n",
      "ann_s0140-6736_2808_2960919-8.tsv\n",
      "ann_704.full.tsv\n",
      "ann_j.clinthera.2019.04.007.tsv\n",
      "ann_1471-2466-8-3.tsv\n",
      "ann_2047-783X-19-2.tsv\n",
      "ann_chest.129.3.683.tsv\n",
      "ann_rccm.201411-2061le.tsv\n",
      "ann_art.21433.tsv\n",
      "ann_382.full.tsv\n",
      "ann_01.fjc.0000166207.74178.d0.tsv\n",
      "ann_09031936.00213911.tsv\n",
      "ann_218.full.tsv\n"
     ]
    }
   ],
   "source": [
    "#inference для RE с файлом\n",
    "import os\n",
    "\n",
    "nfiles = 0\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "entity = []\n",
    "index = []\n",
    "sentence = []\n",
    "\n",
    "# dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/PAH_v2_project_2022-09-14_2106/annotation/'\n",
    "dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/готовые файлы tsv/'\n",
    "#listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "listfiles = os.listdir(dir_)\n",
    "\n",
    "prev = ''\n",
    "# jsons = []\n",
    "jsons_neg = []\n",
    "\n",
    "for file in listfiles:\n",
    "    f = file\n",
    "    print(file)\n",
    "#     file = file+'/sasha.tsv'\n",
    "    if file[-3:]=='tsv' and file.find('Store')==-1 and f in dict_files.keys():\n",
    "        nfiles+=1\n",
    "        bio_annotation = []\n",
    "        entity = []\n",
    "        index = []\n",
    "        sentence = []\n",
    "        ent_ids = []\n",
    "        ent_dict = []\n",
    "        dk=0\n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[3]!= '_':\n",
    "                    ent_dict.append([line[3],line[4]])\n",
    "        df = pd.DataFrame(ent_dict)\n",
    "        if len(ent_dict)>0:\n",
    "            df1 = df[df[0]=='*'].reset_index().reset_index()\n",
    "            nzs = df1.shape[0]+2\n",
    "            df2 = df[df[0]!='*'].drop_duplicates().reset_index().reset_index()\n",
    "            df2['level_0'] = df2['level_0']+df1.shape[0]+1\n",
    "            dict_df2={}\n",
    "            for t, b in zip(df2[1], df2['level_0']):\n",
    "                dict_df2[t] = b\n",
    "            nz = 0\n",
    "            with open(dir_+file) as tsvfile:\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                for line in tsvreader:\n",
    "    #                 print(line)\n",
    "                    if len(line)<5:\n",
    "                        continue\n",
    "                    if line[4] !='_' and len(line[2])>0:\n",
    "    #                     print(line[2], line[6])\n",
    "    #                     if prev == '':\n",
    "\n",
    "                        bio_annotation.append(line[2])\n",
    "                        prev = line[4].split('[')[0]\n",
    "                        if line[4].find(']')>0:\n",
    "                            ent_id = dict_df2[line[4]]\n",
    "                        else:\n",
    "                            ent_id = nz\n",
    "                            nz+=1\n",
    "                        if nz>nzs+1:\n",
    "                            print('nz problem:',file, nz, nzs)\n",
    "                        entity.append(prev)\n",
    "                        sentence.append(line[0])\n",
    "                        ent_ids.append(ent_id)\n",
    "    #                     elif prev == line[6].split('[')[0]:\n",
    "    #                         bio_annotation.append(line[2])\n",
    "    #                         prev = line[6].split('[')[0]\n",
    "    #                         index.append(line[6].split('[')[1])\n",
    "\n",
    "\n",
    "            i = 0\n",
    "            k=1\n",
    "            p=1\n",
    "            while i<len(bio_annotation):\n",
    "                k=1\n",
    "                while i+k< len(entity) and entity[i+k]==entity[i] \\\n",
    "                    and abs(int(sentence[i+k].split('-')[1])-int(sentence[i+k-1].split('-')[1]))==1:\n",
    "                    k+=1 ##весь токен\n",
    "                j = i+k\n",
    "                while j<len(bio_annotation):\n",
    "    #                 print(i,j)\n",
    "                    if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] \\\n",
    "                        and sentence[i].split('-')[0]==sentence[j].split('-')[0]: \n",
    "                            #извлекаем полную запись\n",
    "                            p=1\n",
    "                            while j+p< len(entity) and  entity[j+p]==entity[j] \\\n",
    "                                and abs(int(sentence[j+p].split('-')[1])-int(sentence[j+p-1].split('-')[1]))==1:\n",
    "                                p+=1\n",
    "                            s=[]\n",
    "                            with open(dir_+file) as tsvfile:\n",
    "                                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                                for line in tsvreader:\n",
    "                                    if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "                                        s.append(line[2])\n",
    "                                jsons_neg.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                              't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                              'tokens':s,\n",
    "                                              'h_id': ent_ids[i],\n",
    "                                              't_id': ent_ids[j],\n",
    "                                              'filename': f.replace('.txt', '.tsv')\n",
    "                                             })                        \n",
    "                            j = j+p\n",
    "                    else:\n",
    "                        j+=1\n",
    "                i = i+k  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/готовые тексты/'\n",
    "#listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "listfiles = os.listdir(dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': [['bosentan'], 'Drug', [['12']]],\n",
       " 't': [['SAPH'], 'Reason', [['21']]],\n",
       " 'tokens': ['The',\n",
       "  'objective',\n",
       "  'of',\n",
       "  'this',\n",
       "  'study',\n",
       "  'was',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'the',\n",
       "  'effect',\n",
       "  'of',\n",
       "  'bosentan',\n",
       "  'therapy',\n",
       "  'on',\n",
       "  'pulmonary',\n",
       "  'arterial',\n",
       "  'hemodynamics',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'SAPH',\n",
       "  '.'],\n",
       " 'h_id': 0,\n",
       " 't_id': 1,\n",
       " 'filename': 'ann_chest.13-1766.tsv'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/Users/aelitta/Documents/DMMatrix/PAH/PAH_v2_project_2022-09-14_2106/annotation/ann_09031936.06.00057906.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for j in range(len(jsons_neg)):\n",
    "    data.append([str(j+1) , ' '.join(jsons_neg[j]['tokens']) , jsons_neg[j]['filename']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data.columns = ['sentence_id', 'sentence', 'filename']\n",
    "data.to_csv('/Users/aelitta/Documents/DMMatrix/PAH/sentences_for_inference.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The objective of this study was to determine t...</td>\n",
       "      <td>ann_chest.13-1766.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Methods : This 16-week study was a double-blin...</td>\n",
       "      <td>ann_chest.13-1766.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Methods : This 16-week study was a double-blin...</td>\n",
       "      <td>ann_chest.13-1766.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The cohort was randomized two to one to receiv...</td>\n",
       "      <td>ann_chest.13-1766.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The cohort was randomized two to one to receiv...</td>\n",
       "      <td>ann_chest.13-1766.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62839</th>\n",
       "      <td>62840</td>\n",
       "      <td>[ ] in which the effect of the addition of sil...</td>\n",
       "      <td>ann_218.full.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62840</th>\n",
       "      <td>62841</td>\n",
       "      <td>Indeed , studies reporting outcomes of patient...</td>\n",
       "      <td>ann_218.full.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62841</th>\n",
       "      <td>62842</td>\n",
       "      <td>Indeed , studies reporting outcomes of patient...</td>\n",
       "      <td>ann_218.full.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62842</th>\n",
       "      <td>62843</td>\n",
       "      <td>Indeed , studies reporting outcomes of patient...</td>\n",
       "      <td>ann_218.full.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62843</th>\n",
       "      <td>62844</td>\n",
       "      <td>Whereas the French and PHC registry equations ...</td>\n",
       "      <td>ann_218.full.tsv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62844 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_id                                           sentence  \\\n",
       "0               1  The objective of this study was to determine t...   \n",
       "1               2  Methods : This 16-week study was a double-blin...   \n",
       "2               3  Methods : This 16-week study was a double-blin...   \n",
       "3               4  The cohort was randomized two to one to receiv...   \n",
       "4               5  The cohort was randomized two to one to receiv...   \n",
       "...           ...                                                ...   \n",
       "62839       62840  [ ] in which the effect of the addition of sil...   \n",
       "62840       62841  Indeed , studies reporting outcomes of patient...   \n",
       "62841       62842  Indeed , studies reporting outcomes of patient...   \n",
       "62842       62843  Indeed , studies reporting outcomes of patient...   \n",
       "62843       62844  Whereas the French and PHC registry equations ...   \n",
       "\n",
       "                    filename  \n",
       "0      ann_chest.13-1766.tsv  \n",
       "1      ann_chest.13-1766.tsv  \n",
       "2      ann_chest.13-1766.tsv  \n",
       "3      ann_chest.13-1766.tsv  \n",
       "4      ann_chest.13-1766.tsv  \n",
       "...                      ...  \n",
       "62839       ann_218.full.tsv  \n",
       "62840       ann_218.full.tsv  \n",
       "62841       ann_218.full.tsv  \n",
       "62842       ann_218.full.tsv  \n",
       "62843       ann_218.full.tsv  \n",
       "\n",
       "[62844 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': [['bosentan'], 'Drug', [['12']]],\n",
       " 't': [['SAPH'], 'Reason', [['21']]],\n",
       " 'tokens': ['The',\n",
       "  'objective',\n",
       "  'of',\n",
       "  'this',\n",
       "  'study',\n",
       "  'was',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'the',\n",
       "  'effect',\n",
       "  'of',\n",
       "  'bosentan',\n",
       "  'therapy',\n",
       "  'on',\n",
       "  'pulmonary',\n",
       "  'arterial',\n",
       "  'hemodynamics',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'SAPH',\n",
       "  '.'],\n",
       " 'h_id': 0,\n",
       " 't_id': 1,\n",
       " 'filename': 'ann_chest.13-1766.tsv'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bio_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [lambda: i for i in range(3)]\n",
    "b = [f() for f in a]\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(jsons)):\n",
    "    if jsons[k] in jsons_neg:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jsons), len(jsons_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "entity = []\n",
    "index = []\n",
    "sentence = []\n",
    "\n",
    "listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/new_articles\")\n",
    "# listfiles = [file for file in listfiles if file[:3]=='PMC']\n",
    "\n",
    "\n",
    "prev = ''\n",
    "jsons = []\n",
    "jsons_neg = []\n",
    "\n",
    "for file in listfiles:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv' and file[:3]!='cre':\n",
    "        bio_annotation = []\n",
    "        entity = []\n",
    "        index = []\n",
    "        sentence = []\n",
    "        \n",
    "        with open(\"Documents/Cancer/CLL/Разметка/new_articles/\"+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "#                 print(line)\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[4] !='_' and len(line[2])>0:\n",
    "#                     print(line[2], line[6])\n",
    "#                     if prev == '':\n",
    "                    bio_annotation.append(line[2])\n",
    "                    prev = line[4].split('[')[0]\n",
    "                    index.append(line[4].split('[')[1])\n",
    "                    entity.append(prev)\n",
    "                    sentence.append(line[0])\n",
    "#                     elif prev == line[6].split('[')[0]:\n",
    "#                         bio_annotation.append(line[2])\n",
    "#                         prev = line[6].split('[')[0]\n",
    "#                         index.append(line[6].split('[')[1])\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(len(bio_annotation)):\n",
    "            for j in range(len(bio_annotation)):\n",
    "                if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] and index[i]==index[j]:\n",
    "        #             print(i,j)\n",
    "                    #извлекаем полную запись\n",
    "                    k=0\n",
    "                    while i+k< len(entity) and entity[i+k]==entity[i] and index[i+k]==index[i]:\n",
    "                        k+=1\n",
    "                    p=0\n",
    "                    while j+p< len(entity) and  entity[j+p]==entity[j] and index[j+p]==index[j]:\n",
    "                        p+=1\n",
    "                    s=[]\n",
    "                    with open(\"Documents/Cancer/CLL/Разметка/new_articles/\"+file) as tsvfile:\n",
    "                        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                        for line in tsvreader:\n",
    "                            if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "                                s.append(line[2])\n",
    "                    jsons.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                  't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                  'tokens':s\n",
    "                                 })\n",
    "                    i = i+k\n",
    "                    j = j+p\n",
    "                if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] and index[i]!=index[j] \\\n",
    "                    and sentence[i].split('-')[0]==sentence[j].split('-')[0]:\n",
    "                        #извлекаем полную запись\n",
    "                        k=0\n",
    "                        while i+k< len(entity) and entity[i+k]==entity[i] and index[i+k]==index[i]:\n",
    "                            k+=1\n",
    "                        p=0\n",
    "                        while j+p< len(entity) and  entity[j+p]==entity[j] and index[j+p]==index[j]:\n",
    "                            p+=1\n",
    "                        s=[]\n",
    "                        with open(\"Documents/Cancer/CLL/Разметка/new_articles/\"+file) as tsvfile:\n",
    "                            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                            for line in tsvreader:\n",
    "                                if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "                                    s.append(line[2])\n",
    "                            jsons_neg.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                          't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                          'tokens':s\n",
    "                                         })\n",
    "            \n",
    "# f=open('Documents/Cancer/CLL/Разметка/TRAIN/train.tsv','w')\n",
    "# cnt = 0\n",
    "# for ele in bio_annotation:\n",
    "# #     print(ele)\n",
    "#     if cnt<=50:\n",
    "#         f.write(ele+'\\n')\n",
    "#         cnt+=1\n",
    "#     else:\n",
    "#         f.write('\\n')\n",
    "#         cnt = 0\n",
    "#         f.write(ele+'\\n')\n",
    "\n",
    "# f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jsons_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons_neg[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os = 0\n",
    "pfs = 0\n",
    "with open('Documents/Cancer/CLL/Разметка/re_articles_train.txt', 'w') as f:\n",
    "    for i in range(1500):\n",
    "        ss = ' '.join(jsons[i]['tokens'])\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "        rel = \"NA\"\n",
    "        if jsons[i]['h'][1] =='OS' or jsons[i]['t'][1] =='OS':\n",
    "            rel = 'OS'\n",
    "            os+=1\n",
    "        if jsons[i]['h'][1] =='PFS' or jsons[i]['t'][1] =='PFS':\n",
    "            rel = 'PFS'\n",
    "            pfs+=1\n",
    "        pos1 = ss.find(h)\n",
    "        pos2 = ss.find(t)\n",
    "        s = {\"text\":ss,\"relation\":rel, \\\n",
    "             \"h\": {\"id\":str(i)+'h', \"name\":h, \"pos\": [pos1, pos1+len(h)]}, \\\n",
    "             \"t\": {\"id\":str(i)+'t', \"name\":t, \"pos\": [pos2, pos2+len(t)]}\n",
    "            }\n",
    "        f.write(str(s)+'\\n')\n",
    "f.close()\n",
    "with open('Documents/Cancer/CLL/Разметка/re_articles_val.txt', 'w') as f:\n",
    "    for i in range(len(jsons)-1500):\n",
    "        i = i+1500\n",
    "        ss = ' '.join(jsons[i]['tokens'])\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "        rel = \"NA\"\n",
    "        if jsons[i]['h'][1] =='OS' or jsons[i]['t'][1] =='OS':\n",
    "            rel = 'OS'\n",
    "            os+=1\n",
    "        if jsons[i]['h'][1] =='PFS' or jsons[i]['t'][1] =='PFS':\n",
    "            rel = 'PFS'\n",
    "            pfs+=1\n",
    "        pos1 = ss.find(h)\n",
    "        pos2 = ss.find(t)\n",
    "        s = {\"text\":ss,\"relation\":\"NA\", \\\n",
    "             \"h\": {\"id\":str(i)+'h', \"name\":h, \"pos\": [pos1, pos1+len(h)]}, \\\n",
    "             \"t\": {\"id\":str(i)+'t', \"name\":t, \"pos\": [pos2, pos2+len(t)]}\n",
    "            }\n",
    "        f.write(str(s)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os = 0\n",
    "pfs = 0\n",
    "with open('Documents/Cancer/CLL/Разметка/re_art_train.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(1500):\n",
    "        ss = ' '.join(jsons[i]['tokens'])\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "\n",
    "        s = ss.replace(h,'@'+jsons[i]['h'][1]+'$')\n",
    "        s = s.replace(t,'@'+jsons[i]['t'][1]+'$')\n",
    "\n",
    "        f.write(s+'\\t'+'1'+'\\n')\n",
    "        \n",
    "        ss = ' '.join(jsons_neg[i]['tokens'])\n",
    "        h = ' '.join(jsons_neg[i]['h'][0])\n",
    "        t = ' '.join(jsons_neg[i]['t'][0])\n",
    "\n",
    "        s = ss.replace(h,'@'+jsons_neg[i]['h'][1]+'$')\n",
    "        s = s.replace(t,'@'+jsons_neg[i]['t'][1]+'$')\n",
    "\n",
    "        f.write(s+'\\t'+'0'+'\\n')\n",
    "f.close()\n",
    "with open('Documents/Cancer/CLL/Разметка/re_art_val.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(len(jsons)-1500):\n",
    "        if i<300:\n",
    "            i = i+1500\n",
    "            ss = ' '.join(jsons[i]['tokens'])\n",
    "            h = ' '.join(jsons[i]['h'][0])\n",
    "            t = ' '.join(jsons[i]['t'][0])\n",
    "\n",
    "            s = ss.replace(h,'@'+jsons[i]['h'][1]+'$')\n",
    "            s = s.replace(t,'@'+jsons[i]['t'][1]+'$')\n",
    "\n",
    "            f.write(s+'\\t'+'1'+'\\n')\n",
    "            \n",
    "            ss = ' '.join(jsons_neg[i]['tokens'])\n",
    "            h = ' '.join(jsons_neg[i]['h'][0])\n",
    "            t = ' '.join(jsons_neg[i]['t'][0])\n",
    "\n",
    "            s = ss.replace(h,'@'+jsons_neg[i]['h'][1]+'$')\n",
    "            s = s.replace(t,'@'+jsons_neg[i]['t'][1]+'$')\n",
    "\n",
    "            f.write(s+'\\t'+'0'+'\\n')\n",
    "f.close()\n",
    "with open('Documents/Cancer/CLL/Разметка/re_art_test.tsv', 'w') as f:\n",
    "    f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(len(jsons)-1800):\n",
    "        i = i+1800\n",
    "        ss = ' '.join(jsons[i]['tokens'])\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "\n",
    "        s = ss.replace(h,'@'+jsons[i]['h'][1]+'$')\n",
    "        s = s.replace(t,'@'+jsons[i]['t'][1]+'$')\n",
    "\n",
    "        f.write(str(i)+'\\t'+s+'\\t'+'1'+'\\n')\n",
    "        \n",
    "        ss = ' '.join(jsons_neg[i]['tokens'])\n",
    "        h = ' '.join(jsons_neg[i]['h'][0])\n",
    "        t = ' '.join(jsons_neg[i]['t'][0])\n",
    "\n",
    "        s = ss.replace(h,'@'+jsons_neg[i]['h'][1]+'$')\n",
    "        s = s.replace(t,'@'+jsons_neg[i]['t'][1]+'$')\n",
    "\n",
    "        f.write(str(i)+'\\t'+s+'\\t'+'0'+'\\n')\n",
    "        \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os = 0\n",
    "pfs = 0\n",
    "with open('Documents/Cancer/CLL/Разметка/re_art_train_os.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(1500):\n",
    "        if jsons[i]['h'][1]=='OS' and jsons[i]['t'][1].find('therapy')>0 or \\\n",
    "                jsons[i]['t'][1]=='OS' and jsons[i]['h'][1].find('therapy')>0:           \n",
    "                ss = ' '.join(jsons[i]['tokens'])\n",
    "                h = ' '.join(jsons[i]['h'][0])\n",
    "                t = ' '.join(jsons[i]['t'][0])\n",
    "\n",
    "                s = ss.replace(h,'@'+jsons[i]['h'][1]+'$')\n",
    "                s = s.replace(t,'@'+jsons[i]['t'][1]+'$')\n",
    "\n",
    "                f.write(s+'\\t'+'1'+'\\n')\n",
    "\n",
    "                ss = ' '.join(jsons_neg[i]['tokens'])\n",
    "                h = ' '.join(jsons_neg[i]['h'][0])\n",
    "                t = ' '.join(jsons_neg[i]['t'][0])\n",
    "\n",
    "                s = ss.replace(h,'@'+jsons_neg[i]['h'][1]+'$')\n",
    "                s = s.replace(t,'@'+jsons_neg[i]['t'][1]+'$')\n",
    "\n",
    "                f.write(s+'\\t'+'0'+'\\n')\n",
    "f.close()\n",
    "with open('Documents/Cancer/CLL/Разметка/re_art_val_os.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(len(jsons)-1500):\n",
    "        if i<300:\n",
    "            i = i+1500\n",
    "            if jsons[i]['h'][1]=='OS' and jsons[i]['t'][1].find('therapy')>0 or \\\n",
    "                jsons[i]['t'][1]=='OS' and jsons[i]['h'][1].find('therapy')>0:           \n",
    "                    ss = ' '.join(jsons[i]['tokens'])\n",
    "                    h = ' '.join(jsons[i]['h'][0])\n",
    "                    t = ' '.join(jsons[i]['t'][0])\n",
    "\n",
    "                    ss = ' '.join(jsons[i]['tokens'])\n",
    "                    h = ' '.join(jsons[i]['h'][0])\n",
    "                    t = ' '.join(jsons[i]['t'][0])\n",
    "\n",
    "                    s = ss.replace(h,'@'+jsons[i]['h'][1]+'$')\n",
    "                    s = s.replace(t,'@'+jsons[i]['t'][1]+'$')\n",
    "\n",
    "                    f.write(s+'\\t'+'1'+'\\n')\n",
    "\n",
    "                    ss = ' '.join(jsons_neg[i]['tokens'])\n",
    "                    h = ' '.join(jsons_neg[i]['h'][0])\n",
    "                    t = ' '.join(jsons_neg[i]['t'][0])\n",
    "\n",
    "                    s = ss.replace(h,'@'+jsons_neg[i]['h'][1]+'$')\n",
    "                    s = s.replace(t,'@'+jsons_neg[i]['t'][1]+'$')\n",
    "\n",
    "                    f.write(s+'\\t'+'0'+'\\n')\n",
    "f.close()\n",
    "with open('Documents/Cancer/CLL/Разметка/re_art_test_os.tsv', 'w') as f:\n",
    "    f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(len(jsons)-1800):\n",
    "        i = i+1800\n",
    "        if jsons[i]['h'][1]=='OS' and jsons[i]['t'][1].find('therapy')>0 or \\\n",
    "            jsons[i]['t'][1]=='OS' and jsons[i]['h'][1].find('therapy')>0:           \n",
    "                ss = ' '.join(jsons[i]['tokens'])\n",
    "                h = ' '.join(jsons[i]['h'][0])\n",
    "                t = ' '.join(jsons[i]['t'][0])\n",
    "                ss = ' '.join(jsons[i]['tokens'])\n",
    "                h = ' '.join(jsons[i]['h'][0])\n",
    "                t = ' '.join(jsons[i]['t'][0])\n",
    "\n",
    "                s = ss.replace(h,'@'+jsons[i]['h'][1]+'$')\n",
    "                s = s.replace(t,'@'+jsons[i]['t'][1]+'$')\n",
    "\n",
    "                f.write(str(i)+'\\t'+s+'\\t'+'1'+'\\n')\n",
    "\n",
    "                ss = ' '.join(jsons_neg[i]['tokens'])\n",
    "                h = ' '.join(jsons_neg[i]['h'][0])\n",
    "                t = ' '.join(jsons_neg[i]['t'][0])\n",
    "\n",
    "                s = ss.replace(h,'@'+jsons_neg[i]['h'][1]+'$')\n",
    "                s = s.replace(t,'@'+jsons_neg[i]['t'][1]+'$')\n",
    "\n",
    "                f.write(str(i)+'\\t'+s+'\\t'+'0'+'\\n')\n",
    "        \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ent={}\n",
    "for t, b in zip(np.unique(entity_unique), enumerate(np.unique(entity_unique))):\n",
    "    dict_ent[t] = b[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/Marked_articles/'\n",
    "dir2 = '/Users/aelitta/Documents/DMMatrix/PAH/готовые файлы tsv'\n",
    "# dir2 = '/Users/aelitta/Documents/DMMatrix/PAH/DownloadToInception/'\n",
    "files = [f.replace('.txt','.tsv') for f in os.listdir(dir_)]+[f.replace('.txt','.tsv') for f in os.listdir(dir2)]\n",
    "files = [f.replace('.tsv','.txt') for f in os.listdir(dir_)]+[f.replace('.tsv','.txt') for f in os.listdir(dir2)]\n",
    "files = np.unique(dfresre['document'])\n",
    "\n",
    "\n",
    "dict_files={}\n",
    "for t, b in zip(np.unique(files), enumerate(np.unique(files))):\n",
    "    dict_files[t] = b[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>we hypothesized that</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>milrinone</td>\n",
       "      <td>Drug</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>, an adenosine</td>\n",
       "      <td>O</td>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                tokens  tags  start  end  seq_id  \\\n",
       "19          19  we hypothesized that     O      0   20       2   \n",
       "20          20             milrinone  Drug     21   30       2   \n",
       "21          21        , an adenosine     O     30   44       2   \n",
       "22          22                     -     O     44   45       2   \n",
       "23          23                     3     O     45   46       2   \n",
       "\n",
       "                        document  \n",
       "19  00000539-200112000-00018.txt  \n",
       "20  00000539-200112000-00018.txt  \n",
       "21  00000539-200112000-00018.txt  \n",
       "22  00000539-200112000-00018.txt  \n",
       "23  00000539-200112000-00018.txt  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00000539-200112000-00018.txt': 1,\n",
       " '00003088-200847090-00004.txt': 2,\n",
       " '000090555.txt': 3,\n",
       " '000107977.txt': 4,\n",
       " '000242498.txt': 5,\n",
       " '0003-4819-141-3-200408030-00005.txt': 6,\n",
       " '0003-4975_2895_2900408-d.txt': 7,\n",
       " '0003319711411704.txt': 8,\n",
       " '000355875.txt': 9,\n",
       " '000446583.txt': 10,\n",
       " '000450759.txt': 11,\n",
       " '00498254.2012.664665.txt': 12,\n",
       " '0091270004270833.txt': 13,\n",
       " '0091270007309708.txt': 14,\n",
       " '0091270008315315.txt': 15,\n",
       " '0091270008319793.txt': 16,\n",
       " '0091270009341182.txt': 17,\n",
       " '0091270009351173.txt': 18,\n",
       " '0091270011398241.txt': 19,\n",
       " '0091270011418656.txt': 20,\n",
       " '01.fjc.0000166207.74178.d0.txt': 21,\n",
       " '01.jcm.0000203850.97890.fe.txt': 22,\n",
       " '01.pdr.0000198772.26417.66.txt': 23,\n",
       " '0253-7613.132158.txt': 24,\n",
       " '030079905x30680.txt': 25,\n",
       " '030079905x46232.txt': 26,\n",
       " '030079906x167390.txt': 27,\n",
       " '03007990903210066.txt': 28,\n",
       " '03007995.2011.605440.txt': 29,\n",
       " '03007995.2012.685930.txt': 30,\n",
       " '09031936.00011308.txt': 31,\n",
       " '09031936.00105914.txt': 32,\n",
       " '09031936.00107012.txt': 33,\n",
       " '09031936.00149011.txt': 34,\n",
       " '09031936.00176312.txt': 35,\n",
       " '09031936.00182909.txt': 36,\n",
       " '09031936.00213911.txt': 37,\n",
       " '09031936.02.02572001.txt': 38,\n",
       " '09031936.04.00028404.txt': 39,\n",
       " '09031936.04.00111904.txt': 40,\n",
       " '09031936.05.00075305.txt': 41,\n",
       " '09031936.06.00030206.txt': 42,\n",
       " '09031936.06.00044406.txt': 43,\n",
       " '09031936.06.00057906.txt': 44,\n",
       " '09031936.06.00095705.txt': 45,\n",
       " '09031936.98.12040932.txt': 46,\n",
       " '0961203307076509.txt': 47,\n",
       " '0967-3334_2F28_2F1_2F001.txt': 48,\n",
       " '1007.full.txt': 49,\n",
       " '1074248411429966.txt': 50,\n",
       " '112972980600700304.txt': 51,\n",
       " '113.full.txt': 52,\n",
       " '114.full.txt': 53,\n",
       " '11534940-000000000-00000.txt': 54,\n",
       " '11539110-000000000-00000.txt': 55,\n",
       " '1303.full.txt': 56,\n",
       " '1343.full.txt': 57,\n",
       " '1354.full.txt': 58,\n",
       " '13993003.00090-2016.txt': 59,\n",
       " '13993003.002762019.txt': 60,\n",
       " '13993003.00364-2015.txt': 61,\n",
       " '13993003.01030-2019.txt': 62,\n",
       " '13993003.01886-2017.txt': 63,\n",
       " '13993003.02004-2018.txt': 64,\n",
       " '13993003.02044-2014.txt': 65,\n",
       " '13993003.02449-2016.txt': 66,\n",
       " '13993003.02493-2016.txt': 67,\n",
       " '1410.full.txt': 68,\n",
       " '1465-9921-10-129.txt': 69,\n",
       " '1465-9921-6-92.txt': 70,\n",
       " '1471-2261-9-15.txt': 71,\n",
       " '1471-2466-11-25.txt': 72,\n",
       " '1471-2466-12-54.txt': 73,\n",
       " '1471-2466-8-3.txt': 74,\n",
       " '1477-7525-11-31.txt': 75,\n",
       " '148.full.txt': 76,\n",
       " '1514.full.txt': 77,\n",
       " '154.full.txt': 78,\n",
       " '15412555.2011.651180.txt': 79,\n",
       " '1602493.full.txt': 80,\n",
       " '162.full.txt': 81,\n",
       " '168.full.txt': 82,\n",
       " '1701214.full.txt': 83,\n",
       " '170134.full.txt': 84,\n",
       " '1702638.full.txt': 85,\n",
       " '1745-6215-14-188.txt': 86,\n",
       " '1745-6215-14-91.txt': 87,\n",
       " '1745-6215-15-486.txt': 88,\n",
       " '1753465808103499.txt': 89,\n",
       " '1755-5922.12008.txt': 90,\n",
       " '190023.full.txt': 91,\n",
       " '2000513.full.txt': 92,\n",
       " '200330.full.txt': 93,\n",
       " '2016-10-revue_litt.txt': 94,\n",
       " '2047-783X-19-2.txt': 95,\n",
       " '218.full.txt': 96,\n",
       " '222.full.txt': 97,\n",
       " '234.full.txt': 98,\n",
       " '267.full.txt': 99,\n",
       " '326.full.txt': 100,\n",
       " '382.full.txt': 101,\n",
       " '405.full.txt': 102,\n",
       " '446.full.txt': 103,\n",
       " '469.full.txt': 104,\n",
       " '498.full.txt': 105,\n",
       " '5-MVD_PAHT_Stepien_JSAP2009.txt': 106,\n",
       " '500.full.txt': 107,\n",
       " '503.full.txt': 108,\n",
       " '5584_2016_18.txt': 109,\n",
       " '630.full.txt': 110,\n",
       " '667.full.txt': 111,\n",
       " '67.full.txt': 112,\n",
       " '691.full.txt': 113,\n",
       " '7.full.txt': 114,\n",
       " '704.full.txt': 115,\n",
       " '72.full.txt': 116,\n",
       " '73_1731.txt': 117,\n",
       " '770.full.txt': 118,\n",
       " '787.full.txt': 119,\n",
       " '944.full.txt': 120,\n",
       " 'Becker-Gruenig_et_al_Efficacy_of_exercise_training.txt': 121,\n",
       " 'CIRCHEARTFAILURE.108.796789.txt': 122,\n",
       " 'CIRCHEARTFAILURE.111.963314.txt': 123,\n",
       " 'CIRCHEARTFAILURE.115.002729.txt': 124,\n",
       " 'CIRCHEARTFAILURE.115.003011.txt': 125,\n",
       " 'CIRCIMAGING.108.780247.txt': 126,\n",
       " 'CIRCRESAHA.114.305951.txt': 127,\n",
       " 'CIRCULATIONAHA.104.473371.txt': 128,\n",
       " 'CIRCULATIONAHA.105.605527.txt': 129,\n",
       " 'CIRCULATIONAHA.107.716373.txt': 130,\n",
       " 'CIRCULATIONAHA.107.742510.txt': 131,\n",
       " 'CIRCULATIONAHA.110.015693.txt': 132,\n",
       " 'COPD.S106480.txt': 133,\n",
       " 'FJC.0000000000000039.txt': 134,\n",
       " 'FJC.0000000000000842.txt': 135,\n",
       " 'NEJMoa012212.txt': 136,\n",
       " 'NEJMoa1209655.txt': 137,\n",
       " 'NEJMoa1503184.txt': 138,\n",
       " 'PCC.0000000000002410.txt': 139,\n",
       " 'PHTN2007.txt': 140,\n",
       " 'Phosphodisesterase inhibitors in Pulmonary hypertension.txt': 141,\n",
       " 'S2213-2600_2814_2970013-X.txt': 142,\n",
       " 'S2213-2600_2816_2930019-4.txt': 143,\n",
       " 'S2213-2600_2816_2930307-1.txt': 144,\n",
       " 'S2213-2600_2820_2930394-5.txt': 145,\n",
       " 'Sutendra-2013-Pulmonary arterial h.txt': 146,\n",
       " 'aca.aca_231_16.txt': 147,\n",
       " 'ajh.21446.txt': 148,\n",
       " 'ajh_2Fhps057.txt': 149,\n",
       " 'ajh_2Fhpt018.txt': 150,\n",
       " 'ajrccm.163.4.2007116.txt': 151,\n",
       " 'annalsats.201410-463oc.txt': 152,\n",
       " 'ar3883.txt': 153,\n",
       " 'ard.2005.048967.txt': 154,\n",
       " 'ard.2007.069609.txt': 155,\n",
       " 'ard.2007.079921.txt': 156,\n",
       " 'art.21433.txt': 157,\n",
       " 'art.22634.txt': 158,\n",
       " 'art.33460.txt': 159,\n",
       " 'art.34614.txt': 160,\n",
       " 'asheducation-2011.1.419.txt': 161,\n",
       " 'asheducation-2017.1.423.txt': 162,\n",
       " 'bcp.12584.txt': 163,\n",
       " 'bcp.12616.txt': 164,\n",
       " 'bcp.13267.txt': 165,\n",
       " 'bcp.14039.txt': 166,\n",
       " 'bf02044111.txt': 167,\n",
       " 'bf03021608.txt': 168,\n",
       " 'biomolecules-10-01261-v3.txt': 169,\n",
       " 'bja_2Faet308.txt': 170,\n",
       " 'blood-2010-09-306167.txt': 171,\n",
       " 'blood-2015-08-618561.txt': 172,\n",
       " 'bloodadvances.2019000883.txt': 173,\n",
       " 'bmjopen-2016-011028.txt': 174,\n",
       " 'bph.14621.txt': 175,\n",
       " 'cc7005.txt': 176,\n",
       " 'chd.12764.txt': 177,\n",
       " 'chest.06-2118.txt': 178,\n",
       " 'chest.06-2690.txt': 179,\n",
       " 'chest.06-2903.txt': 180,\n",
       " 'chest.07-0275.txt': 181,\n",
       " 'chest.07-0592.txt': 182,\n",
       " 'chest.07-0767.txt': 183,\n",
       " 'chest.07-2324.txt': 184,\n",
       " 'chest.07-3035.txt': 185,\n",
       " 'chest.08-1028.txt': 186,\n",
       " 'chest.09-2099.txt': 187,\n",
       " 'chest.10-0969.txt': 188,\n",
       " 'chest.10-1166.txt': 189,\n",
       " 'chest.11-0404.txt': 190,\n",
       " 'chest.11-2212.txt': 191,\n",
       " 'chest.12-3023.txt': 192,\n",
       " 'chest.121.5.1561.txt': 193,\n",
       " 'chest.121.6.1860.txt': 194,\n",
       " 'chest.128.2.709.txt': 195,\n",
       " 'chest.128.4.2368.txt': 196,\n",
       " 'chest.128.4.2599.txt': 197,\n",
       " 'chest.129.3.683.txt': 198,\n",
       " 'chest.129.4.1009.txt': 199,\n",
       " 'chest.129.6.1636.txt': 200,\n",
       " 'chest.13-1766.txt': 201,\n",
       " 'chest.130.5.1471.txt': 202,\n",
       " 'chest.14-0193.txt': 203,\n",
       " 'circinterventions.115.002837.txt': 204,\n",
       " 'circj.69.131.txt': 205,\n",
       " 'circj.69.335.txt': 206,\n",
       " 'circj.72.1142.txt': 207,\n",
       " 'circj.72.1147.txt': 208,\n",
       " 'circj.cj-10-1310.txt': 209,\n",
       " 'circj.cj-11-0473.txt': 210,\n",
       " 'circj.cj-12-0203.txt': 211,\n",
       " 'circj.cj-15-0599.txt': 212,\n",
       " 'circj.cj-16-0254.txt': 213,\n",
       " 'circulationaha.106.618397.txt': 214,\n",
       " 'circulationaha.110.016667.txt': 215,\n",
       " 'circulationaha.111.081125.txt': 216,\n",
       " 'clpt.2009.217.txt': 217,\n",
       " 'clpt.2010.120.txt': 218,\n",
       " 'cnq_08879303_2007_30_1_20.txt': 219,\n",
       " 'copd.s260917.txt': 220,\n",
       " 'cts.12382.txt': 221,\n",
       " 'ehf2.12630.txt': 222,\n",
       " 'ehjci_2Fjeaa285.txt': 223,\n",
       " 'ehm074.txt': 224,\n",
       " 'ejhf.1375.txt': 225,\n",
       " 'ejhf.177.txt': 226,\n",
       " 'eurheartj_2Feht540.txt': 227,\n",
       " 'eurheartj_2Fehu035.txt': 228,\n",
       " 'eurjhf_2Fhfs071.txt': 229,\n",
       " 'eurjhf_2Fhft017.txt': 230,\n",
       " 'fjc.0b013e31802b3184.txt': 231,\n",
       " 'fjc.0b013e31827e0fa9.txt': 232,\n",
       " 'fjc.0b013e31828685da.txt': 233,\n",
       " 'fjc.0b013e3182893d90.txt': 234,\n",
       " 'gut.2005.077453.txt': 235,\n",
       " 'ham.2006.7.54.txt': 236,\n",
       " 'ham.2010.1075.txt': 237,\n",
       " 'ham.2013.1137.txt': 238,\n",
       " 'hc3601.096826.txt': 239,\n",
       " 'hcr.0000000000000117.txt': 240,\n",
       " 'heart.83.4.406.txt': 241,\n",
       " 'heart.86.6.661.txt': 242,\n",
       " 'heartjnl-2016-309621.txt': 243,\n",
       " 'hjh.0b013e3282f382ff.txt': 244,\n",
       " 'hr.2009.113.txt': 245,\n",
       " 'hrt.2004.051961.txt': 246,\n",
       " 'hrt.2006.100388.txt': 247,\n",
       " 'hrt.79.2.175.txt': 248,\n",
       " 'icvts_2Fivv325.txt': 249,\n",
       " 'j.1365-2125.2009.03384.x.txt': 250,\n",
       " 'j.1365-2141.2005.05625.x.txt': 251,\n",
       " 'j.1365-2362.2006.01688.x.txt': 252,\n",
       " 'j.1365-2362.2009.02116.x.txt': 253,\n",
       " 'j.1399-3046.2007.00863.x.txt': 254,\n",
       " 'j.1440-1754.1996.tb00942.x.txt': 255,\n",
       " 'j.1540-8175.2012.01809.x.txt': 256,\n",
       " 'j.1743-6109.2005.20359.x.txt': 257,\n",
       " 'j.1747-0803.2009.00297.x.txt': 258,\n",
       " 'j.1755-5922.2010.00213.x.txt': 259,\n",
       " 'j.1755-5922.2011.00279.x.txt': 260,\n",
       " 'j.1939-1676.2010.0517.x.txt': 261,\n",
       " 'j.ahj.2005.07.005.txt': 262,\n",
       " 'j.ahj.2007.03.005.txt': 263,\n",
       " 'j.amjcard.2005.06.083.txt': 264,\n",
       " 'j.amjcard.2005.07.129.txt': 265,\n",
       " 'j.amjcard.2013.04.051.txt': 266,\n",
       " 'j.cardfail.2009.09.008.txt': 267,\n",
       " 'j.cardfail.2010.10.004.txt': 268,\n",
       " 'j.cardfail.2012.02.004.txt': 269,\n",
       " 'j.cct.2010.12.005.txt': 270,\n",
       " 'j.cct.2011.04.001.txt': 271,\n",
       " 'j.chest.2015.11.005.txt': 272,\n",
       " 'j.chest.2017.04.188.txt': 273,\n",
       " 'j.chest.2018.04.027.txt': 274,\n",
       " 'j.chest.2021.01.066.txt': 275,\n",
       " 'j.clinthera.2013.02.013.txt': 276,\n",
       " 'j.clinthera.2016.03.014.txt': 277,\n",
       " 'j.clinthera.2019.04.007.txt': 278,\n",
       " 'j.clinthera.2019.05.006.txt': 279,\n",
       " 'j.echo.2005.07.010.txt': 280,\n",
       " 'j.echo.2005.07.019.txt': 281,\n",
       " 'j.ejcts.2005.09.007.txt': 282,\n",
       " 'j.ejcts.2010.01.045.txt': 283,\n",
       " 'j.ejim.2008.03.008.txt': 284,\n",
       " 'j.healun.2006.09.016.txt': 285,\n",
       " 'j.healun.2006.10.019.txt': 286,\n",
       " 'j.healun.2006.11.009.txt': 287,\n",
       " 'j.healun.2007.04.013.txt': 288,\n",
       " 'j.healun.2007.07.040.txt': 289,\n",
       " 'j.healun.2008.04.009.txt': 290,\n",
       " 'j.healun.2009.09.005.txt': 291,\n",
       " 'j.healun.2009.09.020.txt': 292,\n",
       " 'j.healun.2010.11.009.txt': 293,\n",
       " 'j.healun.2011.03.011.txt': 294,\n",
       " 'j.healun.2011.08.019.txt': 295,\n",
       " 'j.healun.2012.04.005.txt': 296,\n",
       " 'j.healun.2013.06.008.txt': 297,\n",
       " 'j.healun.2014.01.853.txt': 298,\n",
       " 'j.healun.2014.02.019.txt': 299,\n",
       " 'j.healun.2014.12.001.txt': 300,\n",
       " 'j.healun.2015.05.025.txt': 301,\n",
       " 'j.healun.2016.12.012.txt': 302,\n",
       " 'j.healun.2017.09.024.txt': 303,\n",
       " 'j.healun.2018.07.001.txt': 304,\n",
       " 'j.healun.2018.09.003.txt': 305,\n",
       " 'j.hlc.2012.06.013.txt': 306,\n",
       " 'j.hlc.2017.02.015.txt': 307,\n",
       " 'j.hlc.2018.04.299.txt': 308,\n",
       " 'j.hlc.2018.12.005.txt': 309,\n",
       " 'j.hlc.2019.01.013.txt': 310,\n",
       " 'j.hrtlng.2020.04.006.txt': 311,\n",
       " 'j.ihj.2021.07.007.txt': 312,\n",
       " 'j.ijcard.2004.09.002.txt': 313,\n",
       " 'j.ijcard.2010.10.051.txt': 314,\n",
       " 'j.ijcard.2011.07.009.txt': 315,\n",
       " 'j.ijcard.2012.02.007.txt': 316,\n",
       " 'j.ijcard.2014.09.101.txt': 317,\n",
       " 'j.ijcard.2015.08.080.txt': 318,\n",
       " 'j.ijcard.2017.02.094.txt': 319,\n",
       " 'j.ijcard.2017.04.016.txt': 320,\n",
       " 'j.ijcard.2019.07.004.txt': 321,\n",
       " 'j.jacc.2004.06.060.txt': 322,\n",
       " 'j.jacc.2005.04.050.txt': 323,\n",
       " 'j.jacc.2006.01.057.txt': 324,\n",
       " 'j.jacc.2006.05.070.txt': 325,\n",
       " 'j.jacc.2006.06.062.txt': 326,\n",
       " 'j.jacc.2006.12.037.txt': 327,\n",
       " 'j.jchf.2014.07.013.txt': 328,\n",
       " 'j.jclinane.2005.08.018.txt': 329,\n",
       " 'j.jjcc.2014.01.003.txt': 330,\n",
       " 'j.jjcc.2016.07.002.txt': 331,\n",
       " 'j.jpeds.2012.05.050.txt': 332,\n",
       " 'j.jtcvs.2003.11.035.txt': 333,\n",
       " 'j.jtcvs.2006.08.035.txt': 334,\n",
       " 'j.jvca.2007.10.015.txt': 335,\n",
       " 'j.pupt.2005.09.006.txt': 336,\n",
       " 'j.pupt.2008.07.003.txt': 337,\n",
       " 'j.pupt.2008.11.009.txt': 338,\n",
       " 'j.pupt.2014.04.007.txt': 339,\n",
       " 'j.pupt.2018.02.005.txt': 340,\n",
       " 'j.rmed.2006.12.007.txt': 341,\n",
       " 'j.rmed.2016.06.018.txt': 342,\n",
       " 'j.rmed.2016.11.001.txt': 343,\n",
       " 'j.rmed.2016.11.002.txt': 344,\n",
       " 'j.rmed.2017.03.025.txt': 345,\n",
       " 'j.rmed.2017.05.008.txt': 346,\n",
       " 'j.transproceed.2007.08.069.txt': 347,\n",
       " 'j.tvjl.2014.08.009.txt': 348,\n",
       " 'j.vph.2005.03.003.txt': 349,\n",
       " 'j.vph.2006.01.013.txt': 350,\n",
       " 'j.vph.2021.106840.txt': 351,\n",
       " 'jac_2Fdkw125.txt': 352,\n",
       " 'japplphysiol.00087.2016.txt': 353,\n",
       " 'jcm-06-00043-v2.txt': 354,\n",
       " 'jcph.152.txt': 355,\n",
       " 'jcph.193.txt': 356,\n",
       " 'jcph.639.txt': 357,\n",
       " 'jcph.888.txt': 358,\n",
       " 'jcpt.12085.txt': 359,\n",
       " 'journal.pone.0114309.txt': 360,\n",
       " 'journal.pone.0215146.txt': 361,\n",
       " 'jps.21789.txt': 362,\n",
       " 'jrheum.100358.txt': 363,\n",
       " 'mtc.2003.107.txt': 364,\n",
       " 'nejmoa020204.txt': 365,\n",
       " 'nejmoa050010.txt': 366,\n",
       " 'pah-novinky-bwv.txt': 367,\n",
       " 'pedsPAH.txt': 368,\n",
       " 'ppul.23032.txt': 369,\n",
       " 'ppul.23241.txt': 370,\n",
       " 'psp4.12202.txt': 371,\n",
       " 'ptr.6714.txt': 372,\n",
       " 'rccm.200307-957OC.txt': 373,\n",
       " 'rccm.200308-1142OC.txt': 374,\n",
       " 'rccm.200406-804oc.txt': 375,\n",
       " 'rccm.200410-1411OC.txt': 376,\n",
       " 'rccm.200505-766OC.txt': 377,\n",
       " 'rccm.200603-358OC.txt': 378,\n",
       " 'rccm.200605-694OC.txt': 379,\n",
       " 'rccm.201001-0123oc.txt': 380,\n",
       " 'rccm.201101-0093OC.txt': 381,\n",
       " 'rccm.201411-2061le.txt': 382,\n",
       " 'rccm.201506-1091le.txt': 383,\n",
       " 'rccm.201507-1456OC.txt': 384,\n",
       " 'rccm.201605-1024OC.txt': 385,\n",
       " 'rccm.201704-0789LE.txt': 386,\n",
       " 'rccm.201809-1631LE.txt': 387,\n",
       " 'respcare.05280.txt': 388,\n",
       " 'rheumatology_2Fkep398.txt': 389,\n",
       " 's-0029-1239496.txt': 390,\n",
       " 's0009-9236_2803_2900005-5.txt': 391,\n",
       " 's00134-003-2016-4.txt': 392,\n",
       " 's00134-011-2254-9.txt': 393,\n",
       " 's00228-007-0408-z.txt': 394,\n",
       " 's00246-007-9139-2.txt': 395,\n",
       " 's00246-010-9645-5.txt': 396,\n",
       " 's002469900463.txt': 397,\n",
       " 's00277-018-3518-z.txt': 398,\n",
       " 's00296-004-0439-z.txt': 399,\n",
       " 's00296-006-0222-4.txt': 400,\n",
       " 's00296-008-0789-z.txt': 401,\n",
       " 's00330-012-2606-z.txt': 402,\n",
       " 's00380-009-1176-8.txt': 403,\n",
       " 's00380-014-0544-1.txt': 404,\n",
       " 's00392-005-0266-6.txt': 405,\n",
       " 's00392-007-0490-3.txt': 406,\n",
       " 's00408-013-9542-9.txt': 407,\n",
       " 's00408-014-9667-5.txt': 408,\n",
       " 's00408-020-00402-w.txt': 409,\n",
       " 's00421-010-1799-6.txt': 410,\n",
       " 's00421-011-2085-y.txt': 411,\n",
       " 's0100-879x2008000800003.txt': 412,\n",
       " 's0140-6736_2808_2960919-8.txt': 413,\n",
       " 's0140-6736_2895_2991504-4.txt': 414,\n",
       " 's0735-1097_2802_2901786-2.txt': 415,\n",
       " 's0735-1097_2803_2900121-9.txt': 416,\n",
       " 's0735-1097_2803_2900463-7.txt': 417,\n",
       " 's0735-1097_2803_2900555-2.txt': 418,\n",
       " 's0735-1097_2899_2900312-5.txt': 419,\n",
       " 's0735-1097_2899_2900494-5.txt': 420,\n",
       " 's10157-016-1344-y.txt': 421,\n",
       " 's10456-010-9192-y.txt': 422,\n",
       " 's1047951117000981.txt': 423,\n",
       " 's11748-016-0724-2.txt': 424,\n",
       " 's12098-020-03643-y.txt': 425,\n",
       " 's12325-013-0029-0.txt': 426,\n",
       " 's12872-016-0361-9.txt': 427,\n",
       " 's12872-017-0569-3.txt': 428,\n",
       " 's12872-017-0674-3.txt': 429,\n",
       " 's13318-017-0424-z.txt': 430,\n",
       " 's2213-2600_2820_2930532-4.txt': 431,\n",
       " 's40256-014-0081-4.txt': 432,\n",
       " 's40256-015-0117-4.txt': 433,\n",
       " 's40256-017-0262-z.txt': 434,\n",
       " 's40261-014-0207-0.txt': 435,\n",
       " 's40262-013-0063-8.txt': 436,\n",
       " 's41598-021-97396-z.txt': 437,\n",
       " 'smw_147_w14462.txt': 438,\n",
       " 'thoraxjnl-2013-204150.txt': 439,\n",
       " 'thx.2005.041954.txt': 440,\n",
       " 'v058p00797.txt': 441,\n",
       " 'v10007-012-0027-9.txt': 442}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jsons) #[970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для BERT+ linear classifier (ClinicalTransformerRE) - SEMVEAL\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "path = 'Documents/DMMatrix/PAH/CRE/v4/'\n",
    "train_size = int(len(jsons)/5)*4\n",
    "\n",
    "\n",
    "train_types = []\n",
    "with open(path+'train.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(train_size):\n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "        h_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['t_id'])\n",
    "        k=0\n",
    "        while k<len(jsons[i]['tokens']):\n",
    "#             if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons[i]['h'][2][0][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons[i]['h'][0])\n",
    "            else:\n",
    "                ss1.append(jsons[i]['tokens'][k])\n",
    "                k+=1\n",
    "        k=0\n",
    "        while k <len(jsons[i]['tokens']):\n",
    "#             if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons[i]['t'][2][0][0]:\n",
    "                ss2.append('[s2]'+' '.join(jsons[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons[i]['t'][0])\n",
    "            else:\n",
    "                ss2.append(jsons[i]['tokens'][k])\n",
    "                k+=1\n",
    "        type_ = min(jsons[i]['h'][1],jsons[i]['t'][1])+'_'+max(jsons[i]['h'][1],jsons[i]['t'][1])\n",
    "        \n",
    "#         if type_.lower().find('rate')>=0:\n",
    "#             type_='cohort_effect'\n",
    "#         elif type_.lower().find('duration')>=0:\n",
    "#             type_='duration'\n",
    "#         else:\n",
    "#             type_ = 'cohort'\n",
    "        if type_ not in train_types:\n",
    "            train_types.append(type_)        \n",
    "        line = type_ +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons[i]['h'][1] +'\\t'+ jsons[i]['t'][1] +'\\t'+\\\n",
    "        'T_'+h_id+'\\t'+'T_'+t_id+str(i)+'\\t'+'F_'+str(dict_files[jsons[i]['filename']])\n",
    "#         print(i, line)\n",
    "        if type_ in dev_types:\n",
    "            f.write(line+'\\n')\n",
    "        \n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons_neg_train[i]['h'][0])\n",
    "        t = ' '.join(jsons_neg_train[i]['t'][0])\n",
    "        h_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['t_id'])\n",
    "        k=0\n",
    "        while k<len(jsons_neg_train[i]['tokens']):\n",
    "#             if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons_neg_train[i]['h'][2][0][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons_neg_train[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons_neg_train[i]['h'][0])\n",
    "            else:\n",
    "                ss1.append(jsons_neg_train[i]['tokens'][k])\n",
    "                k+=1\n",
    "        k=0\n",
    "        while k <len(jsons_neg_train[i]['tokens']):\n",
    "            if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "                break\n",
    "            if str(k+1) == jsons_neg_train[i]['t'][2][0][0]:\n",
    "                ss2.append('[s2]'+' '.join(jsons_neg_train[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons_neg_train[i]['t'][0])\n",
    "            else:\n",
    "                ss2.append(jsons_neg_train[i]['tokens'][k])\n",
    "                k+=1        \n",
    "\n",
    "        if i%3==0:        \n",
    "            line = 'NotRel' +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons_neg_train[i]['h'][1] +'\\t'+ jsons_neg_train[i]['t'][1] +'\\t'+\\\n",
    "            'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg_train[i]['filename']])\n",
    "    #         print(i, line)\n",
    "            f.write(line+'\\n')\n",
    "\n",
    "\n",
    "f.close()\n",
    "dev_types = []\n",
    "with open(path+'dev.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(len(jsons)-train_size):\n",
    "        if i<train_size:\n",
    "            i = i+train_size\n",
    "            ss1 = []\n",
    "            ss2 = []\n",
    "            h = ' '.join(jsons[i]['h'][0])\n",
    "            t = ' '.join(jsons[i]['t'][0])\n",
    "            h_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['h_id'])\n",
    "            t_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['t_id'])\n",
    "            k=0\n",
    "            while k<len(jsons[i]['tokens']):\n",
    "#                 if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                     break\n",
    "                if str(k+1) == jsons[i]['h'][2][0][0]:\n",
    "                    ss1.append('[s1]'+' '.join(jsons[i]['h'][0])+'[e1]')\n",
    "                    k = k+len(jsons[i]['h'][0])\n",
    "                else:\n",
    "                    ss1.append(jsons[i]['tokens'][k])\n",
    "                    k+=1\n",
    "            k=0\n",
    "            while k <len(jsons[i]['tokens']):\n",
    "                if jsons[i]['tokens'][k].find('.')==0:\n",
    "                    break\n",
    "                if str(k+1) == jsons[i]['t'][2][0][0]:\n",
    "                    ss2.append('[s2]'+' '.join(jsons[i]['t'][0])+'[e2]')\n",
    "                    k = k+len(jsons[i]['t'][0])\n",
    "                else:\n",
    "                    ss2.append(jsons[i]['tokens'][k])\n",
    "                    k+=1\n",
    "            type_ = min(jsons[i]['h'][1],jsons[i]['t'][1])+'_'+max(jsons[i]['h'][1],jsons[i]['t'][1])\n",
    "            \n",
    "#             if type_.lower().find('rate')>=0:\n",
    "#                 type_='cohort_effect'\n",
    "#             elif type_.lower().find('duration')>=0:\n",
    "#                 type_='duration'\n",
    "#             else:\n",
    "#                 type_ = 'cohort'\n",
    "\n",
    "            line = type_ +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons[i]['h'][1] +'\\t'+ jsons[i]['t'][1] +'\\t'+\\\n",
    "            'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons[i]['filename']])\n",
    "    #         print(i, line)\n",
    "            if type_ in train_types:\n",
    "                f.write(line+'\\n')\n",
    "\n",
    "            ss1 = []\n",
    "            ss2 = []\n",
    "            h = ' '.join(jsons_neg_train[i]['h'][0])\n",
    "            t = ' '.join(jsons_neg_train[i]['t'][0])\n",
    "            h_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['h_id'])\n",
    "            t_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['t_id'])\n",
    "            k=0\n",
    "            while k<len(jsons_neg_train[i]['tokens']):\n",
    "                if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "                    break\n",
    "                if str(k+1) == jsons_neg_train[i]['h'][2][0][0]:\n",
    "                    ss1.append('[s1]'+' '.join(jsons_neg_train[i]['h'][0])+'[e1]')\n",
    "                    k = k+len(jsons_neg_train[i]['h'][0])\n",
    "                else:\n",
    "                    ss1.append(jsons_neg_train[i]['tokens'][k])\n",
    "                    k+=1\n",
    "            k=0\n",
    "            while k <len(jsons_neg_train[i]['tokens']):\n",
    "                if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "                    break\n",
    "                if str(k+1) == jsons_neg_train[i]['t'][2][0][0]:\n",
    "                    ss2.append('[s2]'+' '.join(jsons_neg_train[i]['t'][0])+'[e2]')\n",
    "                    k = k+len(jsons_neg_train[i]['t'][0])\n",
    "                else:\n",
    "                    ss2.append(jsons_neg_train[i]['tokens'][k])\n",
    "                    k+=1  \n",
    "                    \n",
    "            if i%3==0:\n",
    "                line = 'NotRel' +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons_neg_train[i]['h'][1] +'\\t'+ jsons_neg_train[i]['t'][1] +'\\t'+\\\n",
    "                'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg_train[i]['filename']])\n",
    "        #         print(i, line)\n",
    "                f.write(line+'\\n')\n",
    "\n",
    "            if type_ not in dev_types:\n",
    "                dev_types.append(type_)\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Documents/DMMatrix/PAH/CRE/v5/'\n",
    "train_size = int(len(jsons)/5)*4\n",
    "\n",
    "train_types = []\n",
    "with open(path+'train.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(train_size):\n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        h_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['t_id'])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "        k=0\n",
    "        while k<len(jsons[i]['tokens']):\n",
    "#             if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons_neg[i]['h'][2][0][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons[i]['h'][0])\n",
    "            else:\n",
    "                ss1.append(jsons[i]['tokens'][k])\n",
    "                k+=1\n",
    "        k=0\n",
    "        while k <len(jsons[i]['tokens']):\n",
    "#             if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons[i]['t'][2][0][0]:\n",
    "                ss2.append('[s2]'+' '.join(jsons[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons[i]['t'][0])\n",
    "            else:\n",
    "                ss2.append(jsons[i]['tokens'][k])\n",
    "                k+=1\n",
    "        type_ = min(jsons[i]['h'][1],jsons[i]['t'][1])+'_'+max(jsons[i]['h'][1],jsons[i]['t'][1])\n",
    "        \n",
    "#         if type_.lower().find('rate')>=0:\n",
    "#             type_='cohort_effect'\n",
    "#         elif type_.lower().find('duration')>=0:\n",
    "#             type_='duration'\n",
    "#         else:\n",
    "#             type_ = 'cohort'\n",
    "        if type_ not in train_types:\n",
    "            train_types.append(type_)        \n",
    "        line = 'Rel' +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons[i]['h'][1] +'\\t'+ jsons[i]['t'][1] +'\\t'+\\\n",
    "        'T_'+h_id+'\\t'+'T_'+t_id+str(i)+'\\t'+'F_'+str(dict_files[jsons[i]['filename']])\n",
    "#         print(i, line)\n",
    "        if type_ in train_types:\n",
    "            f.write(line+'\\n')\n",
    "        \n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        h_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['t_id'])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "        k=0\n",
    "        while k<len(jsons_neg_train[i]['tokens']):\n",
    "#             if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons_neg_train[i]['h'][2][0][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons_neg_train[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons_neg_train[i]['h'][0])\n",
    "            else:\n",
    "                ss1.append(jsons_neg_train[i]['tokens'][k])\n",
    "                k+=1\n",
    "        k=0\n",
    "        while k <len(jsons_neg_train[i]['tokens']):\n",
    "            if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "                break\n",
    "            if str(k+1) == jsons_neg_train[i]['t'][2][0][0]:\n",
    "                ss2.append('[s2]'+' '.join(jsons_neg_train[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons_neg_train[i]['t'][0])\n",
    "            else:\n",
    "                ss2.append(jsons_neg_train[i]['tokens'][k])\n",
    "                k+=1        \n",
    "\n",
    "        if i%3==0:        \n",
    "            line = 'NotRel' +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons_neg_train[i]['h'][1] +'\\t'+ jsons_neg_train[i]['t'][1] +'\\t'+\\\n",
    "            'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg_train[i]['filename']])\n",
    "    #         print(i, line)\n",
    "            f.write(line+'\\n')\n",
    "\n",
    "\n",
    "f.close()\n",
    "dev_types = []\n",
    "with open(path+'dev.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(len(jsons)-train_size):\n",
    "        if i<train_size:\n",
    "            i = i+train_size\n",
    "            ss1 = []\n",
    "            ss2 = []\n",
    "            h = ' '.join(jsons[i]['h'][0])\n",
    "            t = ' '.join(jsons[i]['t'][0])\n",
    "            h_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['h_id'])\n",
    "            t_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['t_id'])\n",
    "            k=0\n",
    "            while k<len(jsons[i]['tokens']):\n",
    "#                 if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                     break\n",
    "                if str(k+1) == jsons[i]['h'][2][0][0]:\n",
    "                    ss1.append('[s1]'+' '.join(jsons[i]['h'][0])+'[e1]')\n",
    "                    k = k+len(jsons[i]['h'][0])\n",
    "                else:\n",
    "                    ss1.append(jsons[i]['tokens'][k])\n",
    "                    k+=1\n",
    "            k=0\n",
    "            while k <len(jsons[i]['tokens']):\n",
    "                if jsons[i]['tokens'][k].find('.')==0:\n",
    "                    break\n",
    "                if str(k+1) == jsons[i]['t'][2][0][0]:\n",
    "                    ss2.append('[s2]'+' '.join(jsons[i]['t'][0])+'[e2]')\n",
    "                    k = k+len(jsons[i]['t'][0])\n",
    "                else:\n",
    "                    ss2.append(jsons[i]['tokens'][k])\n",
    "                    k+=1\n",
    "            type_ = min(jsons[i]['h'][1],jsons[i]['t'][1])+'_'+max(jsons[i]['h'][1],jsons[i]['t'][1])\n",
    "            \n",
    "#             if type_.lower().find('rate')>=0:\n",
    "#                 type_='cohort_effect'\n",
    "#             elif type_.lower().find('duration')>=0:\n",
    "#                 type_='duration'\n",
    "#             else:\n",
    "#                 type_ = 'cohort'\n",
    "\n",
    "            line = 'Rel' +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons[i]['h'][1] +'\\t'+ jsons[i]['t'][1] +'\\t'+\\\n",
    "            'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons[i]['filename']])\n",
    "    #         print(i, line)\n",
    "            if type_ in train_types:\n",
    "                f.write(line+'\\n')\n",
    "\n",
    "            ss1 = []\n",
    "            ss2 = []\n",
    "            h = ' '.join(jsons[i]['h'][0])\n",
    "            t = ' '.join(jsons[i]['t'][0])\n",
    "            h_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['h_id'])\n",
    "            t_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['t_id'])\n",
    "            k=0\n",
    "            while k<len(jsons_neg_train[i]['tokens']):\n",
    "                if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "                    break\n",
    "                if str(k+1) == jsons_neg_train[i]['h'][2][0][0]:\n",
    "                    ss1.append('[s1]'+' '.join(jsons_neg_train[i]['h'][0])+'[e1]')\n",
    "                    k = k+len(jsons_neg_train[i]['h'][0])\n",
    "                else:\n",
    "                    ss1.append(jsons_neg_train[i]['tokens'][k])\n",
    "                    k+=1\n",
    "            k=0\n",
    "            while k <len(jsons_neg_train[i]['tokens']):\n",
    "                if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "                    break\n",
    "                if str(k+1) == jsons_neg_train[i]['t'][2][0][0]:\n",
    "                    ss2.append('[s2]'+' '.join(jsons_neg_train[i]['t'][0])+'[e2]')\n",
    "                    k = k+len(jsons_neg_train[i]['t'][0])\n",
    "                else:\n",
    "                    ss2.append(jsons_neg_train[i]['tokens'][k])\n",
    "                    k+=1  \n",
    "                    \n",
    "            if i%3==0:\n",
    "                line = 'NotRel' +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons_neg_train[i]['h'][1] +'\\t'+ jsons_neg_train[i]['t'][1] +'\\t'+\\\n",
    "                'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg_train[i]['filename']])\n",
    "        #         print(i, line)\n",
    "                f.write(line+'\\n')\n",
    "\n",
    "            if type_ not in dev_types:\n",
    "                dev_types.append(type_)\n",
    " \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Documents/DMMatrix/PAH/CRE/v6_fs/'\n",
    "train_size = int(len(jsons)/5)*4\n",
    "\n",
    "train_types = []\n",
    "with open(path+'train_fs.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(train_size):\n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        h_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['t_id'])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "        k=0\n",
    "        while k<len(jsons[i]['tokens']):\n",
    "#             if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons_neg[i]['h'][2][0][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons[i]['h'][0])\n",
    "            elif str(k+1) == jsons[i]['t'][2][0][0]:\n",
    "                ss1.append('[s2]'+' '.join(jsons[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons[i]['t'][0])\n",
    "            else:\n",
    "                ss1.append(jsons[i]['tokens'][k])\n",
    "                k+=1\n",
    "        type_ = min(jsons[i]['h'][1],jsons[i]['t'][1])+'_'+max(jsons[i]['h'][1],jsons[i]['t'][1])\n",
    "        \n",
    "#         if type_.lower().find('rate')>=0:\n",
    "#             type_='cohort_effect'\n",
    "#         elif type_.lower().find('duration')>=0:\n",
    "#             type_='duration'\n",
    "#         else:\n",
    "#             type_ = 'cohort'\n",
    "        if type_ not in train_types:\n",
    "            train_types.append(type_)        \n",
    "        line = 'Rel' +'\\t' + ' '.join(ss1) +'\\t'+ jsons[i]['h'][1] +'\\t'+ jsons[i]['t'][1] +'\\t'+\\\n",
    "        'T_'+h_id+'\\t'+'T_'+t_id+str(i)+'\\t'+'F_'+str(dict_files[jsons[i]['filename']])\n",
    "#         print(i, line)\n",
    "        if type_ in train_types:\n",
    "            f.write(line+'\\n')\n",
    "        \n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons[i]['h'][0])\n",
    "        h_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['t_id'])\n",
    "        t = ' '.join(jsons[i]['t'][0])\n",
    "        k=0\n",
    "        while k<len(jsons_neg_train[i]['tokens']):\n",
    "#             if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons_neg_train[i]['h'][2][0][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons_neg_train[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons_neg_train[i]['h'][0])\n",
    "            elif str(k+1) == jsons_neg_train[i]['t'][2][0][0]:\n",
    "                ss1.append('[s2]'+' '.join(jsons_neg_train[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons_neg_train[i]['t'][0])\n",
    "            else:\n",
    "                ss1.append(jsons_neg_train[i]['tokens'][k])\n",
    "                k+=1\n",
    "        if i%3==0:        \n",
    "            line = 'NotRel' +'\\t' + ' '.join(ss1) +'\\t'+ jsons_neg_train[i]['h'][1] +'\\t'+ jsons_neg_train[i]['t'][1] +'\\t'+\\\n",
    "            'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg_train[i]['filename']])\n",
    "    #         print(i, line)\n",
    "            f.write(line+'\\n')\n",
    "\n",
    "\n",
    "f.close()\n",
    "dev_types = []\n",
    "with open(path+'dev_fs.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(len(jsons)-train_size):\n",
    "        if i<train_size:\n",
    "            i = i+train_size\n",
    "            ss1 = []\n",
    "            ss2 = []\n",
    "            h = ' '.join(jsons[i]['h'][0])\n",
    "            t = ' '.join(jsons[i]['t'][0])\n",
    "            h_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['h_id'])\n",
    "            t_id = str(dict_files[jsons[i]['filename']])+str(jsons[i]['t_id'])\n",
    "            k=0\n",
    "            while k<len(jsons[i]['tokens']):\n",
    "#                 if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                     break\n",
    "                if str(k+1) == jsons[i]['h'][2][0][0]:\n",
    "                    ss1.append('[s1]'+' '.join(jsons[i]['h'][0])+'[e1]')\n",
    "                    k = k+len(jsons[i]['h'][0])\n",
    "                elif str(k+1) == jsons[i]['t'][2][0][0]:\n",
    "                    ss1.append('[s2]'+' '.join(jsons[i]['t'][0])+'[e2]')\n",
    "                    k = k+len(jsons[i]['t'][0])\n",
    "                else:\n",
    "                    ss1.append(jsons[i]['tokens'][k])\n",
    "                    k+=1\n",
    "            type_ = min(jsons[i]['h'][1],jsons[i]['t'][1])+'_'+max(jsons[i]['h'][1],jsons[i]['t'][1])\n",
    "            \n",
    "#             if type_.lower().find('rate')>=0:\n",
    "#                 type_='cohort_effect'\n",
    "#             elif type_.lower().find('duration')>=0:\n",
    "#                 type_='duration'\n",
    "#             else:\n",
    "#                 type_ = 'cohort'\n",
    "\n",
    "            line = 'Rel' +'\\t' + ' '.join(ss1) +'\\t'+ jsons[i]['h'][1] +'\\t'+ jsons[i]['t'][1] +'\\t'+\\\n",
    "            'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons[i]['filename']])\n",
    "    #         print(i, line)\n",
    "            if type_ in train_types:\n",
    "                f.write(line+'\\n')\n",
    "\n",
    "            ss1 = []\n",
    "            ss2 = []\n",
    "            h = ' '.join(jsons[i]['h'][0])\n",
    "            t = ' '.join(jsons[i]['t'][0])\n",
    "            h_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['h_id'])\n",
    "            t_id = str(dict_files[jsons_neg_train[i]['filename']])+str(jsons_neg_train[i]['t_id'])\n",
    "            k=0\n",
    "            while k<len(jsons_neg_train[i]['tokens']):\n",
    "                if jsons_neg_train[i]['tokens'][k].find('.')==0:\n",
    "                    break\n",
    "                if str(k+1) == jsons_neg_train[i]['h'][2][0][0]:\n",
    "                    ss1.append('[s1]'+' '.join(jsons_neg_train[i]['h'][0])+'[e1]')\n",
    "                    k = k+len(jsons_neg_train[i]['h'][0])\n",
    "                elif str(k+1) == jsons_neg_train[i]['t'][2][0][0]:\n",
    "                    ss1.append('[s2]'+' '.join(jsons_neg_train[i]['t'][0])+'[e2]')\n",
    "                    k = k+len(jsons_neg_train[i]['t'][0])\n",
    "                else:\n",
    "                    ss1.append(jsons_neg_train[i]['tokens'][k])\n",
    "                    k+=1\n",
    "                    \n",
    "            if i%3==0:\n",
    "                line = 'NotRel' +'\\t' + ' '.join(ss1) +'\\t'+ jsons_neg_train[i]['h'][1] +'\\t'+ jsons_neg_train[i]['t'][1] +'\\t'+\\\n",
    "                'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg_train[i]['filename']])\n",
    "        #         print(i, line)\n",
    "                f.write(line+'\\n')\n",
    "\n",
    "            if type_ not in dev_types:\n",
    "                dev_types.append(type_)\n",
    " \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dosage_drug',\n",
       " 'drug_duration',\n",
       " '6MWD_drug',\n",
       " 'drug_reason',\n",
       " 'FC_reason',\n",
       " '6MWD_dosage',\n",
       " 'FC_drug',\n",
       " 'death\\\\_rate_drug',\n",
       " 'duration_ntproBPN',\n",
       " '6MWD_duration',\n",
       " 'death\\\\_rate_duration',\n",
       " '6mwd_drug',\n",
       " 'drug_hospitalization',\n",
       " 'dosage_duration']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>selective</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pulmonary vasodilation</td>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>is an advantageous therapeutic</td>\n",
       "      <td>O</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>strategy</td>\n",
       "      <td>O</td>\n",
       "      <td>64</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          tokens tags  start  end  seq_id  \\\n",
       "0           0                             NaN   -1     -1   -1      -1   \n",
       "1           1                       selective    O      0    9       1   \n",
       "2           2          pulmonary vasodilation    O     10   32       1   \n",
       "3           3  is an advantageous therapeutic    O     33   63       1   \n",
       "4           4                        strategy    O     64   72       1   \n",
       "\n",
       "                       document  \n",
       "0                            -1  \n",
       "1  00000539-200112000-00018.txt  \n",
       "2  00000539-200112000-00018.txt  \n",
       "3  00000539-200112000-00018.txt  \n",
       "4  00000539-200112000-00018.txt  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresre = pd.read_csv('Documents/DMMatrix/PAH/CRE/spert/Блендинг объединеные токены.csv', delimiter = '\\t')\n",
    "dfresre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_id  Unnamed: 0  tokens  tags  start  end  document\n",
       "0      -1           1       0     1      1    1         1\n",
       "1       2           4       4     4      4    4         4\n",
       "2       3           8       8     8      8    8         8\n",
       "3       4           5       5     5      5    5         5\n",
       "4       5           7       7     7      7    7         7"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresrecnt = dfresre[dfresre['tags']!='O'].groupby('seq_id').count().reset_index()\n",
    "dfresrecnt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211662, 7)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresre = dfresre[dfresre['seq_id'].isin(dfresrecnt[dfresrecnt['document']>1]['seq_id'])]\n",
    "dfresre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresre['document'].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>419208</th>\n",
       "      <td>419208</td>\n",
       "      <td>51 % at</td>\n",
       "      <td>percent_rate</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>60681</td>\n",
       "      <td>rheumatology_2Fkep398.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421098</th>\n",
       "      <td>421098</td>\n",
       "      <td>51 % at</td>\n",
       "      <td>percent_rate</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>60938</td>\n",
       "      <td>rheumatology_2Fkep398.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   tokens          tags  start  end  seq_id  \\\n",
       "419208      419208  51 % at  percent_rate     43   49   60681   \n",
       "421098      421098  51 % at  percent_rate     31   37   60938   \n",
       "\n",
       "                         document  \n",
       "419208  rheumatology_2Fkep398.txt  \n",
       "421098  rheumatology_2Fkep398.txt  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresre[(dfresre['document']=='rheumatology_2Fkep398.txt')&(dfresre['tokens']=='51 % at')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': [['78', 'mg'], 'Strength', ['13', '14']],\n",
       " 't': [['to'], 'Frequency', ['19']],\n",
       " 'tokens': 'it was also observed that increasing the concentration of pharmaburst from 58 to 78 mg and ssg from 10 to 25 mg enhanced dissolution ( s5 vs. s7 , s6 vs. s8 and s2 vs. s4 ) .',\n",
       " 'h_id': 19560,\n",
       " 't_id': 19582,\n",
       " 'filename': 'v10007-012-0027-9.txt'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_neg[149969]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the mean plasma concentration versus time curves of sild following administration of sd sublingual tablets ( s7 ) , lyophylized sublingual tablets ( f4 ) and the commercial oral tablets ( revatio ) to human volunteers are shown in fig .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(dfresre[dfresre['seq_id']==69348]['tokens']).find('lyophylized sublingual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sublingual</td>\n",
       "      <td>Drug</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lyophylized sublingual</td>\n",
       "      <td>Drug</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oral</td>\n",
       "      <td>Route</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tokens   tags  start\n",
       "1              sublingual   Drug     88\n",
       "3  lyophylized sublingual   Drug    116\n",
       "5                    oral  Route    173"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'h': [['sublingual'], 'Drug', ['13']],\n",
       "  't': [['lyophylized', 'sublingual'], 'Drug', ['19', '20']],\n",
       "  'tokens': 'the mean plasma concentration versus time curves of sild following administration of sd sublingual tablets ( s7 ) , lyophylized sublingual tablets ( f4 ) and the commercial oral tablets ( revatio ) to human volunteers are shown in fig .',\n",
       "  'h_id': 19571,\n",
       "  't_id': 19599,\n",
       "  'filename': 'v10007-012-0027-9.txt'},\n",
       " {'h': [['sublingual'], 'Drug', ['13']],\n",
       "  't': [['oral'], 'Route', ['28']],\n",
       "  'tokens': 'the mean plasma concentration versus time curves of sild following administration of sd sublingual tablets ( s7 ) , lyophylized sublingual tablets ( f4 ) and the commercial oral tablets ( revatio ) to human volunteers are shown in fig .',\n",
       "  'h_id': 19571,\n",
       "  't_id': 19656,\n",
       "  'filename': 'v10007-012-0027-9.txt'},\n",
       " {'h': [['lyophylized', 'sublingual'], 'Drug', ['19', '20']],\n",
       "  't': [['oral'], 'Route', ['28']],\n",
       "  'tokens': 'the mean plasma concentration versus time curves of sild following administration of sd sublingual tablets ( s7 ) , lyophylized sublingual tablets ( f4 ) and the commercial oral tablets ( revatio ) to human volunteers are shown in fig .',\n",
       "  'h_id': 19599,\n",
       "  't_id': 19656,\n",
       "  'filename': 'v10007-012-0027-9.txt'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ' '.join(dfresre[dfresre['seq_id']==69348]['tokens'])\n",
    "json_neg_test = []\n",
    "seq_id = 69348\n",
    "# starts = dfresre[(dfresre['seq_id']==seq_id)&(dfresre['tags']!='O')]['start']\n",
    "starts = []\n",
    "strt = 0\n",
    "tokens = list(dfresre[dfresre['seq_id']==69348]['tokens'])\n",
    "tags = list(dfresre[dfresre['seq_id']==69348]['tags'])\n",
    "for token in tokens:\n",
    "    starts.append(sent[strt:].find(token)+strt)\n",
    "    strt+=len(token)+1\n",
    "dc = pd.concat([pd.Series(tokens), pd.Series(tags), pd.Series(starts)], axis = 1)\n",
    "dc.columns = ['tokens','tags','start']\n",
    "dc = dc[dc['tags']!='O']\n",
    "starts = dc[dc['tags']!='O']['start']\n",
    "for tkn1s in starts:\n",
    "    df1 = dc[(dc['start']==tkn1s)].copy()\n",
    "    token1 = list(df1['tokens'])[0]\n",
    "    pos1 = sent[max(tkn1s-len(token1),0):tkn1s+len(token1)+10].find(token1)+max(tkn1s-len(token1),0)\n",
    "    s1 = len(sent[:pos1].split())\n",
    "    for tkn2s in starts:\n",
    "        if tkn1s<tkn2s: ##только последующие\n",
    "            df2 = dc[dc['start']==tkn2s].copy()\n",
    "            token2 = list(df2['tokens'])[0]\n",
    "            pos2 = sent[max(tkn2s-len(token2),0):tkn2s+len(token2)+10].find(token2)+max(tkn2s-len(token2),0)\n",
    "            s2 = len(sent[:pos2].split())\n",
    "            if tkn1s < tkn2s and list(df1['tokens'])[0]!=list(df2['tokens'])[0]:\n",
    "                 json_neg_test.append({\n",
    "                       'h': [token1.split(' '), list(df1['tags'])[0], [str(y) for y in range(s1,s1+len(token1.split(' ')))]],\n",
    "                       't': [token2.split(' '), list(df2['tags'])[0], [str(y) for y in range(s2,s2+len(token2.split(' ')))]],\n",
    "                        'tokens': sent,\n",
    "                        'h_id': cnt+tkn1s,\n",
    "                        't_id': cnt+tkn2s,\n",
    "                        'filename':filename\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfresre[(dfresre['seq_id']==seq_id)&(dfresre['start']==88)].copy()\n",
    "df2 = dfresre[(dfresre['seq_id']==seq_id)&(dfresre['start']==113)].copy()\n",
    "token1 = list(df1['tokens'])[0]\n",
    "token2 = list(df2['tokens'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sublingual'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[max(113-len(token2),0):113+len(token2)+5].find(token2)+max(113-len(token2),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lingual tablets ( s7 ) , lyophylized subling'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[max(113-len(token2),0):113+len(token2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the mean plasma concentration versus time curves of sild following administration of sd sublingual tablets ( s7 ) , lyophylized sublingual tablets ( f4 ) and the commercial oral tablets ( revatio ) to human volunteers are shown in fig .'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.find('lyophylized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'h': [['sublingual'], 'Drug', ['13']],\n",
       "  't': [['lyophylized', 'sublingual'], 'Drug', ['19', '20']],\n",
       "  'tokens': 'the mean plasma concentration versus time curves of sild following administration of sd sublingual tablets ( s7 ) , lyophylized sublingual tablets ( f4 ) and the commercial oral tablets ( revatio ) to human volunteers are shown in fig .',\n",
       "  'h_id': 19571,\n",
       "  't_id': 19596,\n",
       "  'filename': 'v10007-012-0027-9.txt'},\n",
       " {'h': [['sublingual'], 'Drug', ['13']],\n",
       "  't': [['oral'], 'Route', ['28']],\n",
       "  'tokens': 'the mean plasma concentration versus time curves of sild following administration of sd sublingual tablets ( s7 ) , lyophylized sublingual tablets ( f4 ) and the commercial oral tablets ( revatio ) to human volunteers are shown in fig .',\n",
       "  'h_id': 19571,\n",
       "  't_id': 19651,\n",
       "  'filename': 'v10007-012-0027-9.txt'},\n",
       " {'h': [['lyophylized', 'sublingual'], 'Drug', ['19', '20']],\n",
       "  't': [['oral'], 'Route', ['28']],\n",
       "  'tokens': 'the mean plasma concentration versus time curves of sild following administration of sd sublingual tablets ( s7 ) , lyophylized sublingual tablets ( f4 ) and the commercial oral tablets ( revatio ) to human volunteers are shown in fig .',\n",
       "  'h_id': 19596,\n",
       "  't_id': 19651,\n",
       "  'filename': 'v10007-012-0027-9.txt'}]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [10:40<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "##for SEMVEAL\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "json_neg = []\n",
    "cnt = 0\n",
    "for filename in tqdm(dfresre['document'].unique()):\n",
    "    for seq_id in dfresre.loc[dfresre['document']==filename,'seq_id'].unique():\n",
    "        sent = ' '.join(dfresre[dfresre['seq_id']==seq_id]['tokens'])\n",
    "        starts = dfresre[(dfresre['seq_id']==seq_id)&(dfresre['tags']!='O')]['start']\n",
    "        for tkn1s in starts:\n",
    "            df1 = dfresre[(dfresre['seq_id']==seq_id)&(dfresre['start']==tkn1s)].copy()\n",
    "            token1 = list(df1['tokens'])[0]\n",
    "            pos1 = sent[max(tkn1s-len(token1),0):tkn1s+len(token1)+10].find(token1)+max(tkn1s-len(token1),0)\n",
    "            s1 = len(sent[:pos1].split())\n",
    "            for tkn2s in starts:\n",
    "                if tkn1s<tkn2s: ##только последующие\n",
    "                    df2 = dfresre[(dfresre['seq_id']==seq_id)&(dfresre['start']==tkn2s)].copy()\n",
    "                    token2 = list(df2['tokens'])[0]\n",
    "                    pos2 = sent[max(tkn2s-len(token2),0):tkn2s+len(token2)+10].find(token2)+max(tkn2s-len(token2),0)\n",
    "                    s2 = len(sent[:pos2].split())\n",
    "                    if tkn1s < tkn2s and list(df1['tokens'])[0]!=list(df2['tokens'])[0]:\n",
    "                         json_neg.append({\n",
    "                               'h': [token1.split(' '), list(df1['tags'])[0], [str(y) for y in range(s1,s1+len(token1.split(' ')))]],\n",
    "                               't': [token2.split(' '), list(df2['tags'])[0], [str(y) for y in range(s2,s2+len(token2.split(' ')))]],\n",
    "                                'tokens': sent,\n",
    "                                'h_id': cnt+tkn1s,\n",
    "                                't_id': cnt+tkn2s,\n",
    "                                'filename':filename\n",
    "                            })\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [05:11<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "json_neg = []\n",
    "cnt = 0\n",
    "for filename in tqdm(dfresre['document'].unique()):\n",
    "    for seq_id in dfresre.loc[dfresre['document']==filename,'seq_id'].unique():\n",
    "        starts = []\n",
    "        strt = 0\n",
    "        tokens = list(dfresre[dfresre['seq_id']==seq_id]['tokens'])\n",
    "        sent = ' '.join(tokens)\n",
    "        tags = list(dfresre[dfresre['seq_id']==seq_id]['tags'])\n",
    "        for token in tokens:\n",
    "            starts.append(sent[strt:].find(token)+strt)\n",
    "            strt+=len(token)+1\n",
    "        dc = pd.concat([pd.Series(tokens), pd.Series(tags), pd.Series(starts)], axis = 1)\n",
    "        dc.columns = ['tokens','tags','start']\n",
    "        dc = dc[dc['tags']!='O']\n",
    "        starts = dc[dc['tags']!='O']['start']\n",
    "        for tkn1s in starts:\n",
    "            df1 = dc[(dc['start']==tkn1s)].copy()\n",
    "            token1 = list(df1['tokens'])[0]\n",
    "            pos1 = sent[max(tkn1s-len(token1),0):tkn1s+len(token1)+10].find(token1)+max(tkn1s-len(token1),0)\n",
    "            s1 = len(sent[:pos1].split())\n",
    "            for tkn2s in starts:\n",
    "                if tkn1s<tkn2s: ##только последующие\n",
    "                    df2 = dc[dc['start']==tkn2s].copy()\n",
    "                    token2 = list(df2['tokens'])[0]\n",
    "                    pos2 = sent[max(tkn2s-len(token2),0):tkn2s+len(token2)+10].find(token2)+max(tkn2s-len(token2),0)\n",
    "                    s2 = len(sent[:pos2].split())\n",
    "                    if tkn1s < tkn2s and list(df1['tokens'])[0]!=list(df2['tokens'])[0]:\n",
    "                         json_neg.append({\n",
    "                               'h': [token1.split(' '), list(df1['tags'])[0], [str(y) for y in range(s1,s1+len(token1.split(' ')))]],\n",
    "                               't': [token2.split(' '), list(df2['tags'])[0], [str(y) for y in range(s2,s2+len(token2.split(' ')))]],\n",
    "                                'tokens': sent,\n",
    "                                'h_id': tkn1s,\n",
    "                                't_id': tkn2s,\n",
    "                                'filename':filename,\n",
    "                                 'seq_id':seq_id\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': [['milrinone'], 'Drug', ['3']],\n",
       " 't': [['inhaled'], 'Route', ['32']],\n",
       " 'tokens': \"we hypothesized that milrinone , an adenosine - 3 ' , 5 ' - cyclic monophosphate ( camp ) - selective phosphodiesterase enzyme ( pde ) inhibitor may , when nebulized and inhaled , cause selective pulmonary vasodilation and potentiate the vasodilation by inhaled prostacyclin ( ipgi2 ) .\",\n",
       " 'h_id': 21,\n",
       " 't_id': 170,\n",
       " 'filename': '00000539-200112000-00018.txt'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reason              21781\n",
       "drug                20927\n",
       "duration            14571\n",
       "dosage               5926\n",
       "6mwd                 3346\n",
       "prev_treat           2594\n",
       "fc                   1880\n",
       "nt-probnp             900\n",
       "death_rate            670\n",
       "hospitalization         6\n",
       "progression_rate        2\n",
       "Name: tag_name, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbio = pd.read_csv('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/pred_entities_joint.csv')\n",
    "dfbio.tag_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_files={}\n",
    "for t, b in zip(np.unique(dfresre['document']), enumerate(np.unique(dfresre['document']))):\n",
    "    dict_files[t] = b[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для inference\n",
    "path = 'Documents/DMMatrix/PAH/CRE/spert/'\n",
    "from nltk import word_tokenize\n",
    "\n",
    "jsons_neg = json_neg\n",
    "\n",
    "with open(path+'test2.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in tqdm(range(len(jsons_neg))):\n",
    "        #i = i+299968\n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons_neg[i]['h'][0])\n",
    "        t = ' '.join(jsons_neg[i]['t'][0])\n",
    "        h_id = str(dict_files[jsons_neg[i]['filename']])+str(jsons_neg[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons_neg[i]['filename']])+str(jsons_neg[i]['t_id'])\n",
    "        str_tokens = jsons_neg[i]['tokens'].split(' ')\n",
    "#         str_tokens = word_tokenize(jsons_neg[i]['tokens'])\n",
    "        k=0\n",
    "        while k<len(str_tokens):\n",
    "#             if jsons_neg[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k) == jsons_neg[i]['h'][2][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons_neg[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons_neg[i]['h'][0])\n",
    "#                 print(k)\n",
    "            elif str(k) in jsons_neg[i]['h'][2]:\n",
    "#                 print(k)\n",
    "                continue\n",
    "            else:\n",
    "                ss1.append(str_tokens[k])\n",
    "#                 print(k)\n",
    "                k+=1\n",
    "        k=0\n",
    "        while k <len(str_tokens):\n",
    "#             if jsons_neg[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k) == jsons_neg[i]['t'][2][0]:\n",
    "                ss2.append('[s2]'+' '.join(jsons_neg[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons_neg[i]['t'][0])\n",
    "            elif str(k) in jsons_neg[i]['t'][2]:\n",
    "#                 print(k)\n",
    "                continue\n",
    "            else:\n",
    "                ss2.append(str_tokens[k])\n",
    "                k+=1\n",
    "        \n",
    "        line = 'NotRel' +'\\t' + ' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons_neg[i]['h'][1] +'\\t'+ jsons_neg[i]['t'][1] +'\\t'+\\\n",
    "        'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg[i]['filename']])\n",
    "#         print(line)\n",
    "        f.write(line+'\\n')   \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': [['milrinone'], 'Drug', ['3']],\n",
       " 't': [['inhaled'], 'Route', ['32']],\n",
       " 'tokens': \"we hypothesized that milrinone , an adenosine - 3 ' , 5 ' - cyclic monophosphate ( camp ) - selective phosphodiesterase enzyme ( pde ) inhibitor may , when nebulized and inhaled , cause selective pulmonary vasodilation and potentiate the vasodilation by inhaled prostacyclin ( ipgi2 ) .\",\n",
       " 'h_id': 21,\n",
       " 't_id': 170,\n",
       " 'filename': '00000539-200112000-00018.txt',\n",
       " 'seq_id': 2}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149986/149986 [00:16<00:00, 9262.32it/s] \n"
     ]
    }
   ],
   "source": [
    "#для inference\n",
    "path = 'Documents/DMMatrix/PAH/CRE/v6_fs/'\n",
    "from nltk import word_tokenize\n",
    "\n",
    "jsons_neg = json_neg\n",
    "\n",
    "with open(path+'test_fs.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in tqdm(range(len(jsons_neg))):\n",
    "        #i = i+299968\n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons_neg[i]['h'][0])\n",
    "        t = ' '.join(jsons_neg[i]['t'][0])\n",
    "        h_id = str(dict_files[jsons_neg[i]['filename']])+str(jsons_neg[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons_neg[i]['filename']])+str(jsons_neg[i]['t_id'])\n",
    "        str_tokens = jsons_neg[i]['tokens'].split(' ')\n",
    "#         str_tokens = word_tokenize(jsons_neg[i]['tokens'])\n",
    "        k=0\n",
    "        while k<len(str_tokens):\n",
    "#                 if jsons[i]['tokens'][k].find('.')==0:\n",
    "#                     break\n",
    "            if str(k+1) == jsons_neg[i]['h'][2][0][0]:\n",
    "                ss1.append('[s1] '+' '.join(jsons_neg[i]['h'][0])+' [e1]')\n",
    "                k = k+len(jsons_neg[i]['h'][0])\n",
    "            elif str(k+1) == jsons_neg[i]['t'][2][0][0]:\n",
    "                ss1.append('[s2] '+' '.join(jsons_neg[i]['t'][0])+' [e2]')\n",
    "                k = k+len(jsons_neg[i]['t'][0])\n",
    "            else:\n",
    "                ss1.append(str_tokens[k])\n",
    "                k+=1\n",
    "        type_ = min(jsons_neg[i]['h'][1],jsons_neg[i]['t'][1])+'_'+max(jsons_neg[i]['h'][1],jsons_neg[i]['t'][1])\n",
    "\n",
    "#             if type_.lower().find('rate')>=0:\n",
    "#                 type_='cohort_effect'\n",
    "#             elif type_.lower().find('duration')>=0:\n",
    "#                 type_='duration'\n",
    "#             else:\n",
    "#                 type_ = 'cohort'\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        line = 'NotRel' +'\\t' + ' '.join(ss1) +'\\t'+ jsons_neg[i]['h'][1] +'\\t'+ jsons_neg[i]['t'][1] +'\\t'+\\\n",
    "        'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg[i]['filename']])\n",
    "#         print(line)\n",
    "        f.write(line+'\\n')   \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NotRel</td>\n",
       "      <td>we hypothesized [s1]milrinone[e1] milrinone , ...</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Route</td>\n",
       "      <td>T_121</td>\n",
       "      <td>T_1170</td>\n",
       "      <td>F_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NotRel</td>\n",
       "      <td>we hypothesized [s1]milrinone[e1] [s2]inhaled[...</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Route</td>\n",
       "      <td>T_121</td>\n",
       "      <td>T_1254</td>\n",
       "      <td>F_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NotRel</td>\n",
       "      <td>we hypothesized [s1]milrinone[e1] [s2]prostacy...</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Drug</td>\n",
       "      <td>T_121</td>\n",
       "      <td>T_1262</td>\n",
       "      <td>F_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NotRel</td>\n",
       "      <td>we hypothesized [s1]inhaled[e1] [s2]prostacycl...</td>\n",
       "      <td>Route</td>\n",
       "      <td>Drug</td>\n",
       "      <td>T_1170</td>\n",
       "      <td>T_1262</td>\n",
       "      <td>F_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NotRel</td>\n",
       "      <td>we hypothesized that [s1]inhaled[e1] , an aden...</td>\n",
       "      <td>Route</td>\n",
       "      <td>Drug</td>\n",
       "      <td>T_1254</td>\n",
       "      <td>T_1262</td>\n",
       "      <td>F_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                                                  1      2      3  \\\n",
       "0  NotRel  we hypothesized [s1]milrinone[e1] milrinone , ...   Drug  Route   \n",
       "1  NotRel  we hypothesized [s1]milrinone[e1] [s2]inhaled[...   Drug  Route   \n",
       "2  NotRel  we hypothesized [s1]milrinone[e1] [s2]prostacy...   Drug   Drug   \n",
       "3  NotRel  we hypothesized [s1]inhaled[e1] [s2]prostacycl...  Route   Drug   \n",
       "4  NotRel  we hypothesized that [s1]inhaled[e1] , an aden...  Route   Drug   \n",
       "\n",
       "        4       5    6  \n",
       "0   T_121  T_1170  F_1  \n",
       "1   T_121  T_1254  F_1  \n",
       "2   T_121  T_1262  F_1  \n",
       "3  T_1170  T_1262  F_1  \n",
       "4  T_1254  T_1262  F_1  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fs = pd.read_csv(path+'test_fs.tsv', delimiter = '\\t', header = None)\n",
    "\n",
    "test_fs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149986/149986 [00:23<00:00, 6307.89it/s]\n"
     ]
    }
   ],
   "source": [
    "#for SEMVEAL 2\n",
    "path = 'Documents/DMMatrix/PAH/CRE/spert/'\n",
    "from nltk import word_tokenize\n",
    "\n",
    "jsons_neg = json_neg\n",
    "\n",
    "with open(path+'test2_with_name.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in tqdm(range(len(jsons_neg))):\n",
    "#         i = i+22000\n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons_neg[i]['h'][0])\n",
    "        t = ' '.join(jsons_neg[i]['t'][0])\n",
    "        h_id = str(dict_files[jsons_neg[i]['filename']])+str(jsons_neg[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons_neg[i]['filename']])+str(jsons_neg[i]['t_id'])\n",
    "        str_tokens = jsons_neg[i]['tokens'].split(' ')\n",
    "#         str_tokens = word_tokenize(jsons_neg[i]['tokens'])\n",
    "        k=0\n",
    "        while k<len(str_tokens):\n",
    "#             if jsons_neg[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k) == jsons_neg[i]['h'][2][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons_neg[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons_neg[i]['h'][0])\n",
    "#                 print(k)\n",
    "            elif str(k) in jsons_neg[i]['h'][2]:\n",
    "#                 print(k)\n",
    "                continue\n",
    "            else:\n",
    "                ss1.append(str_tokens[k])\n",
    "#                 print(k)\n",
    "                k+=1\n",
    "        k=0\n",
    "        while k <len(str_tokens):\n",
    "#             if jsons_neg[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k) == jsons_neg[i]['t'][2][0]:\n",
    "                ss2.append('[s2]'+' '.join(jsons_neg[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons_neg[i]['t'][0])\n",
    "            elif str(k) in jsons_neg[i]['t'][2]:\n",
    "#                 print(k)\n",
    "                continue\n",
    "            else:\n",
    "                ss2.append(str_tokens[k])\n",
    "                k+=1\n",
    "\n",
    "        \n",
    "        line = jsons_neg[i]['filename']+'\\t'+' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons_neg[i]['h'][1] +'\\t'+ jsons_neg[i]['t'][1] +'\\t'+\\\n",
    "        'T_'+h_id+'\\t'+'T_'+t_id+'\\t'+'F_'+str(dict_files[jsons_neg[i]['filename']]) +'\\t'\\\n",
    "        +' '.join(jsons_neg[i]['h'][0])+'\\t' +' '.join(jsons_neg[i]['t'][0])+'\\t'+str(json_neg[i]['seq_id'])\n",
    "#         print(line)\n",
    "        f.write(line+'\\n')   \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для inference RE processing\n",
    "path = 'Documents/DMMatrix/PAH/CRE/v5/'\n",
    "\n",
    "with open(path+'test_with_name.tsv', 'w') as f:\n",
    "#     f.write('index'+'\\t'+'sentence'+'\\t'+'label'+'\\n')\n",
    "    for i in range(len(jsons_neg)):\n",
    "#         i = i+22000\n",
    "        ss1 = []\n",
    "        ss2 = []\n",
    "        h = ' '.join(jsons_neg[i]['h'][0])\n",
    "        t = ' '.join(jsons_neg[i]['t'][0])\n",
    "        h_id = str(dict_files[jsons_neg[i]['filename']])+str(jsons_neg[i]['h_id'])\n",
    "        t_id = str(dict_files[jsons_neg[i]['filename']])+str(jsons_neg[i]['t_id'])\n",
    "        k=0\n",
    "        while k<len(jsons_neg[i]['tokens']):\n",
    "#             if jsons_neg[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons_neg[i]['h'][2][0][0]:\n",
    "                ss1.append('[s1]'+' '.join(jsons_neg[i]['h'][0])+'[e1]')\n",
    "                k = k+len(jsons_neg[i]['h'][0])\n",
    "            else:\n",
    "                ss1.append(jsons_neg[i]['tokens'][k])\n",
    "                k+=1\n",
    "        k=0\n",
    "        while k <len(jsons_neg[i]['tokens']):\n",
    "#             if jsons_neg[i]['tokens'][k].find('.')==0:\n",
    "#                 break\n",
    "            if str(k+1) == jsons_neg[i]['t'][2][0][0]:\n",
    "                ss2.append('[s2]'+' '.join(jsons_neg[i]['t'][0])+'[e2]')\n",
    "                k = k+len(jsons_neg[i]['t'][0])\n",
    "            else:\n",
    "                ss2.append(jsons_neg[i]['tokens'][k])\n",
    "                k+=1\n",
    "\n",
    "        \n",
    "        line = jsons_neg[i]['filename']+'\\t'+' '.join(ss1) +'\\t'+ ' '.join(ss2) +'\\t'+ jsons_neg[i]['h'][1] +'\\t'+ jsons_neg[i]['t'][1] +'\\t'+\\\n",
    "        'T_'+h_id+'\\t'+'T_'+t_id+str(i)+'\\t'+'F_'+str(dict_files[jsons_neg[i]['filename']]) +'\\t' +' '.join(jsons_neg[i]['h'][0])+'\\t' +' '.join(jsons_neg[i]['t'][0])\n",
    "#         print(line)\n",
    "        f.write(line+'\\n')\n",
    "   \n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path+'test.tsv', delimiter = '\\t',header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint NER + RE by Santosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_NEJMoa012212.tsv\n",
      "ann_13993003.00364-2015.tsv\n",
      "ann_13993003.01886-2017.tsv\n",
      "ann_405.full.tsv\n",
      "ann_NEJMoa1209655.tsv\n",
      "ann_Phosphodisesterase inhibitors in Pulmonary hypertension.tsv\n",
      "ann_09031936.04.00028404.tsv\n",
      "ann_469.full.tsv\n",
      "ann_CIRCULATIONAHA.104.473371.tsv\n",
      "ann_NEJMoa1503184.tsv\n",
      "ann_13993003.01030-2019.tsv\n",
      "ann_000355875.tsv\n",
      "ann_S2213-2600_2816_2930307-1.tsv\n",
      "ann_1753465808103499.tsv\n",
      "ann_S2213-2600_2816_2930019-4.tsv\n",
      "ann_CIRCULATIONAHA.107.742510.tsv\n",
      "ann_09031936.06.00030206.tsv\n",
      "ann_13993003.02044-2014.tsv\n",
      "ann_1745-6215-14-188.tsv\n",
      "ann_1755-5922.12008.tsv\n",
      "ann_S2213-2600_2814_2970013-X.tsv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "pp = re.compile('(?<!\\\\\\\\)\\'')\n",
    "\n",
    "nfiles = 0\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "entity = []\n",
    "index = []\n",
    "sentence = []\n",
    "entity_unique = []\n",
    "\n",
    "dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/Marked_articles/'\n",
    "# dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/готовые файлы из инцепта/'\n",
    "#listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "listfiles = os.listdir(dir_)\n",
    "\n",
    "prev = ''\n",
    "# jsons = [] #jsons for SEMVEAL\n",
    "# jsons_neg_train = [] #jsons for SEMVEAL with no relation\n",
    "ent_ids = [] #id of every token\n",
    "entities = [] #entities for joint NER and RE model\n",
    "relations = [] #relationss for joint NER and RE model\n",
    "entities_unique = [] #id of complete entity\n",
    "# jsonlist = [] #list of jsons for joint NER+RE model\n",
    "# jsonlist_test = [] #list of jsons for joint NER+RE model, test\n",
    "# sglobal = 0 #sentence counter\n",
    "\n",
    "for file in listfiles:\n",
    "    print(file)\n",
    "    if file[-3:]=='tsv' and file[:3]!='cre' and file.find('ann_382')<0:\n",
    "        nfiles+=1\n",
    "        bio_annotation = [] #entities (tokens)\n",
    "        entity = [] #type of entities\n",
    "        index = [] #indexes for relations\n",
    "        sentence = [] \n",
    "        nconn = []\n",
    "        ent_ids = [] #id of every token\n",
    "        ent_dict = []\n",
    "        dk=0\n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[3]!= '_':\n",
    "                    ent_dict.append([line[3],line[4]])\n",
    "        df = pd.DataFrame(ent_dict)\n",
    "        df1 = df[df[0]=='*'].reset_index().reset_index()\n",
    "        nzs = df1.shape[0]+2\n",
    "        df2 = df[df[0]!='*'].drop_duplicates().reset_index().reset_index()\n",
    "        df2['level_0'] = df2['level_0']+df1.shape[0]+1\n",
    "        dict_df2={}\n",
    "        for t, b in zip(df2[1], df2['level_0']):\n",
    "            dict_df2[t] = b\n",
    "        nz = 0\n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "            for line in tsvreader:\n",
    "#                 print(line)\n",
    "                if len(line)<5:\n",
    "                    continue\n",
    "                if line[5] !='_' and len(line[2])>0:\n",
    "#                     print(line[2], line[6])\n",
    "#                     if prev == '':\n",
    "                    nc = len(line[6].split('|'))                      \n",
    "                    prev = line[4].split('[')[0]\n",
    "                    if prev not in entity_unique:\n",
    "                        entity_unique.append(prev)\n",
    "                    if line[4].find(']')>0:\n",
    "                        ent_id = dict_df2[line[4]]\n",
    "                    else:\n",
    "                        ent_id = nz\n",
    "                        nz+=1\n",
    "#                     if ent_id>0:\n",
    "#                         print(ent_id)\n",
    "                    bio_annotation.append(line[2])\n",
    "                    index.append([line[6].split('|')[r].split('[')[1].split(']')[0] for r in range(nc)])\n",
    "                    entity.append(prev)\n",
    "                    sentence.append(line[0])\n",
    "                    ent_ids.append(ent_id)\n",
    "                elif line[4] != '_' and len(line[2])>0:\n",
    "                    prev = line[4].split('[')[0]\n",
    "                    if prev not in entity_unique:\n",
    "                        entity_unique.append(prev)\n",
    "                    if line[4].find(']')>0:\n",
    "                        ent_id = dict_df2[line[4]]\n",
    "                    else:\n",
    "                        ent_id = nz\n",
    "                        nz+=1\n",
    "                    if nz>nzs+1:\n",
    "                        print('nz problem:',file, nz, nzs)\n",
    "                    bio_annotation.append(line[2])\n",
    "                    index.append([''])\n",
    "                    entity.append(prev)\n",
    "                    sentence.append(line[0])\n",
    "                    ent_ids.append(ent_id)\n",
    "#                     elif prev == line[6].split('[')[0]:\n",
    "#                         bio_annotation.append(line[2])\n",
    "#                         prev = line[6].split('[')[0]\n",
    "#                         index.append(line[6].split('[')[1])\n",
    "        cnt_pat=0\n",
    "        if os.path.exists('/Users/aelitta/Documents/DMMatrix/PAH/SPERT/PAH_v2_project_2022-10-31_1418/annotation/'+file[:-4]+'.txt/admin.tsv'):\n",
    "            file1 = 'admin.tsv'\n",
    "            cnt_pat =1\n",
    "        elif os.path.exists('/Users/aelitta/Documents/DMMatrix/PAH/SPERT/PAH_v2_project_2022-10-31_1418/annotation/'+file[:-4]+'.txt/anna.tsv'):\n",
    "            file1 = 'anna.tsv'\n",
    "            cnt_pat =1\n",
    "        if cnt_pat>0: \n",
    "            nfiles+=1\n",
    "            bio_annotation = [] #entities (tokens)\n",
    "            entity = [] #type of entities\n",
    "            index = [] #indexes for relations\n",
    "            sentence = [] \n",
    "            nconn = []\n",
    "            ent_ids = [] #id of every token\n",
    "            ent_dict = []\n",
    "            dk=0\n",
    "            with open('/Users/aelitta/Documents/DMMatrix/PAH/SPERT/PAH_v2_project_2022-10-31_1418/annotation/'+file[:-4]+'.txt/'+file1) as tsvfile:\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                for line in tsvreader:\n",
    "                    if len(line)<5:\n",
    "                        continue\n",
    "                    if line[3]!= '_':\n",
    "                        ent_dict.append([line[3],line[4]])\n",
    "            df = pd.DataFrame(ent_dict)\n",
    "            df1 = df[df[0]=='*'].reset_index().reset_index()\n",
    "            nzs = df1.shape[0]+2\n",
    "            df2 = df[df[0]!='*'].drop_duplicates().reset_index().reset_index()\n",
    "            df2['level_0'] = df2['level_0']+df1.shape[0]+1\n",
    "            dict_df2={}\n",
    "            for t, b in zip(df2[1], df2['level_0']):\n",
    "                dict_df2[t] = b\n",
    "            nz = 0\n",
    "\n",
    "\n",
    "            with open('/Users/aelitta/Documents/DMMatrix/PAH/SPERT/PAH_v2_project_2022-10-31_1418/annotation/'+file[:-4]+'.txt/'+file1) as tsvfile:\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                for line in tsvreader:\n",
    "    #                 print(line)\n",
    "                    if len(line)<5:\n",
    "                        continue\n",
    "                    if line[5] !='_' and len(line[2])>0 and line[5].find('cnt\\_patients')>=0:\n",
    "    #                     print(line[2], line[6])\n",
    "    #                     if prev == '':\n",
    "                        nc = len(line[6].split('|'))                      \n",
    "                        prev = line[4].split('[')[0]\n",
    "                        if prev not in entity_unique:\n",
    "                            entity_unique.append(prev)\n",
    "                        if line[4].find(']')>0:\n",
    "                            ent_id = dict_df2[line[4]]\n",
    "                        else:\n",
    "                            ent_id = nz\n",
    "                            nz+=1\n",
    "    #                     if ent_id>0:\n",
    "    #                         print(ent_id)\n",
    "                        bio_annotation.append(line[2])\n",
    "                        index.append([line[6].split('|')[r].split('[')[1].split(']')[0] for r in range(nc)])\n",
    "                        entity.append(prev)\n",
    "                        sentence.append(line[0])\n",
    "                        ent_ids.append(ent_id)\n",
    "                    elif line[4] != '_' and len(line[2])>0 and line[4].find('cnt\\\\_patients')>=0:\n",
    "                        prev = line[4].split('[')[0]\n",
    "                        if prev not in entity_unique:\n",
    "                            entity_unique.append(prev)\n",
    "                        if line[4].find(']')>0:\n",
    "                            ent_id = dict_df2[line[4]]\n",
    "                        else:\n",
    "                            ent_id = nz\n",
    "                            nz+=1\n",
    "                        if nz>nzs+1:\n",
    "                            print('nz problem:',file, nz, nzs)\n",
    "                        bio_annotation.append(line[2])\n",
    "                        index.append([''])\n",
    "                        entity.append(prev)\n",
    "                        sentence.append(line[0])\n",
    "                        ent_ids.append(ent_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        k=1\n",
    "        p=1\n",
    "        scounter = 1 #sentences\n",
    "        entities = []\n",
    "        relations = []\n",
    "        entities_unique = []\n",
    "       \n",
    "        while i<len(bio_annotation):\n",
    "            if int(sentence[i].split('-')[0]) > scounter:\n",
    "                if len(relations)>0 and sglobal%5!=0 and len(s)>0 and len(s)<80:\n",
    "                    jsonlist.append({\"tokens\": s, \"entities\": entities, \"relations\": relations, \"orig_id\": sglobal})\n",
    "                if len(relations)>0 and sglobal%5==0  and len(s)>0 and len(s)<80:\n",
    "                        jsonlist_test.append({\"tokens\": s, \"entities\": entities, \"relations\": relations, \"orig_id\": sglobal})\n",
    "                scounter=int(sentence[i].split('-')[0])\n",
    "                sglobal+=1\n",
    "                entities = []\n",
    "                relations = []\n",
    "                entities_unique = []\n",
    "            k=1\n",
    "            while i+k< len(entity) and entity[i+k]==entity[i] \\\n",
    "                and abs(int(sentence[i+k].split('-')[1])-int(sentence[i+(k-1)].split('-')[1]))==1:\n",
    "                k+=1 ##весь токен\n",
    "            if ent_ids[i] not in entities_unique and entity[i]!='*' and entity[i]!='_':\n",
    "                entities_unique.append(ent_ids[i])\n",
    "                entities.append({\"type\": entity[i].lower().replace(\"\\\\\",\"\").replace('ntprobpn','nt-probnp').\\\n",
    "                                 replace('bnp_rate','nt-probnp').replace('6mwd_rate', '6mwd').replace('functional_class','fc')\\\n",
    "                                 .replace('progression_rate','progression').replace('death','death_rate')\\\n",
    "                                 .replace('death_rate_rate','death_rate').replace('previous_treatment','prev_treat'), \\\n",
    "                                 \"start\": int(sentence[i].split('-')[1])-1, \"end\": int(sentence[i+k-1].split('-')[1])})\n",
    "\n",
    "            j = i+k\n",
    "            while j<len(bio_annotation):\n",
    "#                 print(i,j)\n",
    "                if i!=j and i<len(entity) and j<len(entity) and entity[i]!=entity[j] \\\n",
    "                    and sentence[i].split('-')[0]==sentence[j].split('-')[0]: \n",
    "                        #извлекаем полную запись\n",
    "                        p=1\n",
    "                        while j+p< len(entity) and  entity[j+p]==entity[j] \\\n",
    "                            and abs(int(sentence[j+p].split('-')[1])-int(sentence[j+p-1].split('-')[1]))==1:\n",
    "                            p+=1\n",
    "                        if ent_ids[j] not in entities_unique and entity[i]!='*' and entity[i]!='_':\n",
    "                            entities_unique.append(ent_ids[j])\n",
    "                            entities.append({\"type\": entity[j].lower().replace(\"\\\\\",\"\").replace('ntprobpn','nt-probnp').\\\n",
    "                                             replace('bnp_rate','nt-probnp').replace('6mwd_rate', '6mwd').replace('functional_class','fc').\\\n",
    "                                             replace('progression_rate','progression').replace('death_rate','death').\\\n",
    "                                             replace('death_rate_rate','death_rate').replace('previous_treatment','prev_treat'),\\\n",
    "                                             \"start\": int(sentence[j].split('-')[1])-1, \"end\": int(sentence[j+p-1].split('-')[1])})\n",
    "                        s=[]\n",
    "                        with open(dir_+file) as tsvfile:\n",
    "                            tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                            for line in tsvreader:\n",
    "                                if len(line)>0 and line[0].split('-')[0]==sentence[i].split('-')[0]:\n",
    "                                    if line[2].replace(\"'\",'')!='':\n",
    "                                        s.append(pp.sub('\\\"', str(line[2])))\n",
    "                            ni = 0 #flag of conn\n",
    "                            for nii in index[i]:\n",
    "                                for nij in index[j]:\n",
    "                                    if nii == nij and nii !='' and entity[i]!='_' and entity[j]!='_'  and entity[i]!='*' and entity[j]!='*':\n",
    "                                        jsons.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                                      't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                                      'tokens':s,\n",
    "                                                      'h_id': ent_ids[i],\n",
    "                                                      't_id': ent_ids[j],\n",
    "                                                      'filename': file\n",
    "                                                     })\n",
    "                                        relations.append({\"type\": \"Rel\", \"head\": entities_unique.index(ent_ids[i]), \n",
    "                                                          \"tail\":entities_unique.index(ent_ids[j])})\n",
    "                                        ni+=1\n",
    "                            if ni==0 and entity[i]!='_' and entity[j]!='_' and entity[i]!='*' and entity[j]!='*':\n",
    "                                jsons_neg_train.append({'h':[bio_annotation[i:i+k],entity[i], [[sentence[i+r].split('-')[1] for r in range(k)]]],\n",
    "                                          't':[bio_annotation[j:j+p],entity[j], [[sentence[j+r].split('-')[1] for r in range(p)]]],\n",
    "                                          'tokens':s,\n",
    "                                          'h_id': ent_ids[i],\n",
    "                                          't_id': ent_ids[j],\n",
    "                                          'filename': file\n",
    "                                                       })\n",
    "                                relations.append({\"type\": \"NotRel\", \"head\": entities_unique.index(ent_ids[i]), \"tail\":entities_unique.index(ent_ids[j])})\n",
    "                        j = j+p\n",
    "                else:\n",
    "                    j+=1\n",
    "            i = i+k    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jsonlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonlist[458]['tokens'][48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drug',\n",
       " 'reason',\n",
       " '6MWD',\n",
       " 'dosage',\n",
       " 'duration',\n",
       " 'FC',\n",
       " 'death\\\\_rate',\n",
       " 'cnt\\\\_patients',\n",
       " 'ntproBPN',\n",
       " '6mwd',\n",
       " 'hospitalization',\n",
       " 'progression\\\\_rate',\n",
       " 'prev\\\\_treat']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "opath = '/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/relations_marked_train_a.json'\n",
    "with open(opath, \"w\") as ofile:\n",
    "    json.dump(jsonlist, ofile) \n",
    "opath = '/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/relations_marked_test_a.json'\n",
    "with open(opath, \"w\") as ofile:\n",
    "    json.dump(jsonlist_test, ofile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import scispacy\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from more_itertools import locate\n",
    "\n",
    "\n",
    "#!pip install scispacy\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\n",
    "#####################################\n",
    "#### Customized tokenizer        ####\n",
    "#####################################\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nlp = spacy.load(\"en_core_sci_sm\")\n",
    "# nlp = spacy.load('en_core_sci_sm')\n",
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return Doc(nlp.vocab, tokens)\n",
    "    #global tokens_dict\n",
    "    #if text in tokens_dict:\n",
    "    #   return Doc(nlp.vocab, tokens_dict[text])\n",
    "    #else:\n",
    "    #   VaueError(\"No tokenization for input text: \", text)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer\n",
    "#####################################\n",
    "\n",
    "\n",
    "class JsonInputAugmenter():\n",
    "    def __init__(self):\n",
    "        basepath = '/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/'\n",
    "        self.input_dataset_paths  = [basepath + 'relations_marked_train_a.json',\n",
    "                                     basepath + 'relations_marked_test_a.json'\n",
    "#                                       basepath + 'articles_inf_aa.json'\n",
    "                                    ]\n",
    "#         basepath = './data/datasets/ade/'\n",
    "#         self.input_dataset_paths  = [basepath + 'ade_split_0_train.json', \n",
    "#                                      basepath + 'ade_split_0_test.json', \n",
    "#                                      basepath + 'ade_split_1_train.json',\n",
    "#                                      basepath + 'ade_split_1_test.json',\n",
    "#                                      basepath + 'ade_split_2_train.json',\n",
    "#                                      basepath + 'ade_split_2_test.json',\n",
    "#                                      basepath + 'ade_split_3_train.json',\n",
    "#                                      basepath + 'ade_split_3_test.json',\n",
    "#                                      basepath + 'ade_split_4_train.json',\n",
    "#                                      basepath + 'ade_split_4_test.json',\n",
    "#                                      basepath + 'ade_split_5_train.json',\n",
    "#                                      basepath + 'ade_split_5_test.json',\n",
    "#                                      basepath + 'ade_split_6_train.json',\n",
    "#                                      basepath + 'ade_split_6_test.json',\n",
    "#                                      basepath + 'ade_split_7_train.json',\n",
    "#                                      basepath + 'ade_split_7_test.json',\n",
    "#                                      basepath + 'ade_split_8_train.json',\n",
    "#                                      basepath + 'ade_split_8_test.json',\n",
    "#                                      basepath + 'ade_split_9_train.json',\n",
    "#                                      basepath + 'ade_split_9_test.json'                                 \n",
    "#                                      ]\n",
    "#         self.output_dataset_paths = [basepath + 'ade_split_0_train_aug.json', \n",
    "#                                      basepath + 'ade_split_0_test_aug.json', \n",
    "#                                      basepath + 'ade_split_1_train_aug.json',\n",
    "#                                      basepath + 'ade_split_1_test_aug.json',\n",
    "#                                      basepath + 'ade_split_2_train_aug.json',\n",
    "#                                      basepath + 'ade_split_2_test_aug.json',\n",
    "#                                      basepath + 'ade_split_3_train_aug.json',\n",
    "#                                      basepath + 'ade_split_3_test_aug.json',\n",
    "#                                      basepath + 'ade_split_4_train_aug.json',\n",
    "#                                      basepath + 'ade_split_4_test_aug.json',\n",
    "#                                      basepath + 'ade_split_5_train_aug.json',\n",
    "#                                      basepath + 'ade_split_5_test_aug.json',\n",
    "#                                      basepath + 'ade_split_6_train_aug.json',\n",
    "#                                      basepath + 'ade_split_6_test_aug.json',\n",
    "#                                      basepath + 'ade_split_7_train_aug.json',\n",
    "#                                      basepath + 'ade_split_7_test_aug.json',\n",
    "#                                      basepath + 'ade_split_8_train_aug.json',\n",
    "#                                      basepath + 'ade_split_8_test_aug.json',\n",
    "#                                      basepath + 'ade_split_9_train_aug.json',\n",
    "#                                      basepath + 'ade_split_9_test_aug.json'                                 \n",
    "#                                      ]\n",
    "        self.output_dataset_paths =  [basepath + 'relations_marked_train_aug.json',\n",
    "                                     basepath + 'relations_marked_test_aug.json'\n",
    "#                                       basepath + 'articles_inf_aa_aug.json'\n",
    "                                     ]  \n",
    "            \n",
    "        #self.output_dataset_paths = [basepath + 'scierc_train_aug2.json', basepath + 'scierc_dev_aug2.json', basepath + 'scierc_train_dev_aug2.json', basepath + 'scierc_test_aug2.json']\n",
    "        #self.taglist =[]\n",
    "\n",
    "    def augment_docs_in_datasets(self):\n",
    "        for ipath, opath  in zip(self.input_dataset_paths, self.output_dataset_paths):\n",
    "            self._augment_docs(ipath, opath)\n",
    "            #self._datasets[dataset_label] = dataset\n",
    "\n",
    "    def _augment_docs(self, ipath, opath):\n",
    "        global tokens_dict\n",
    "        documents = json.load(open(ipath))\n",
    "        augmented_documents = []\n",
    "        nmultiroot=0\n",
    "        for document in documents:\n",
    "            jtokens = document['tokens']\n",
    "            jrelations = document['relations']\n",
    "            jentities = document['entities']\n",
    "            jorig_id = document['orig_id']\n",
    "\n",
    "            lower_jtokens = jtokens #[t.lower() for t in jtokens]\n",
    "            text = ' '.join(lower_jtokens)\n",
    "            #text = str.lower(text)\n",
    "    \n",
    "            #tokens_dict = {text: jtokens} #put the text in token_dict\n",
    "            tokens = nlp(text)            #get annotated tokens\n",
    "            jtags = [token.tag_ for token in tokens]\n",
    "            #self.taglist =self.taglist + jtags\n",
    "            jdeps = [token.dep_ for token in tokens]\n",
    "            #\"verb_indicator\", \"dep_head\"\n",
    "            #root = jdeps.index(\"ROOT\") + 1 #as tokens are numbered from 1 by CoreNLP convention \n",
    "            vpos = list(locate(jdeps, lambda x: x == 'ROOT'))\n",
    "            \n",
    "            if (len(vpos) != 1):\n",
    "                flag = 1\n",
    "                nmultiroot += 1\n",
    "                print(\"*** Full sentence:\", text)\n",
    "                for i in vpos:\n",
    "                    print(\"ROOT [\", i, \"]: \", jtokens[i], \", pos tag: \", jtags[i], \", dep: \", jdeps[i])\n",
    "            else:\n",
    "                flag = 0\n",
    "\n",
    "            verb_indicator = [0] * len(jdeps)\n",
    "            for i in vpos:\n",
    "                verb_indicator[i] = 1  \n",
    "\n",
    "            jdep_heads = []\n",
    "            for i, token in enumerate(tokens):\n",
    "              if token.head == token:\n",
    "                 token_idx = 0\n",
    "              else:\n",
    "                 token_idx = token.head.i - tokens[0].i + 1\n",
    "              jdep_heads.append(token_idx)\n",
    "            if (flag==1):\n",
    "              print(\"dep_head: \", jdep_heads)\n",
    "            d = {\"tokens\": jtokens, \"pos_tags\": jtags, \"dep_label\": jdeps, \"verb_indicator\": verb_indicator, \"dep_head\": jdep_heads, \"entities\": jentities, \"relations\": jrelations, \"orig_id\": jorig_id}\n",
    "            augmented_documents.append(d)\n",
    "        print(\"===============  #docs with multiroot = \", nmultiroot)\n",
    "        with open(opath, \"w\") as ofile:\n",
    "            json.dump(augmented_documents, ofile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT  --  None\n",
      "acl  --  clausal modifier of noun (adjectival clause)\n",
      "acomp  --  adjectival complement\n",
      "advcl  --  adverbial clause modifier\n",
      "advmod  --  adverbial modifier\n",
      "agent  --  agent\n",
      "amod  --  adjectival modifier\n",
      "appos  --  appositional modifier\n",
      "attr  --  attribute\n",
      "aux  --  auxiliary\n",
      "auxpass  --  auxiliary (passive)\n",
      "case  --  case marking\n",
      "cc  --  coordinating conjunction\n",
      "ccomp  --  clausal complement\n",
      "compound  --  compound\n",
      "conj  --  conjunct\n",
      "csubj  --  clausal subject\n",
      "csubjpass  --  clausal subject (passive)\n",
      "dative  --  dative\n",
      "dep  --  unclassified dependent\n",
      "det  --  determiner\n",
      "dobj  --  direct object\n",
      "expl  --  expletive\n",
      "intj  --  interjection\n",
      "mark  --  marker\n",
      "meta  --  meta modifier\n",
      "neg  --  negation modifier\n",
      "nmod  --  modifier of nominal\n",
      "npadvmod  --  noun phrase as adverbial modifier\n",
      "nsubj  --  nominal subject\n",
      "nsubjpass  --  nominal subject (passive)\n",
      "nummod  --  numeric modifier\n",
      "oprd  --  object predicate\n",
      "parataxis  --  parataxis\n",
      "pcomp  --  complement of preposition\n",
      "pobj  --  object of preposition\n",
      "poss  --  possession modifier\n",
      "preconj  --  pre-correlative conjunction\n",
      "predet  --  None\n",
      "prep  --  prepositional modifier\n",
      "prt  --  particle\n",
      "punct  --  punctuation\n",
      "quantmod  --  modifier of quantifier\n",
      "relcl  --  relative clause modifier\n",
      "xcomp  --  open clausal complement\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.tokenizer.vocab.morphology.tag_map\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "for label in nlp.get_pipe(\"parser\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: Figure 2A demonstrates a signiﬁcant drop in the PA mean pressure for patients treated with bosentan ( baseline , 36 ± 7.1 mm Hg ; 16 weeks , 32 ± 8.8 mm Hg ; P , .02 ) .\n",
      "ROOT [ 2 ]:  demonstrates , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 32 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 6, 6, 3, 6, 11, 11, 11, 7, 11, 12, 13, 14, 15, 16, 6, 18, 24, 24, 23, 24, 18, 6, 27, 6, 27, 32, 32, 32, 27, 0, 33, 33, 35, 35, 35, 33]\n",
      "*** Full sentence: Figure 2B demonstrates no signiﬁcant change in the PA mean pressure for patients treated with placebo ( baseline , 30 ± 4.1 mm Hg ; 16 weeks , 31 ± 6.3 mm Hg ; P. .05 ) .\n",
      "ROOT [ 2 ]:  demonstrates , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 32 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 6, 6, 3, 6, 11, 11, 11, 7, 11, 12, 13, 14, 15, 16, 16, 18, 21, 19, 23, 24, 21, 18, 27, 18, 27, 32, 32, 32, 27, 0, 33, 33, 35, 35, 33]\n",
      "*** Full sentence: The fall in PVR in patients treated with bosentan ( 21.7 ± 2.75 Wood units ) was not signiﬁcantly different from the rise in PVR for those treated with placebo ( 0.1 ± 1.42 Wood units , P . .05 ) .\n",
      "ROOT [ 16 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 39 ]:  .05 , pos tag:  NFP , dep:  ROOT\n",
      "dep_head:  [2, 17, 2, 3, 2, 5, 6, 7, 8, 9, 12, 15, 14, 15, 9, 9, 0, 17, 20, 17, 20, 23, 21, 23, 24, 20, 26, 27, 28, 29, 30, 36, 36, 35, 36, 30, 36, 39, 17, 0, 40, 40]\n",
      "*** Full sentence: No signiﬁcant difference in the 6MWD was found for either the bosentan group ( baseline , 343 ± 95.5 m ; 16 weeks , 320 ± 75.3 m ; P . .05 by Student paired- sample t test ) or the placebo group ( baseline , 313 ± 72.6 m ; 16 weeks , 331 ± 70.3 m ; P . .05 ) .\n",
      "ROOT [ 7 ]:  found , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 31 ]:  .05 , pos tag:  NFP , dep:  ROOT\n",
      "ROOT [ 32 ]:  by , pos tag:  IN , dep:  ROOT\n",
      "ROOT [ 61 ]:  .05 , pos tag:  NFP , dep:  ROOT\n",
      "dep_head:  [3, 3, 8, 3, 6, 4, 8, 0, 8, 13, 13, 13, 9, 13, 20, 20, 20, 20, 20, 13, 13, 23, 13, 23, 28, 28, 28, 23, 13, 13, 8, 0, 0, 38, 34, 38, 38, 33, 38, 38, 43, 43, 38, 43, 50, 50, 50, 50, 50, 43, 43, 53, 43, 53, 58, 58, 58, 53, 43, 43, 43, 0, 62, 62]\n",
      "*** Full sentence: Over the 16 weeks of the study , the 6MWD fell for the bosen- tan group ( 223 ± 69.5 m ) but rose for the placebo group ( 17 ± 44.1 m ) ( Fig 4 ) .\n",
      "ROOT [ 10 ]:  fell , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 35 ]:  Fig , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [11, 4, 4, 1, 4, 7, 5, 11, 10, 11, 0, 11, 16, 16, 16, 12, 16, 19, 21, 21, 16, 16, 11, 11, 24, 28, 28, 25, 33, 33, 33, 33, 28, 33, 36, 0, 36, 36, 36]\n",
      "*** Full sentence: Pulmonary arterial hypertension was defined as systolic pulmonary arterial pressure N 35 mmHg or mean pulmonary arterial pressure N 25 mmHg .\n",
      "ROOT [ 4 ]:  defined , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 12 ]:  mmHg , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 17 ]:  pressure , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 18 ]:  N , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 20 ]:  mmHg , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [3, 3, 5, 5, 0, 5, 10, 10, 10, 6, 5, 13, 0, 13, 13, 18, 18, 0, 0, 21, 0, 21]\n",
      "*** Full sentence: The CHEST-1 study methodology has been pub- lished previously.Patients with CTEPH who were adjudicated to be technically inoperable or had per- sistent/recurrent PH after PEA were eligible if they had a 6MWD of 150–450 m , PVR > 300 dyn s/cm and mean pulmonary artery pressure ( mPAP ) ≥25 mm Hg .\n",
      "ROOT [ 7 ]:  lished , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 51 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 8, 8, 8, 8, 0, 8, 8, 10, 14, 14, 11, 16, 14, 18, 16, 14, 14, 23, 23, 20, 26, 26, 20, 26, 30, 30, 20, 32, 30, 32, 35, 33, 30, 41, 41, 41, 41, 42, 30, 30, 46, 46, 43, 46, 46, 48, 46, 52, 0, 52]\n",
      "*** Full sentence: Haemodynamic parameters are objective indicators of the status of the pulmonary circulation , with reports indicating that PVR , mPAP , RAP and cardiac index are predictive of outcome in patients with CTEPH . Whether the acute effects of rioci- guat on haemodynamics translate into long-term beneﬁts for patients with inoperable CTEPH or persistent/recurrent PH fol- lowing PEA is unknown .\n",
      "ROOT [ 2 ]:  are , pos tag:  VBP , dep:  ROOT\n",
      "ROOT [ 43 ]:  translate , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 56 ]:  lowing , pos tag:  VBG , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 5, 3, 5, 8, 6, 8, 12, 12, 9, 3, 3, 14, 15, 26, 25, 18, 18, 20, 20, 22, 22, 26, 16, 26, 27, 28, 29, 30, 31, 32, 3, 44, 38, 38, 44, 38, 41, 39, 41, 42, 0, 44, 47, 45, 47, 48, 49, 52, 50, 52, 56, 56, 44, 0, 59, 57, 59, 57]\n",
      "*** Full sentence: Treatment of Pulmonary Arterial Hypertension With the Selective Endothelin-A Receptor Antagonist Sitaxsentan Robyn J. Barst , MD , * David Langleben , MD , † David Badesch , MD , ‡ Adaani Frost , MD , § Pulmonary arterial hypertension ( PAH ) is a progressive disease characterized by vasoconstriction and structural changes in small pulmonary arteries , with increased pulmo- nary artery pressure and pulmonary vascular resistance ultimately leading to right heart failure and death ( ) .\n",
      "ROOT [ 0 ]:  Treatment , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 43 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [0, 1, 5, 5, 2, 1, 10, 9, 10, 11, 15, 15, 15, 15, 6, 15, 15, 17, 17, 21, 44, 21, 21, 23, 27, 27, 23, 27, 27, 29, 33, 33, 29, 33, 33, 33, 40, 40, 40, 33, 40, 40, 40, 0, 47, 47, 44, 47, 48, 49, 50, 53, 50, 53, 57, 57, 54, 44, 44, 64, 60, 64, 64, 70, 64, 67, 68, 64, 70, 59, 70, 74, 74, 71, 74, 74, 44, 44, 44]\n",
      "*** Full sentence: Endothelin-1 ( ET ) , a 21-amino acid peptide with vasocon- strictor , mitogenic , and profibrotic effects ( ) , appears to play a key role in the pathobiology of PAH ( ) .\n",
      "ROOT [ 2 ]:  ET , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 21 ]:  appears , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 3, 3, 9, 9, 9, 3, 9, 9, 22, 12, 12, 14, 14, 18, 14, 22, 22, 22, 0, 24, 22, 27, 27, 24, 27, 30, 28, 30, 31, 22, 22, 22]\n",
      "*** Full sentence: Although the Sitaxsentan To Relieve ImpaireD Exercise-1 ( STRIDE-1 ) study demonstrated improved exercise capacity ( as assessed by the six-min walk [ 6MW ] test ) , World Health Organi- zation ( WHO ) functional class ( FC ) , cardiac index , and pulmonary vascular resistance at both sitaxsentan doses studied , i.e. , 100 and 300 mg orally once daily , the safety profile was unacceptable with the 300-mg dose ( ) .\n",
      "ROOT [ 11 ]:  demonstrated , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 31 ]:  zation , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 67 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [5, 3, 5, 5, 12, 11, 11, 11, 11, 11, 5, 0, 15, 15, 12, 15, 18, 15, 18, 26, 26, 26, 26, 26, 26, 19, 15, 15, 30, 15, 30, 0, 34, 32, 34, 37, 53, 39, 37, 39, 37, 43, 37, 43, 43, 48, 48, 43, 48, 52, 52, 49, 68, 53, 57, 55, 60, 57, 57, 53, 63, 63, 60, 68, 67, 67, 68, 0, 68, 69, 73, 73, 70, 68, 68, 68]\n",
      "*** Full sentence: For observation only , an open-label ( OL ) ( 6MW tests , Borg dyspnea scores , and WHO FC assessments third-party blind ) bosentan arm was included .\n",
      "ROOT [ 0 ]:  For , pos tag:  IN , dep:  ROOT\n",
      "ROOT [ 27 ]:  included , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 1, 2, 1, 6, 4, 8, 6, 8, 12, 12, 6, 12, 16, 16, 12, 16, 16, 16, 21, 6, 23, 21, 6, 26, 28, 28, 0, 28]\n",
      "*** Full sentence: Patients who had been randomized , double-blind , to sitax- sentan 50 mg or 100 mg in the STRIDE-2 study received sitaxsentan 100 mg in the extension ; patients who had been randomized to OL bosentan in the STRIDE-2 study continued OL bosentan in the extension .\n",
      "ROOT [ 0 ]:  Patients , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 40 ]:  continued , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 5, 5, 5, 1, 1, 1, 1, 1, 9, 21, 13, 11, 13, 16, 13, 16, 20, 20, 17, 41, 21, 24, 21, 24, 27, 25, 41, 41, 33, 33, 33, 29, 33, 36, 34, 33, 40, 40, 37, 0, 43, 41, 43, 46, 44, 41]\n",
      "*** Full sentence: Two hundred forty-seven patients were random- ized , 245 patients treated : 62 patients received placebo , 62 patients received sitaxsentan 50 mg , 61 patients received sitaxsentan 100 mg , and 60 patients received OL bosentan .\n",
      "ROOT [ 19 ]:  received , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 26 ]:  received , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 5, 20, 7, 5, 11, 10, 11, 5, 15, 14, 15, 11, 15, 20, 19, 20, 0, 20, 23, 20, 23, 26, 27, 0, 27, 30, 27, 27, 27, 34, 35, 27, 37, 35, 35]\n",
      "*** Full sentence: At week 18 ( ) , the change from baseline in WHO FC was significantly better for the sitaxsentan 100-mg group compared with those receiving placebo ( p = 0.04 ) , i.e. , 98 % of the sitaxsentan 100-mg patients improved WHO FC ( 13 % ) or remained unchanged ( 85 % ) , whereas 87 % of the placebo patients improved WHO FC ( 10 % ) or remained unchanged ( 77 % ) .\n",
      "ROOT [ 13 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 41 ]:  improved , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [14, 1, 2, 14, 14, 14, 8, 14, 8, 9, 8, 11, 11, 0, 16, 14, 16, 21, 21, 21, 17, 14, 22, 23, 24, 25, 30, 30, 30, 26, 26, 36, 36, 36, 36, 14, 36, 39, 37, 41, 42, 0, 42, 43, 47, 47, 44, 47, 42, 42, 54, 54, 54, 50, 50, 42, 64, 59, 64, 59, 62, 60, 64, 42, 64, 65, 69, 69, 66, 69, 64, 64, 72, 76, 76, 72, 72, 42]\n",
      "*** Full sentence: Eighteen patients discontinued the STRIDE-2 study because of an adverse event : six ( 10 % ) placebo patients , four ( 7 % ) sitaxsentan 50-mg patients , two ( 3 % ) sitaxsentan 100-mg patients , and six ( 10 % ) OL bosentan patients .\n",
      "ROOT [ 2 ]:  discontinued , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 36 ]:  patients , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 46 ]:  patients , pos tag:  NNS , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 6, 6, 3, 3, 7, 11, 11, 7, 11, 16, 16, 16, 11, 16, 19, 11, 19, 26, 24, 24, 26, 24, 28, 28, 19, 28, 33, 33, 33, 19, 33, 37, 37, 0, 37, 37, 37, 43, 43, 40, 43, 47, 47, 0, 47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: Daily warfarin dosages at week 18 were : 3.7 + 2.7 mg/ day for placebo patients , 2 .8 +1.2 mg/day for 50-mg sitaxsentan patients , 2 .1 + 1.0 mg/day for 100-mg sitaxsentan patients , and 5 . 1 + 2 .9 mg/day for OL bosentan patients .\n",
      "ROOT [ 6 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 20 ]:  mg/day , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 3, 7, 3, 4, 5, 0, 7, 13, 9, 9, 13, 7, 13, 16, 14, 13, 13, 13, 21, 0, 21, 25, 25, 22, 25, 25, 25, 25, 31, 25, 25, 35, 35, 32, 25, 25, 39, 21, 39, 39, 39, 39, 39, 39, 48, 48, 45, 21]\n",
      "*** Full sentence: Although the STRIDE-2 study was not powered to assess comparison between sitaxsentan 100 mg and OL bosentan , during this 18- week study , the incidence of elevated hepatic transaminases was 3 % for sitaxsentan 100 mg and 11 % for OL bosentan ( i.e. , the same as the rate reported in the bosentan package insert ) .\n",
      "ROOT [ 30 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 47 ]:  same , pos tag:  JJ , dep:  ROOT\n",
      "dep_head:  [7, 4, 4, 7, 7, 7, 31, 9, 7, 9, 10, 11, 14, 12, 14, 17, 14, 7, 7, 23, 22, 23, 19, 23, 26, 23, 26, 30, 30, 27, 0, 33, 31, 33, 34, 37, 33, 37, 40, 37, 40, 43, 41, 48, 48, 48, 48, 0, 52, 51, 52, 48, 52, 57, 56, 57, 53, 48, 48]\n",
      "*** Full sentence: After baseline haemodynamic parameters had been recorded , adenosine infusions ( Ever Bright Pharmaceutical Company , Shenyang , China ) were started at a dose of 50 mg ? kg -1 ? min-1 and increased by 50 mg ? kg-1 ? min-1 at 2-min intervals to a maximum dose of 200 mg ? kg- 1 ? min-1 [ 4 , 7 ] .\n",
      "ROOT [ 21 ]:  started , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 29 ]:  kg , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 32 ]:  min-1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 39 ]:  kg-1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 41 ]:  min-1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 53 ]:  kg- , pos tag:  FW , dep:  ROOT\n",
      "ROOT [ 56 ]:  min-1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 60 ]:  7 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [7, 4, 4, 7, 7, 7, 22, 22, 10, 22, 15, 15, 15, 15, 10, 15, 15, 17, 15, 15, 22, 0, 22, 25, 23, 25, 28, 26, 22, 0, 30, 30, 0, 35, 33, 35, 38, 36, 35, 0, 40, 0, 42, 45, 43, 45, 49, 49, 46, 49, 52, 50, 43, 0, 54, 54, 0, 59, 61, 61, 0, 61, 61]\n",
      "*** Full sentence: The recommended dosage of infused adenosine to acutely assess vasodilator responsiveness is 50 mg ? kg-1 ? min-1 , increased by 50 mg ? kg-1 ? min-1 every 2 min to a maximum dose of 350– 500 mg ? kg-1 ? min-1 [ 3 , 4 ] .\n",
      "ROOT [ 11 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 15 ]:  kg-1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 19 ]:  increased , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 24 ]:  kg-1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 29 ]:  min , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 39 ]:  kg-1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 43 ]:  3 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [3, 3, 12, 3, 6, 4, 9, 9, 6, 11, 12, 0, 14, 12, 12, 0, 16, 20, 20, 0, 20, 23, 21, 20, 0, 25, 30, 30, 30, 0, 30, 34, 34, 31, 34, 35, 38, 36, 38, 0, 40, 44, 44, 0, 44, 44, 44, 44]\n",
      "*** Full sentence: The inclu- sion criteria were 6-minute walk distance ( 6MWD ) between 150 and 450 m ; mean pulmonary artery pressure ( mPAP ) greater than 25 mmHg ; PVR greater than 3 mmHg /L/min ; and mean pulmonary capillary wedge pressure ( PCWP ) or left ventricular end-diastolic pressure ( LVEDP ) less than 15 mmHg ( if measured ) .\n",
      "ROOT [ 4 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 46 ]:  left , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 59 ]:  measured , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 5, 0, 8, 8, 5, 8, 8, 8, 8, 16, 13, 13, 12, 5, 5, 21, 21, 18, 21, 21, 21, 30, 25, 28, 26, 30, 47, 30, 31, 35, 35, 32, 30, 30, 30, 42, 42, 42, 30, 42, 42, 42, 42, 0, 50, 50, 47, 50, 50, 50, 56, 56, 57, 50, 60, 60, 0, 60, 60]\n",
      "*** Full sentence: Ambrisentan efficacy The mean increase in 6MWD from the baseline measure- ment was 33.5 m after 12 weeks of treatment and 46.8 m after 24 weeks ( Table 2 ; Figure 2 ) .\n",
      "ROOT [ 1 ]:  efficacy , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 12 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 30 ]:  Figure , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 0, 5, 5, 13, 5, 6, 5, 12, 12, 12, 8, 0, 15, 13, 15, 18, 16, 18, 19, 18, 23, 24, 13, 26, 24, 31, 31, 28, 31, 0, 31, 31, 31]\n",
      "*** Full sentence: The mean change in BNP was 76.9 160.9 ng/L after 12 weeks of treatment and 60.2 248.3 ng/L after 24 weeks of treatment .\n",
      "ROOT [ 5 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 8 ]:  ng/L , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 16 ]:  248.3 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 17 ]:  ng/L , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [3, 3, 6, 3, 4, 0, 8, 6, 0, 9, 12, 10, 12, 13, 12, 17, 0, 0, 18, 21, 19, 21, 22, 18]\n",
      "*** Full sentence: Cardiopulmonary hemodynamics As shown in Table 3 , mPAP was 51.2 16.6 mmHg at baseline , and the mean change from the baseline in mPAP was -6.3 ± 11.2 mmHg after 12 weeks of treatment and - 8.7 ± 13.9 mmHg after 24 weeks of treatment .\n",
      "ROOT [ 9 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 30 ]:  after , pos tag:  IN , dep:  ROOT\n",
      "ROOT [ 41 ]:  after , pos tag:  IN , dep:  ROOT\n",
      "dep_head:  [2, 10, 4, 10, 4, 5, 6, 10, 10, 0, 13, 13, 10, 10, 14, 10, 10, 20, 20, 26, 20, 23, 21, 23, 24, 10, 26, 26, 30, 31, 0, 33, 31, 33, 34, 33, 39, 39, 33, 41, 42, 0, 44, 42, 44, 45, 42]\n",
      "*** Full sentence: Those patients continued to receive the lowest dose of epoprostenol or trepostinil that controlled symptoms ( pre-PGI2 epoprostenol dose , 48 ± 9 ng/ kg / min ; pre-PGI2 trepostinil dose , 211 ± 82 ng/kg /min ; post- PGI2 epoprostenol dose , 44 ± 7 ng/kg/min ; post- PGI2 trepostinil dose , 219 ± 74 ng/kg/min ) [ Fig 1 ] .\n",
      "ROOT [ 2 ]:  continued , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 30 ]:  dose , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 41 ]:  dose , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 51 ]:  dose , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 59 ]:  Fig , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 5, 3, 8, 8, 5, 8, 9, 10, 10, 5, 15, 13, 19, 19, 19, 31, 19, 27, 27, 27, 27, 27, 27, 19, 19, 30, 31, 0, 31, 34, 31, 36, 42, 42, 42, 40, 41, 42, 0, 42, 45, 42, 47, 45, 42, 52, 52, 52, 0, 52, 55, 52, 57, 55, 52, 60, 0, 60, 60, 60]\n",
      "*** Full sentence: The condition of one patient in the transitioned group deteriorated from class II to class IV compared to four patients in the nontransitioned group , who changed from class I/II to class III/IV by 12 months ( p = 0.02 ) [ Fig 3 , Tables 2 , 3 ] .\n",
      "ROOT [ 9 ]:  deteriorated , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 42 ]:  Fig , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 10, 2, 5, 3, 5, 9, 9, 6, 0, 10, 13, 11, 15, 10, 15, 10, 17, 20, 18, 20, 24, 24, 21, 24, 27, 24, 27, 28, 17, 32, 17, 32, 32, 36, 34, 40, 40, 40, 32, 40, 43, 0, 43, 43, 43, 46, 46, 46, 43, 43]\n",
      "*** Full sentence: Mean PA pressures ( derived from pre-study en- rollment catheterizations ) were lower in patients who transitioned therapy than in those who did not ( 38 ± 2.6 vs 56 ± 2.6 mm Hg , respectively ; p < 0.05 ) [ Table 1 ] .\n",
      "ROOT [ 11 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 42 ]:  Table , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [3, 3, 12, 3, 3, 5, 10, 7, 10, 6, 3, 0, 12, 12, 14, 17, 15, 17, 17, 19, 20, 23, 21, 23, 23, 27, 23, 27, 27, 31, 34, 33, 34, 29, 23, 23, 12, 12, 40, 38, 38, 43, 0, 43, 43, 43]\n",
      "*** Full sentence: Discussion Our open-label , multicenter pilot study demon- strates that transitioning from therapy with prostacy- clin infusion to bosentan is possible in a minority of PAH patients ( 10 of 22 patients ; 45 % ) .\n",
      "ROOT [ 0 ]:  Discussion , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 8 ]:  strates , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [0, 3, 1, 1, 7, 7, 1, 1, 0, 20, 20, 11, 12, 11, 17, 17, 14, 17, 18, 9, 20, 20, 24, 22, 24, 27, 25, 27, 27, 29, 32, 30, 27, 35, 27, 27, 9]\n",
      "*** Full sentence: This is slightly higher than our 1-year transition rate of 32 % , possibly reflecting the greater impairment of our patient population ( mean initial 6MWD : our study , 304 ± 31 m ; Suleman and Frost12 study , 392 ± 21 m ) , but both studies are in agreement that a minority of patients can be transitioned successfully in the long term .\n",
      "ROOT [ 1 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 28 ]:  study , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 0, 4, 2, 4, 9, 9, 9, 5, 9, 12, 10, 2, 15, 2, 18, 18, 15, 18, 22, 22, 19, 24, 25, 26, 18, 26, 29, 0, 29, 32, 29, 34, 32, 29, 29, 36, 36, 36, 36, 42, 36, 44, 42, 36, 29, 29, 49, 50, 29, 50, 51, 60, 55, 60, 55, 56, 60, 60, 52, 60, 60, 65, 65, 62, 50]\n",
      "*** Full sentence: ORIGINAL ARTICLE The effect of bosentan on matrix metalloproteinase-9 levels in patients with systemic sclerosis-induced pulmonary hypertension Gianluigi Giannelli , Florenzo Iannone b , Felice Marinosc Giovanni Lapadula b and Salvatore Antonac ntroduction Systemic sclerosis ( SSc ) is a connective tissue disease characterized by vascular disturbances , autoimmune abnormalities and an altered turnover of extracellular matrix ( ECM ) components that can involve almost all the tissues and organs of the human body .\n",
      "ROOT [ 1 ]:  ARTICLE , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 38 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [2, 0, 4, 39, 4, 5, 4, 10, 10, 7, 10, 11, 12, 17, 17, 17, 19, 19, 13, 19, 23, 23, 19, 23, 29, 29, 29, 29, 23, 29, 32, 35, 35, 35, 29, 37, 35, 37, 0, 43, 43, 43, 39, 43, 44, 47, 45, 47, 50, 47, 50, 54, 54, 50, 54, 57, 55, 61, 61, 61, 54, 64, 64, 61, 66, 68, 68, 64, 68, 68, 68, 74, 74, 71, 39]\n",
      "*** Full sentence: Inclusion criteria were : isolated PAH in functional classes II –III according to the WHO classification , and a 6-min walking distance between 60 m and 450 7 Exclusion criteria were : concomitant pulmonary fibrosis and/or liver disease .\n",
      "ROOT [ 2 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 30 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 3, 3, 5, 5, 9, 7, 5, 3, 31, 12, 16, 16, 13, 12, 12, 22, 22, 22, 31, 22, 25, 23, 25, 30, 30, 30, 25, 0, 31, 35, 35, 31, 35, 38, 35, 31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: Results Of 376 patients receiving background endothelin receptor antagonist ( ERA ) and phosphodiesterase-5 inhibitor ( PDE-5i ) therapy , 115 had WHO FC II symptoms and 255 had WHO FC III symptoms at baseline .\n",
      "ROOT [ 0 ]:  Results , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 21 ]:  had , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 1, 4, 2, 4, 9, 8, 9, 5, 9, 9, 11, 9, 15, 9, 15, 18, 15, 22, 19, 19, 0, 26, 25, 26, 29, 26, 26, 22, 33, 32, 33, 29, 33, 34, 29]\n",
      "*** Full sentence: Conclusion The addition of selexipag to background double combination therapy with an ERA and PDE-5i provides an incremental benefit similar to that seen in the overall population , including in patients with WHO FC II or III symptoms at baseline .\n",
      "ROOT [ 0 ]:  Conclusion , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 15 ]:  provides , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [0, 3, 16, 3, 4, 7, 3, 10, 10, 7, 10, 13, 11, 13, 13, 0, 19, 19, 16, 19, 20, 21, 22, 23, 27, 27, 24, 27, 27, 29, 30, 31, 38, 35, 38, 35, 35, 32, 38, 39, 16]\n",
      "*** Full sentence: By the end of the study , 30 patients ( 16.8 % ) in the selex- ipag arm and 34 patients ( 17.3 % ) in the placebo arm had died ( HR 1.06 ; 95 % CI 0.65–1.73 ) ( Table S2 in the ESM ; Fig .\n",
      "ROOT [ 30 ]:  died , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 47 ]:  Fig , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [31, 3, 1, 3, 6, 4, 31, 9, 31, 9, 12, 9, 9, 9, 18, 18, 18, 14, 18, 21, 18, 21, 24, 21, 21, 18, 29, 29, 26, 31, 0, 31, 31, 33, 33, 37, 39, 39, 33, 31, 48, 43, 48, 43, 46, 44, 48, 0, 48]\n",
      "*** Full sentence: Objective : The Pulmonary Hypertension And Cell-Therapy ( PHACeT ) trial was a phase 1 , dose- escalating clinical study of the tolerability of culture-derived endothelial progenitor cells ( EPCs ) , transiently transfected with endothelial NO-synthase ( eNOS ) , in patients with PAH refractory to PAH- specific therapies .\n",
      "ROOT [ 0 ]:  Objective , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 11 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 1, 5, 5, 12, 5, 11, 11, 11, 11, 5, 0, 14, 20, 14, 14, 14, 20, 20, 12, 20, 23, 21, 23, 28, 27, 28, 24, 28, 28, 28, 12, 34, 12, 34, 37, 39, 39, 35, 34, 12, 12, 42, 43, 46, 44, 46, 47, 50, 47, 12]\n",
      "*** Full sentence: Mean pulmonary arterial pressure ( mPAP ) at baseline was 55±13 mm Hg ( median 57 mm Hg ) , with a cardiac output ( CO ) of 4.91±1.87 L/m ( median 5.47 L/m ) and a calculated total pulmonary resistance ( TPR ) of 1062±585 dynes/sec * cm5 ( median 800 dynes/sec * cm5 ) ( Table 1b ) .\n",
      "ROOT [ 9 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 12 ]:  Hg , pos tag:  FW , dep:  ROOT\n",
      "ROOT [ 48 ]:  cm5 , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 10, 4, 4, 4, 4, 8, 0, 12, 10, 0, 13, 13, 17, 18, 13, 13, 13, 13, 24, 24, 21, 26, 24, 26, 24, 30, 28, 24, 24, 34, 32, 24, 24, 41, 41, 41, 41, 24, 41, 41, 43, 41, 45, 48, 49, 0, 55, 53, 51, 54, 55, 49, 55, 59, 59, 49, 59, 49]\n",
      "*** Full sentence: The acute effects of inhaled nitric oxide and inhaled treprostinil on mean pulmonary arterial pres- sure ( PAPmean ) are shown in .\n",
      "ROOT [ 2 ]:  effects , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 15 ]:  sure , pos tag:  JJ , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 7, 7, 4, 3, 3, 9, 10, 14, 14, 15, 16, 0, 18, 21, 18, 21, 16, 21, 16]\n",
      "*** Full sentence: Inhaled treprostinil was associated with a trend toward improvement 12 weeks in peak 6MWD ( baseline 339 ± 86 , 12 week , peak post-inhalation 406 ± 121 m , 67-m change ; p = 0.01 ) ( ) .\n",
      "ROOT [ 3 ]:  associated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 37 ]:  ( , pos tag:  -LRB- , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 4, 7, 5, 7, 8, 11, 7, 11, 12, 7, 14, 18, 18, 14, 18, 18, 22, 18, 4, 25, 4, 29, 29, 29, 25, 25, 32, 25, 25, 36, 36, 25, 36, 0, 38, 38]\n",
      "*** Full sentence: The mean improve- ment in walk distance of 67 m was , in fact , greater than reported in the pivotal monotherapy trials for epoprostenol ( 31 m ) , subcutaneous treprostinil ( 16 m ) , iloprost ( 36 m ) , bosentan ( 35 m ) , and sildenafil ( 45 m ) .\n",
      "ROOT [ 10 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 37 ]:  iloprost , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 11, 4, 7, 5, 7, 10, 8, 0, 11, 11, 13, 11, 11, 18, 16, 18, 23, 23, 23, 19, 23, 24, 28, 28, 23, 28, 11, 32, 4, 32, 35, 32, 32, 32, 0, 41, 41, 38, 41, 38, 38, 44, 47, 44, 44, 44, 44, 44, 51, 54, 51, 51, 38]\n",
      "*** Full sentence: In addition , there appeared to be a sustained effect of inhaled treprostinil , with trough 6MWD improving by 49 m . There appears to be a trend to improved exercise capacity with the higher ( 45-µg ) dose .\n",
      "ROOT [ 4 ]:  appeared , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 23 ]:  appears , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [5, 1, 5, 5, 0, 7, 5, 10, 10, 7, 10, 13, 11, 7, 7, 17, 18, 15, 18, 21, 19, 5, 24, 0, 26, 24, 28, 26, 30, 28, 32, 30, 32, 39, 39, 39, 39, 39, 33, 24]\n",
      "*** Full sentence: The small size of the nebulizer , 4 times daily dosing , and short ( less than 1 min ) treatment times are all attractive aspects to inhaled treprostinil therapy .\n",
      "ROOT [ 2 ]:  size , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 22 ]:  are , pos tag:  VBP , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 6, 4, 3, 11, 8, 11, 3, 11, 11, 11, 19, 18, 18, 19, 11, 11, 22, 23, 0, 23, 26, 23, 28, 26, 30, 28, 23]\n",
      "*** Full sentence: A large , phase III , multinational , randomized , placebo-controlled trial examining the efficacy and safety of inhaled trepro- stinil as add-on therapy to bosentan ( Treprostinil Inha- lation Used for the Management of Pulmonary Hyper- tension [ TRIUMPH I ] ) is currently ongoing .\n",
      "ROOT [ 4 ]:  III , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 29 ]:  lation , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 43 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [5, 5, 5, 5, 0, 5, 12, 12, 12, 12, 12, 5, 12, 15, 13, 15, 15, 15, 21, 21, 18, 13, 24, 26, 26, 22, 26, 29, 30, 0, 30, 31, 34, 32, 34, 38, 36, 35, 40, 44, 40, 41, 40, 0, 46, 44, 44]\n",
      "*** Full sentence: Of the 34 patients in the conventional group , 27 patients died of cardiopulmonary causes , 1 died of a traffic accident , 1 had lung transplantation and 1 patient received continu- ous intravenous infusion of epoprostenol during a mean follow-up period of 44 ± 45 months .\n",
      "ROOT [ 24 ]:  had , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 30 ]:  received , pos tag:  JJ , dep:  ROOT\n",
      "dep_head:  [12, 4, 4, 1, 4, 8, 8, 5, 12, 11, 12, 18, 12, 15, 13, 18, 18, 25, 18, 22, 22, 19, 25, 25, 0, 27, 25, 27, 30, 27, 0, 35, 35, 35, 31, 35, 36, 35, 42, 42, 42, 38, 42, 45, 43, 47, 45, 31]\n",
      "*** Full sentence: The present study describes the results of the Combination Therapy of Bosentan and aeroso- lised Iloprost in Idiopathic Pulmonary Arterial Hypertension ( COMBI ) trial , a German multi- centre study addressing the safety and efficacy of adding inhaled iloprost to idiopathic PAH ( IPAH ) patients already receiving bosentan ther- apy ( Clinical Trials ID : NCT00120380 ) .\n",
      "ROOT [ 3 ]:  describes , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 51 ]:  apy , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [3, 3, 4, 0, 6, 4, 6, 10, 10, 7, 10, 11, 6, 15, 16, 6, 16, 25, 21, 21, 25, 25, 25, 25, 17, 16, 31, 31, 30, 31, 16, 31, 34, 32, 34, 34, 34, 37, 6, 39, 39, 47, 47, 47, 47, 47, 41, 49, 47, 51, 49, 0, 52, 56, 56, 52, 56, 56, 52, 52]\n",
      "*** Full sentence: Inclusion criteria were as follows : 1 ) aged 18–75 yrs ; 2 ) bosentan therapy for .3 months ; 3 ) stable functional class III for .3 months ; 4 ) 6-min walk distance ( 6MWD ) between 150 and 425 m ; and 5 ) serum aminotransferase levels two times below the upper level of normal .\n",
      "ROOT [ 2 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 34 ]:  distance , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 5, 3, 5, 9, 7, 5, 11, 9, 9, 16, 13, 16, 9, 16, 17, 17, 9, 26, 21, 26, 26, 26, 9, 26, 27, 27, 26, 33, 31, 26, 35, 0, 35, 35, 35, 35, 43, 40, 40, 39, 35, 35, 50, 46, 49, 50, 35, 52, 53, 50, 56, 56, 53, 56, 57, 50]\n",
      "*** Full sentence: In the iloprost groups , the 6MWD was 317¡74 m at baseline and 309 ¡ 124 m after 12 weeks ( mean change -9 ¡ 100 m ; p50.65 ) .\n",
      "ROOT [ 7 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 24 ]:  ¡ , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [8, 4, 4, 1, 8, 7, 8, 0, 10, 8, 10, 11, 10, 15, 10, 17, 15, 15, 20, 18, 22, 23, 15, 25, 0, 27, 25, 25, 25, 29, 25]\n",
      "*** Full sentence: Patients with IPAH showed a mean baseline systolic PAP of 81 mm Hg ( 95 % CI 70-93 mm Hg ) , a diastolic PAP of 32 mm Hg ( 95 % CI 26-39 mm Hg ) , and a mean PAP of 51 mm Hg ( 95 % CI 43-59 mm Hg ) .\n",
      "ROOT [ 3 ]:  showed , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 12 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 28 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 45 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 1, 2, 0, 9, 9, 9, 9, 4, 9, 12, 10, 0, 13, 16, 20, 18, 19, 20, 13, 13, 13, 25, 25, 13, 25, 28, 26, 0, 29, 32, 36, 34, 35, 36, 29, 29, 29, 29, 42, 42, 29, 42, 45, 43, 0, 46, 49, 53, 52, 50, 53, 46, 46, 46]\n",
      "*** Full sentence: In patients with IPAH , inhalation of aerosolized iloprost did not significantly lower blood pressure ( ) , neither at systolic ( 124 mm Hg [ 95 % CI 115- 133 mm Hg ] after iloprost inhalation [ P = .234 ] vs pre-iloprost ) nor at diastolic level ( 68 mm Hg [ 95 % CI 61-75 mm Hg ] after iloprost inhalation [ P = .204 ] vs pre-iloprost ) .\n",
      "ROOT [ 12 ]:  lower , pos tag:  JJR , dep:  ROOT\n",
      "ROOT [ 24 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 32 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 52 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 59 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 61 ]:  after , pos tag:  IN , dep:  ROOT\n",
      "dep_head:  [13, 1, 2, 3, 13, 13, 6, 9, 7, 13, 13, 13, 0, 15, 13, 13, 13, 13, 20, 13, 20, 21, 24, 22, 0, 25, 28, 25, 25, 29, 32, 33, 0, 33, 33, 37, 35, 39, 40, 33, 33, 33, 33, 43, 43, 33, 33, 49, 47, 53, 52, 53, 0, 53, 56, 60, 58, 59, 60, 0, 60, 0, 64, 62, 64, 64, 64, 62, 62, 62, 70, 62, 62]\n",
      "*** Full sentence: However , there was a significant correla- tion between FMD and the subsequent percent de- crease in mean PAP in response to inhalation of aerosolized iloprost ( ) with Spearman ’ s corre- lation coefficients of r = 0.65 ( P = .003 ) for FMDmm and r = 0.61 ( P = .007 , ) for FMD % , respectively .\n",
      "ROOT [ 3 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 49 ]:  0.61 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 53 ]:  .007 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 0, 8, 8, 8, 4, 8, 9, 10, 14, 14, 10, 16, 14, 16, 19, 17, 8, 20, 21, 22, 23, 26, 24, 8, 4, 4, 35, 30, 30, 30, 35, 29, 35, 39, 39, 36, 41, 43, 43, 35, 35, 4, 45, 46, 46, 50, 0, 52, 54, 54, 0, 54, 54, 54, 59, 57, 57, 57, 54]\n",
      "*** Full sentence: Iloprost has , however , been shown to be at least equallyor even more effectivein lowering increased PVR than inhaled NO .\n",
      "ROOT [ 6 ]:  shown , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 11 ]:  equallyor , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [7, 7, 7, 7, 7, 7, 0, 9, 7, 11, 12, 0, 14, 15, 12, 15, 18, 16, 20, 15, 12, 12]\n",
      "*** Full sentence: Iloprost was inhaled at a concentration of 4 /Jg/ml ( 6 min inhalation time ; n 44 ) and treprostinil was inhaled at concentrations of 4 /Jg/ml ( 6 min inhalation ; n 14 ) , 8 /Jg/ml ( 6 min inhalation ; n 14 ) or 16 /Jg/ml ( 3 min inhalation ; n 16 ) .\n",
      "ROOT [ 2 ]:  inhaled , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 26 ]:  /Jg/ml , pos tag:  SYM , dep:  ROOT\n",
      "ROOT [ 32 ]:  n , pos tag:  CC , dep:  ROOT\n",
      "ROOT [ 36 ]:  8 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 37 ]:  /Jg/ml , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 43 ]:  n , pos tag:  CC , dep:  ROOT\n",
      "ROOT [ 48 ]:  /Jg/ml , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 54 ]:  n , pos tag:  CC , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 6, 4, 6, 7, 3, 9, 12, 14, 14, 9, 3, 1, 16, 16, 16, 22, 22, 3, 22, 23, 24, 25, 0, 31, 30, 31, 33, 31, 0, 33, 33, 33, 0, 0, 42, 41, 42, 44, 42, 0, 44, 45, 44, 44, 0, 53, 52, 53, 55, 53, 0, 55, 55, 55]\n",
      "*** Full sentence: The treprostinil dose of 15 /Jg was either generated during 18 cycles ( Optineb filled with 100 /Jg/ml treprostinil , n 6 ) , 9 cycles ( 200 /Jg/ml treprostinil , n 6 ) , 3 cycles ( 600 /Jg/ml treprostinil , n 21 ) , 2 cycles ( 1 ,000 /Jg/ml trepro- stinil , n 7 ) , or 1 cycle ( 2,000 / Jg /ml treprostinil , n 8 ) .\n",
      "ROOT [ 2 ]:  dose , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 8 ]:  generated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 51 ]:  /Jg/ml , pos tag:  . , dep:  ROOT\n",
      "ROOT [ 53 ]:  stinil , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 67 ]:  treprostinil , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 3, 4, 3, 9, 9, 0, 9, 12, 10, 15, 15, 12, 15, 19, 19, 16, 19, 19, 21, 19, 12, 26, 12, 26, 30, 30, 26, 30, 30, 32, 30, 26, 37, 26, 37, 41, 41, 37, 41, 41, 43, 41, 37, 48, 37, 48, 48, 50, 0, 54, 0, 54, 54, 56, 54, 54, 54, 62, 54, 62, 66, 66, 68, 66, 0, 68, 68, 70, 68, 68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: All patients received the following medications concomitantly after withdrawal of cardiopulmonary bypass : dopamine 5 μg/kg/min , olprinone 0.3 μg/kg/min and , if needed , dexmedetomidine 0.5–1 μg/kg/h .\n",
      "ROOT [ 2 ]:  received , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 13 ]:  dopamine , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 25 ]:  dexmedetomidine , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 6, 6, 3, 8, 3, 8, 9, 12, 10, 3, 0, 16, 14, 16, 26, 20, 18, 18, 18, 24, 18, 26, 0, 26, 26, 26]\n",
      "*** Full sentence: The dose was then advanced every 3–4 days to 50– 100 mg three times daily , as tolerated , while we monitored symptoms ( dyspnea , chest pain , or lightheadedness with exertion , and lower extremity edema ) , systemic blood pressure , cardi- ac echocardiograms and 6 min walk distances .\n",
      "ROOT [ 4 ]:  advanced , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 11 ]:  mg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 42 ]:  pressure , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 3, 5, 5, 0, 8, 8, 5, 5, 9, 12, 0, 14, 12, 12, 12, 18, 12, 12, 22, 22, 12, 22, 23, 23, 25, 28, 25, 28, 28, 28, 31, 32, 31, 28, 38, 38, 28, 23, 12, 43, 43, 0, 43, 46, 47, 43, 47, 52, 52, 52, 47, 43]\n",
      "*** Full sentence: INO did not affect mean systemic blood pressure , either alone ( from 9073 to 8873 mm Hg , P ns ) , or when added to sildenafil ( from 7573 to 7774 mm Hg , P ns ) .\n",
      "ROOT [ 3 ]:  affect , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 17 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 0, 8, 8, 8, 4, 4, 11, 4, 11, 11, 13, 13, 17, 15, 0, 18, 21, 18, 18, 18, 18, 26, 18, 26, 27, 28, 28, 30, 30, 34, 35, 32, 38, 38, 35, 28, 18]\n",
      "*** Full sentence: The average total dose of sildenafil given acutely was 122 .5744 mg ( mean7SD ) , start- ing with doses of 25–50 mg , depending on age 25 mg twice a day , but her exertional dyspnea continued to worsen .\n",
      "ROOT [ 8 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 11 ]:  mg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 17 ]:  ing , pos tag:  VBG , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 9, 4, 5, 6, 7, 0, 9, 9, 0, 12, 12, 12, 12, 18, 0, 18, 19, 20, 23, 21, 18, 18, 25, 29, 27, 26, 32, 32, 29, 18, 18, 37, 37, 38, 18, 40, 38, 38]\n",
      "*** Full sentence: Baseline hemodynamic data were as follows : mean cardiac index , 2.4 ± 0.5 L/min/m2 ; mean PVR , 712 ± 213 dyne/s-1/cm-5 ; mean mixed venous saturation , 61 ± 9 % ; and mean pulmonary arterial pressure , 52 ± 10 mm Hg .\n",
      "ROOT [ 3 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 9 ]:  index , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 16 ]:  mean , pos tag:  VB , dep:  ROOT\n",
      "dep_head:  [3, 3, 4, 0, 6, 4, 6, 10, 10, 0, 10, 15, 15, 15, 10, 10, 0, 25, 18, 21, 18, 23, 21, 18, 17, 28, 28, 25, 28, 33, 33, 33, 28, 25, 25, 25, 39, 39, 36, 39, 45, 45, 44, 45, 39, 17]\n",
      "*** Full sentence: RESEARCH ARIES-3 : Ambrisentan Therapy in a Diverse Population of Patients with Pulmonary Hypertension David B. Badesch,1 Jeremy Feldman,2 Anne Keogh,3 Michael A. Mathier,4 Ronald J. Oudiz,5 Shelley Shapiro,6 Harrison W. Farber,7 Michael McGoon,8 Adaani Frost,9 Martine Allard,10 Darrin Despain,10 SUMMARY Introduction : Ambrisentan is an oral , once daily , endothelin receptor antagonist approved for treatment of pulmonary arterial hypertension ( PAH ) .\n",
      "ROOT [ 1 ]:  ARIES-3 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 4 ]:  Therapy , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 16 ]:  Badesch,1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 17 ]:  Jeremy , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 31 ]:  Farber,7 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 34 ]:  Adaani , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 44 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 5, 0, 5, 9, 9, 6, 9, 10, 9, 14, 16, 16, 12, 0, 0, 20, 18, 18, 23, 18, 18, 26, 18, 28, 26, 32, 32, 32, 0, 35, 35, 0, 35, 42, 42, 42, 42, 42, 45, 45, 45, 0, 54, 54, 54, 50, 48, 54, 53, 54, 45, 54, 55, 56, 57, 61, 61, 58, 61, 61, 61, 45]\n",
      "*** Full sentence: Eligible patients received 5 mg of ambrisen- tan ( Letairis R , Gilead Sciences , Inc. ) , once daily for 24 weeks .\n",
      "ROOT [ 2 ]:  received , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 7 ]:  tan , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 5, 3, 5, 6, 0, 8, 11, 8, 11, 14, 11, 14, 14, 11, 8, 20, 8, 20, 23, 21, 8]\n",
      "*** Full sentence: Long-Term Survival and Clinical Worsening Six subjects died during this 24-week study : cardio-respiratory ar- rest ( n = 2 ) , worsening PH ( n = 2 ) , left ventricular failure ( n = 1 ) , and illicit drug toxicity ( n = 1 ) .\n",
      "ROOT [ 7 ]:  died , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 15 ]:  rest , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 30 ]:  left , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 8, 2, 5, 6, 7, 2, 0, 8, 12, 12, 9, 12, 16, 14, 0, 20, 19, 20, 16, 20, 16, 16, 23, 28, 28, 28, 24, 24, 31, 0, 33, 31, 33, 36, 37, 33, 33, 31, 31, 43, 43, 31, 43, 46, 47, 43, 43, 31]\n",
      "*** Full sentence: The increase in 6MWD observed in the subpopulation of PAH patients receiving ambrisentan monotherapy ( Table 2 ) was consistent with that ob- served for similar PAH populations in the ARIES-1 and ARIES-2 trials [ 14 ] , thus reconfirming the efficacy of ambrisentan in the population for which it is currently indicated .\n",
      "ROOT [ 4 ]:  observed , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 18 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 23 ]:  served , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 5, 2, 3, 0, 5, 8, 6, 8, 11, 9, 11, 14, 12, 14, 14, 16, 14, 0, 19, 20, 21, 19, 0, 24, 28, 28, 25, 28, 34, 34, 31, 31, 29, 36, 24, 36, 24, 40, 24, 42, 40, 42, 43, 42, 47, 45, 53, 48, 53, 53, 53, 47, 24]\n",
      "*** Full sentence: The oral endothelin-1 receptor antagonist bosen- tan has been established as a safe and effective treatment in patients with idiopathic , scleroderma and HIV-related PAH , chronic thromboembolic PH , Eisenmenger syndrome and porto-PH [ 14–18 ] .\n",
      "ROOT [ 4 ]:  antagonist , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 5 ]:  bosen- , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 9 ]:  established , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 5, 0, 0, 10, 10, 10, 0, 10, 16, 16, 13, 13, 11, 16, 17, 18, 22, 20, 19, 22, 22, 24, 25, 29, 29, 25, 29, 32, 29, 32, 32, 36, 25, 36, 10]\n",
      "*** Full sentence: The adjusted mean change at week 16 was 50 50 60 60 29.8 meters for the sildenafil group and 1.0 meter for the placebo group , giving an adjusted treatment difference of 28.8 meters ( 95 % CI , 13.9 to 43.8 meters ; P < 0.001 ) The protocol-specified last-observation-carried-forward analysis produced a relative difference of 26.0 meters ( CI , 10.8 to 41.2 meters ) between groups .\n",
      "ROOT [ 7 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 52 ]:  produced , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 8, 4, 5, 6, 0, 14, 14, 14, 14, 14, 8, 14, 18, 18, 15, 14, 21, 14, 21, 25, 25, 22, 8, 8, 31, 31, 31, 27, 31, 34, 32, 38, 37, 38, 8, 38, 42, 42, 43, 38, 38, 8, 47, 45, 45, 52, 52, 52, 53, 0, 56, 56, 53, 56, 59, 57, 61, 53, 61, 65, 65, 66, 61, 61, 53, 68, 53]\n",
      "*** Full sentence: RESEARCH Impact of Sildenaﬁl Therapy on Pulmonary Arterial Hypertension in Adults with Congenital Heart Disease Xian-Ling Lu,1,2 Chang-Ming Xiong,1 Guang-Liang Shan,3 Xian-Yang Zhu,4 Bing-Xiang Wu,5 Guang-Hua Wu,6 Zhi-Hong Liu,1 Xin-Hai Ni,1 Xian-Sheng Cheng,1 Qing Gu,1 Zhi-Hu Zhao,1 Duan-Zhen Zhang,4 Wei-Min Li,5 Cheng Zhang,6 Hong-Yan Tian,7 Ya-Juan Guo,7 Tao Guo,8 Hong-Min Liu,8 Wei-Jun Zhang,9 Hong Gu,9 SUMMARY Background : It has been demonstrated that sildenafil is effective in patients with pul- monary arterial hypertension ( PAH ) .\n",
      "ROOT [ 1 ]:  Impact , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 18 ]:  Xiong,1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 19 ]:  Guang-Liang , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 22 ]:  Zhu,4 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 23 ]:  Bing-Xiang , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 25 ]:  Guang-Hua , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 26 ]:  Wu,6 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 29 ]:  Xin-Hai , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 33 ]:  Qing , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 37 ]:  Duan-Zhen , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 38 ]:  Zhang,4 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 39 ]:  Wei-Min , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 40 ]:  Li,5 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 41 ]:  Cheng , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 45 ]:  Ya-Juan , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 51 ]:  Wei-Jun , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 56 ]:  Background , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 61 ]:  demonstrated , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 5, 3, 2, 9, 9, 6, 9, 10, 2, 15, 15, 12, 2, 16, 2, 0, 0, 22, 23, 0, 0, 24, 0, 0, 30, 30, 0, 30, 34, 34, 0, 36, 34, 34, 0, 0, 0, 0, 0, 44, 42, 46, 0, 51, 51, 51, 51, 52, 0, 52, 55, 57, 57, 0, 57, 62, 62, 62, 0, 65, 65, 62, 65, 66, 67, 68, 73, 73, 73, 69, 73, 73, 73, 62]\n",
      "*** Full sentence: Objective : In this prospec- tive , open-label , uncontrolled and multicenter study , 60 patients with PAH related to CHD received oral sildenafil ( 75 mg/day ) for 12 weeks .\n",
      "ROOT [ 0 ]:  Objective , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 21 ]:  received , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 1, 22, 6, 6, 3, 13, 13, 8, 8, 10, 10, 3, 22, 16, 22, 16, 17, 16, 19, 20, 0, 24, 22, 24, 27, 24, 24, 22, 31, 29, 22]\n",
      "*** Full sentence: Results : Oral sidenafil significantly increased SMWT distances ( 422.94 ± 76.95 m vs. 371.99 ± 78.73 m , P < 0.0001 ) .\n",
      "ROOT [ 0 ]:  Results , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 5 ]:  increased , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 1, 4, 6, 6, 0, 8, 6, 8, 13, 10, 13, 8, 13, 18, 15, 18, 14, 18, 18, 22, 6, 22, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: The mean SMWT distance increased by 50 .94 m from 371 . 99 ± 78.73 m at baseline to 422 .94 ± 76.95 m at week 12 ( P < 0.0001 ) .\n",
      "ROOT [ 3 ]:  distance , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 15 ]:  m , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 30 ]:  0.0001 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 0, 4, 5, 9, 9, 6, 5, 10, 4, 16, 16, 16, 0, 16, 17, 16, 22, 22, 19, 24, 16, 24, 25, 26, 29, 31, 31, 0, 31, 31]\n",
      "*** Full sentence: The mean pulmonary artery pressure decreased by 8 % ( 80.19 ± 20.76 mmHg at baseline and 73.77 ± 18.24 mmHg at week 12 ; P = 0.0002 ) .\n",
      "ROOT [ 5 ]:  decreased , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 20 ]:  mmHg , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 27 ]:  0.0002 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [5, 5, 5, 5, 6, 0, 6, 9, 7, 9, 12, 9, 14, 12, 12, 15, 12, 19, 12, 21, 0, 28, 22, 23, 28, 27, 28, 0, 28, 28]\n",
      "*** Full sentence: The most frequent adverse events were headache and flushing , which were reported in 5.0 % and 1 . 67 % in our patients , respectively .\n",
      "ROOT [ 5 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 20 ]:  % , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [5, 3, 5, 5, 6, 0, 6, 7, 7, 7, 13, 13, 7, 13, 16, 14, 16, 19, 6, 21, 0, 21, 24, 22, 21, 21, 21]\n",
      "*** Full sentence: Other ad - verse effects reported in previous studies in patients receiving oral sildenafil therapy , including myalgia , dizziness and diarrhea , in- digestion , visual disturbance , skin rashes , were not found in this study .\n",
      "ROOT [ 4 ]:  effects , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 34 ]:  found , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [5, 4, 4, 5, 0, 5, 6, 9, 7, 9, 10, 11, 15, 15, 12, 15, 15, 17, 18, 18, 20, 20, 20, 15, 35, 25, 28, 25, 28, 31, 28, 35, 35, 35, 0, 35, 38, 36, 35]\n",
      "*** Full sentence: The data collected in this prospective , open-label , uncontrolled and mul- ticenter study of a medium sample size showed that oral silde- nafil therapy for 12 weeks in PAH with CHD in adults significantly improved exercise capacity , Borg dyspnea score , WHO functional class , and hemodynamic parameters with well tolerated , which confirmed and extended previous studies .\n",
      "ROOT [ 1 ]:  data , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 19 ]:  showed , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 3, 4, 5, 6, 6, 8, 8, 10, 10, 14, 20, 14, 19, 19, 19, 15, 0, 36, 23, 24, 25, 36, 25, 28, 26, 28, 29, 25, 31, 25, 33, 36, 20, 38, 36, 20, 42, 42, 20, 42, 46, 46, 42, 46, 46, 50, 46, 50, 53, 51, 50, 56, 50, 56, 56, 60, 58, 20]\n",
      "*** Full sentence: Following multiple- dose administration of macitentan , ACT-132577 was slowly formed ( tmax , 9 hours ) and slowly eliminated , with an elimination half-life of approximately 48 hours.15 ACT-132577 accumulated around 8.5-fold af- ter multiple -dose administration .\n",
      "ROOT [ 10 ]:  formed , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 37 ]:  administration , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [11, 3, 4, 1, 4, 5, 11, 11, 11, 11, 0, 11, 11, 13, 16, 13, 11, 11, 20, 11, 20, 20, 25, 25, 31, 25, 28, 30, 30, 26, 22, 31, 32, 31, 38, 38, 38, 0, 38]\n",
      "*** Full sentence: In SERAPHIN , it was shown that for the 10 mg macitentan PAH patient group ( n 187 ) , the mean arithmetic Ctrough ( standard deviation , SD ) for macitentan at steady state was 291 155 ng/mL .19 These values were higher than those observed at steady state in healthy subjects dosed with 10 mg macitentan , which were 142 36 ng/mL . 14,20 A similar observation was made for the metabolite , ACT-132577 .\n",
      "ROOT [ 5 ]:  shown , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 42 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 65 ]:  14,20 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 70 ]:  made , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [6, 1, 6, 6, 6, 0, 36, 36, 15, 12, 12, 15, 15, 15, 8, 15, 15, 17, 15, 36, 24, 24, 24, 36, 24, 27, 24, 27, 27, 27, 24, 31, 32, 35, 33, 6, 39, 39, 36, 6, 42, 43, 0, 43, 44, 45, 46, 47, 50, 48, 47, 53, 51, 53, 54, 58, 58, 55, 58, 61, 58, 64, 64, 61, 43, 0, 69, 69, 71, 71, 0, 71, 74, 72, 74, 74, 71]\n",
      "*** Full sentence: As a reference , PK data of healthy subjects ( n 6 ) were taken from a previously published phase 1 study conducted in healthy male subjects investigating multiple-dose administration of macitentan 10 mg once daily for 10 days . 14 Statistical Analysis System ( SAS ) software , version 9.2 ( SAS Institute , Cary , North Carolina ) was used for the statistical analysis .\n",
      "ROOT [ 14 ]:  taken , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 61 ]:  used , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [15, 3, 1, 15, 6, 15, 6, 9, 7, 9, 9, 11, 9, 15, 0, 15, 20, 19, 20, 22, 22, 16, 22, 23, 27, 27, 24, 27, 30, 28, 30, 31, 34, 28, 36, 28, 28, 39, 37, 15, 48, 43, 48, 48, 48, 48, 48, 62, 48, 48, 50, 50, 54, 48, 54, 54, 56, 59, 56, 54, 62, 0, 62, 66, 66, 63, 62]\n",
      "*** Full sentence: Patients with SLE have benefited from immuno- suppressive treatments,4 while improved manage- ment of the specific complications associated with SSc and MCTD ( eg , scleroderma renal crisis ) , has improved prognosis.5 However , PAH remains a major cause of long-term morbidity and mortality .\n",
      "ROOT [ 4 ]:  benefited , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 36 ]:  remains , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [5, 1, 2, 5, 0, 5, 8, 6, 5, 11, 13, 13, 5, 13, 17, 17, 14, 17, 18, 19, 20, 20, 26, 26, 26, 22, 28, 20, 13, 5, 32, 5, 32, 37, 37, 37, 0, 40, 40, 37, 40, 43, 41, 43, 43, 37]\n",
      "*** Full sentence: In the absence of concomitant PAH , survival of patients with SSc exceeds 90 % at 1 year16 17 but once PAH has been diagnosed , it decreases to 50 % , 18 19 which is worse than for patients with iPAH ( 84 % ) .19 The risk of death from PAH related to SSc is threefold higher than from iPAH . 19 Bosentan is an oral dual ( ETA and ETB ) endothelin-1 receptor antagonist .\n",
      "ROOT [ 12 ]:  exceeds , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 46 ]:  .19 , pos tag:  . , dep:  ROOT\n",
      "ROOT [ 56 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 65 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [13, 3, 1, 3, 6, 4, 6, 6, 8, 9, 10, 11, 0, 15, 13, 13, 19, 17, 16, 13, 25, 25, 25, 25, 28, 25, 28, 13, 28, 31, 29, 31, 34, 31, 36, 31, 36, 37, 38, 39, 40, 41, 42, 45, 42, 42, 0, 49, 57, 49, 50, 49, 52, 49, 54, 55, 0, 59, 57, 59, 60, 61, 57, 65, 66, 0, 69, 69, 66, 69, 69, 71, 71, 71, 76, 71, 69, 66]\n",
      "*** Full sentence: PAH was confirmed in all patients by right heart catheterisation requiring mean pulmonary artery pressure > 25 mmHg at rest , pulmon- ary vascular resistance .3 Wood units , and pulmonary capillary wedge pressure ,15 mmHg.26 This catheterisation was per- formed within 6 months prior to the start of bosentan therapy .\n",
      "ROOT [ 2 ]:  confirmed , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 17 ]:  mmHg , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 24 ]:  resistance , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 27 ]:  units , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 40 ]:  formed , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 6, 4, 3, 10, 10, 7, 10, 15, 15, 15, 11, 11, 18, 0, 18, 19, 18, 25, 25, 25, 0, 25, 28, 0, 28, 28, 34, 34, 34, 28, 41, 41, 38, 41, 41, 41, 0, 41, 44, 42, 44, 45, 48, 46, 48, 51, 49, 41]\n",
      "*** Full sentence: Safety The most frequent adverse events ( % patients ) observed during the study were peripheral oedema ( 17 % ) , liver enzyme eleva - tions ( 17 % , of which 11 % were specified as aminotransferase increases ) , diarrhoea ( 13 % ) , exacerbated dyspnoea ( 13 % ) , and nausea ( 13 % ) .\n",
      "ROOT [ 0 ]:  Safety , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 48 ]:  exacerbated , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 6, 4, 6, 6, 15, 6, 9, 6, 9, 6, 11, 14, 12, 49, 17, 15, 17, 20, 17, 17, 49, 27, 27, 27, 27, 49, 27, 30, 27, 27, 37, 32, 35, 37, 37, 27, 37, 40, 38, 27, 27, 27, 46, 46, 43, 46, 27, 0, 49, 50, 53, 50, 50, 49, 49, 49, 57, 60, 57, 57, 49]\n",
      "*** Full sentence: In a different cohort of SSc patients without PAH followed for a mean of 1.8 years , the HAQ disability index increased by 0.4 units and VAS scores by 0.5–0.6 despite state of the art therapy.47 Hence , our findings suggest that bosentan PAH therapy was specifically associated with a stabilisation of the lung score despite other coexisting CTD respiratory manifesta- tions , but , as expected , had less impact on other organ-related quality of life indicators .\n",
      "ROOT [ 21 ]:  increased , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 40 ]:  suggest , pos tag:  VBP , dep:  ROOT\n",
      "dep_head:  [10, 4, 4, 1, 4, 7, 5, 4, 8, 22, 10, 13, 11, 13, 16, 14, 22, 21, 20, 21, 22, 0, 22, 25, 23, 25, 28, 25, 22, 29, 22, 31, 32, 35, 33, 37, 41, 41, 40, 41, 0, 48, 45, 45, 48, 48, 48, 41, 48, 51, 49, 51, 55, 55, 52, 48, 62, 62, 60, 62, 62, 56, 41, 41, 41, 67, 69, 69, 41, 71, 69, 71, 75, 75, 72, 75, 78, 76, 41]\n",
      "*** Full sentence: Additional entry criteria included : 1 ) aged o8 yrs ; 2 ) mean pulmonary artery pressure o25 mmHg ; 3 ) mean pulmonary capillary wedge pressure f15 mmHg ; 4 ) pulmonary vascular resistance .3 units ( measured or calcu- lated by right heart catheterisation ) ; and 5 ) 6-min walk distance 50–450 m. Patients had no previous exposure to prostagland- ins or their analogues .\n",
      "ROOT [ 3 ]:  included , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 22 ]:  mean , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 34 ]:  resistance , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 35 ]:  .3 , pos tag:  . , dep:  ROOT\n",
      "ROOT [ 36 ]:  units , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 57 ]:  had , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [3, 3, 4, 0, 4, 10, 6, 10, 10, 4, 10, 14, 12, 23, 17, 17, 18, 14, 14, 14, 23, 21, 0, 27, 27, 27, 28, 23, 35, 29, 35, 31, 35, 35, 0, 0, 0, 37, 37, 39, 39, 39, 42, 46, 46, 43, 37, 37, 37, 52, 50, 37, 54, 57, 54, 57, 58, 0, 61, 61, 58, 63, 61, 63, 64, 67, 64, 58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: Patients receiving placebo in previous controlled studies and de novo patients started treprostinil at a dose of 1.25 ng ? kg-1 ? min -1 with dose increases based on PAH signs and symptoms , and side effects .\n",
      "ROOT [ 11 ]:  started , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 20 ]:  kg-1 , pos tag:  ADD , dep:  ROOT\n",
      "ROOT [ 22 ]:  min , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 24 ]:  with , pos tag:  IN , dep:  ROOT\n",
      "dep_head:  [12, 1, 2, 2, 7, 7, 4, 2, 11, 11, 12, 0, 12, 12, 16, 14, 16, 19, 17, 12, 0, 21, 0, 23, 0, 27, 25, 27, 28, 31, 29, 31, 31, 33, 33, 37, 33, 25]\n",
      "*** Full sentence: Out of the 412 IPAH patients , baseline haemodynamics were available for 332 patients as follows : mean pulmonary artery pressure 59¡ 13 mmHg ; mean right atrial pressure 10¡ 5 mmHg ; and cardiac index 2.2¡0.7 L ? min-1 ? m-2 .\n",
      "ROOT [ 9 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 17 ]:  mean , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 21 ]:  59¡ , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 25 ]:  mean , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 29 ]:  10¡ , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 31 ]:  mmHg , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 42 ]:  . , pos tag:  . , dep:  ROOT\n",
      "dep_head:  [10, 1, 6, 6, 6, 2, 10, 9, 10, 0, 10, 10, 14, 12, 16, 10, 16, 0, 21, 21, 18, 0, 24, 22, 26, 0, 29, 29, 26, 0, 32, 0, 32, 32, 36, 32, 38, 36, 36, 41, 42, 43, 0]\n",
      "*** Full sentence: The patient had been receiving treprostinil for 1,123 days and was receiving 45 ng ? kg-1 ? min-1 at the time of death .\n",
      "ROOT [ 4 ]:  receiving , pos tag:  VBG , dep:  ROOT\n",
      "ROOT [ 15 ]:  kg-1 , pos tag:  ADD , dep:  ROOT\n",
      "ROOT [ 17 ]:  min-1 , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 5, 5, 5, 0, 5, 5, 9, 7, 5, 12, 5, 14, 12, 5, 0, 16, 0, 18, 21, 19, 21, 22, 18]\n",
      "*** Full sentence: The treatment-related events rated as severe in intensity for o1 % of patients were : headache ( 1.7 % ; n515 ) ; pain ( 1.4 % ; n512 ) ; diarrhoea ( 1.3 % ; n511 ) ; and nausea ( 1.2 % ; n510 ) .\n",
      "ROOT [ 3 ]:  rated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 23 ]:  pain , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [3, 3, 4, 0, 4, 5, 4, 7, 8, 11, 9, 11, 12, 4, 14, 4, 16, 19, 16, 16, 24, 21, 21, 0, 24, 27, 24, 24, 24, 24, 24, 24, 32, 35, 32, 32, 24, 37, 24, 24, 24, 41, 44, 41, 41, 41, 46, 24]\n",
      "*** Full sentence: diarrhoea ( 42 % ; n5365 ) , nausea ( 27 % ; n5235 ) , headache ( 25 % ; n5214 ) , jaw pain ( 23 % ; n5195 ) , pain ( 16 % ; n5139 ) , vasodilatation ( 13 % ; n5115 ) , anorexia ( 10 % ; n589 ) and rash ( 10 % ; n588 ) .\n",
      "ROOT [ 0 ]:  diarrhoea , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 8 ]:  nausea , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 16 ]:  headache , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [0, 1, 4, 1, 1, 1, 6, 1, 0, 9, 12, 9, 9, 9, 14, 14, 0, 20, 20, 17, 20, 20, 22, 17, 26, 17, 26, 29, 26, 26, 26, 31, 26, 26, 34, 37, 34, 34, 34, 39, 39, 39, 42, 45, 42, 42, 42, 47, 26, 26, 53, 53, 50, 26, 26, 55, 55, 55, 58, 61, 58, 58, 26, 26, 17]\n",
      "*** Full sentence: Inhibition of phos- phodiesterase type 5 increases cyclic guanosine monophosphate levels , which may mediate the antiproliferative ( 11 ) and vasodilat- ing ( 12 ) effects of endogenous nitric oxide .\n",
      "ROOT [ 6 ]:  increases , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 22 ]:  ing , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [7, 1, 4, 5, 7, 5, 0, 11, 10, 11, 7, 11, 15, 15, 11, 19, 19, 19, 15, 19, 15, 15, 0, 27, 27, 27, 23, 27, 31, 31, 28, 23]\n",
      "*** Full sentence: The placebo-adjusted change in 6MWD in patients in WHO functional class II was 32 m ( 95 % CI , 6–59 m ; P 0.017 ) compared with 83 m ( 95 % CI , 64– 102 m ; P , 0.001 ) in patients in WHO functional class III .\n",
      "ROOT [ 12 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 18 ]:  CI , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 26 ]:  compared , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 3, 13, 3, 4, 3, 6, 7, 12, 12, 12, 8, 0, 15, 13, 15, 18, 19, 0, 19, 22, 19, 19, 19, 24, 24, 0, 27, 30, 28, 30, 33, 34, 30, 30, 30, 38, 30, 30, 30, 40, 40, 40, 30, 44, 45, 50, 50, 50, 46, 27]\n",
      "*** Full sentence: During the course of the randomized phase of the study ( # 12 wk ) , five patients had clinical worsening events , four in the placebo group ( 20 % ) and one in the vardenafil group Sex ( 2.3 % ) ( hazard ratio 0.105 ; 95 % CI , 0.012–0.938 ; P 0.044 ) ( Table 3 , Figure 5 ) .\n",
      "ROOT [ 18 ]:  had , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 58 ]:  Table , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [19, 3, 1, 3, 7, 7, 4, 7, 10, 8, 10, 14, 14, 10, 10, 19, 18, 19, 0, 22, 22, 19, 19, 22, 24, 28, 28, 25, 24, 31, 24, 24, 24, 24, 34, 38, 38, 35, 38, 34, 42, 34, 34, 34, 46, 34, 46, 34, 50, 51, 34, 51, 51, 34, 34, 55, 19, 59, 0, 59, 59, 59, 62, 59, 59]\n",
      "*** Full sentence: This is also the first study in which monotherapy with a phosphodiesterase type 5 inhibitor in treatment-naive patients with PAH was able to significantly re- duce the occurrence of clinical worsening events , although the numbers of such events was small .\n",
      "ROOT [ 1 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 25 ]:  duce , pos tag:  VB , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 6, 6, 2, 21, 9, 21, 9, 15, 13, 15, 13, 10, 15, 18, 16, 18, 19, 6, 21, 25, 25, 22, 0, 28, 26, 28, 32, 32, 29, 26, 40, 36, 40, 36, 39, 37, 26, 40, 26]\n",
      "*** Full sentence: In addition , improvements were seen in 6MWD ( 171 ± 93 vs 230 ± 114 ) , 36-Item Short Form Health Survey Mental Component Summary aggregate ( 38 ± 11 vs 44.2 ± 10.7 ) , University of California , San Diego Shortness of Breath Questionnaire ( 87 ± 17.1 vs 73.1 ± 21 ) , and brain natriuretic peptide ( 558 ± 859 vs 228 ± 340 ) .\n",
      "ROOT [ 5 ]:  seen , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 26 ]:  aggregate , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [6, 1, 6, 6, 6, 0, 6, 7, 6, 11, 4, 11, 11, 15, 13, 15, 11, 6, 6, 21, 23, 23, 27, 26, 26, 27, 0, 27, 30, 27, 30, 30, 34, 32, 34, 30, 27, 27, 38, 39, 38, 43, 44, 38, 44, 47, 45, 38, 50, 38, 50, 50, 54, 52, 54, 38, 27, 27, 61, 61, 27, 61, 64, 61, 64, 64, 68, 66, 68, 61, 27]\n",
      "*** Full sentence: Pulmonary hypertension ( PH ) may complicate pul- monary ﬁbrosis ( PF ) of different causes , but few studies have focused on the treatment of advanced PH in this context.1–4 Furthermore , no prospective chronic parenteral prostanoid administration studies are available in patients with PF homogenised for the less common advanced PH phenotype , characterised by signiﬁcantly altered right heart haemodynamics and right ventricular ( RV ) dysfunction .\n",
      "ROOT [ 6 ]:  complicate , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 40 ]:  are , pos tag:  VBP , dep:  ROOT\n",
      "dep_head:  [2, 7, 2, 2, 2, 7, 0, 10, 10, 7, 10, 10, 12, 10, 16, 14, 7, 7, 20, 22, 22, 7, 22, 25, 23, 25, 28, 26, 25, 31, 29, 41, 41, 40, 40, 40, 39, 39, 40, 41, 0, 41, 42, 43, 44, 45, 44, 47, 54, 51, 54, 54, 54, 48, 41, 41, 56, 59, 62, 62, 62, 57, 62, 69, 69, 69, 69, 69, 62, 41]\n",
      "*** Full sentence: We previously reported a case where the rationale for parenteral tre- prostinil as a bridge to lung transplantation ( LT ) was outlined in the index patient with PF-PH for this study.5 Importantly , parenteral prostanoid therapy associated with worsening of ventilation-perfusion ( V-Q ) mismatch and subsequent hypoxaemia remains a major clinical concern .\n",
      "ROOT [ 2 ]:  reported , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 37 ]:  associated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 49 ]:  remains , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 5, 3, 23, 8, 23, 8, 12, 12, 9, 12, 15, 13, 15, 18, 16, 15, 15, 20, 23, 5, 23, 27, 27, 24, 23, 28, 23, 32, 30, 38, 38, 37, 37, 38, 0, 38, 39, 40, 41, 46, 46, 46, 50, 46, 49, 46, 0, 54, 54, 54, 50, 50]\n",
      "*** Full sentence: Of the 15 patients , 14 patients received subcutaneous treprosti- nil and 1 patient was placed on intravenous treprostinil . 5 The treprostinil dose for the group at 12 weeks was 34 ± 21 ng/kg/ min ( mean ± SD ) and a range of 18–97 ng/kg/min .\n",
      "ROOT [ 7 ]:  received , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 20 ]:  5 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 23 ]:  dose , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 30 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 35 ]:  min , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [8, 4, 4, 1, 8, 7, 8, 0, 11, 11, 8, 11, 14, 16, 16, 8, 16, 19, 17, 16, 0, 23, 24, 0, 24, 27, 25, 24, 30, 28, 0, 31, 32, 36, 36, 0, 36, 36, 40, 36, 36, 36, 44, 36, 44, 47, 45, 36]\n",
      "*** Full sentence: Of the 7 patients not offered LT , 5 died ( mean ± SD ) 504 ± 295 days after treprostinil initiation , while 2 patients remained alive 1059 and 1401 days after treprostinil initiation .\n",
      "ROOT [ 9 ]:  died , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 16 ]:  ± , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [9, 4, 4, 1, 6, 4, 6, 10, 10, 0, 12, 14, 14, 10, 10, 17, 0, 19, 20, 17, 22, 20, 17, 27, 26, 27, 17, 29, 27, 29, 32, 33, 27, 35, 33, 17]\n",
      "*** Full sentence: Cardiac output may itself be associated with increased intrapulmonary shunt.26 Based on the above , we surmise that a gentler uptitration of par- enteral prostanoid , as employed in our study PF population with advanced PH , may lessen the potential for haemodynamic instability and/or hypoxaemia and rather parallel the chronic parenteral prostanoid administration haemodynamic and gas exchange data reported in WHO Group I PAH .\n",
      "ROOT [ 5 ]:  associated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 16 ]:  surmise , pos tag:  VBP , dep:  ROOT\n",
      "dep_head:  [2, 6, 6, 3, 6, 0, 6, 9, 7, 6, 17, 11, 14, 12, 17, 17, 0, 39, 21, 21, 39, 21, 25, 25, 22, 21, 28, 21, 28, 33, 32, 33, 29, 33, 36, 34, 39, 39, 17, 41, 39, 41, 44, 42, 44, 44, 39, 49, 39, 59, 59, 58, 58, 55, 58, 55, 55, 59, 49, 59, 60, 61, 61, 59, 49, 17]\n",
      "*** Full sentence: These salutary effects on right heart function may further optimise arterial oxygen content via enhanced mixed venous oxygen saturation.16 27 The improvement of 59 m in 6MWD following 12 weeks of parenteral treprostinil was noted in parallel with improvements in the UCSD SOB questionnaire and the SF-36 MCS , represent- ing preliminary but encouraging ﬁndings .\n",
      "ROOT [ 34 ]:  noted , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 49 ]:  represent- , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 50 ]:  ing , pos tag:  VBG , dep:  ROOT\n",
      "dep_head:  [3, 3, 10, 3, 7, 7, 4, 10, 10, 35, 13, 13, 10, 10, 18, 18, 18, 14, 10, 22, 22, 10, 22, 25, 23, 10, 26, 10, 30, 28, 30, 33, 31, 35, 0, 35, 36, 37, 38, 39, 44, 44, 44, 40, 44, 48, 48, 44, 39, 0, 0, 55, 52, 52, 51, 51]\n",
      "*** Full sentence: In recent years , a large body of evidence re‐ vealed that Rho kinase is involved in the development of pulmonary hypertension . 10,11 Animal experiments and clinical studies showed high Rho kinase activity in rat models of pulmonary hypertension12 and tissues from lung and pulmonary arteries of patients with severe pulmonary hypertension.13 Fasudil is currently the only available intravenous Rho kinase inhibitor in China .\n",
      "ROOT [ 10 ]:  vealed , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 29 ]:  showed , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 52 ]:  hypertension.13 , pos tag:  . , dep:  ROOT\n",
      "ROOT [ 54 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [11, 3, 1, 11, 7, 7, 11, 7, 10, 8, 0, 16, 14, 16, 16, 11, 16, 19, 17, 19, 22, 20, 11, 26, 26, 30, 26, 29, 26, 0, 34, 34, 34, 30, 34, 37, 35, 37, 40, 38, 37, 37, 42, 47, 44, 44, 43, 47, 48, 49, 52, 50, 0, 55, 0, 55, 63, 63, 63, 63, 62, 63, 55, 63, 64, 55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: In both groups , sPAP , dPAP , mPAP , and PVR were all significantly decreased and CI was increased after the intravenous infusion of Fasudil com‐ pared with baseline ( all P < .05 ) .\n",
      "ROOT [ 12 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 34 ]:  .05 , pos tag:  XX , dep:  ROOT\n",
      "dep_head:  [13, 3, 1, 3, 3, 5, 5, 7, 7, 9, 9, 9, 0, 16, 16, 13, 13, 20, 20, 13, 20, 24, 24, 21, 24, 27, 25, 24, 28, 29, 33, 33, 35, 35, 0, 35, 35]\n",
      "*** Full sentence: Pharmacokinetic and Pharmacodynamic Comparison of Sildenafil-Bosentan and Sildenafil-Ambrisentan Combination Therapies for Pulmonary Hypertension Clin Transl Sci ( 2016 ) 9 , 29–35 ; doi:10.1111/cts.12382 ; published online on 12 January 2016 .\n",
      "ROOT [ 3 ]:  Comparison , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 25 ]:  published , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [4, 1, 1, 0, 4, 9, 6, 6, 10, 5, 10, 13, 14, 11, 16, 26, 20, 20, 20, 16, 20, 20, 16, 16, 16, 0, 26, 26, 30, 28, 30, 26]\n",
      "*** Full sentence: or ambrisen- tan 10 mg q.d . )\n",
      "ROOT [ 1 ]:  ambrisen- , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 6 ]:  . , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 0, 7, 3, 7, 7, 0, 7]\n",
      "*** Full sentence: Pharmacokinetic assessments After baseline blood collection , participants were given silde- nafil ( 20 mg ) plus either bosentan ( 62.5 mg ) or ambrisen- tan ( 10 mg ) , according to the protocol for each therapy period .\n",
      "ROOT [ 1 ]:  assessments , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 9 ]:  given , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 11 ]:  nafil , pos tag:  RB , dep:  ROOT\n",
      "ROOT [ 25 ]:  tan , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 31 ]:  according , pos tag:  VBG , dep:  ROOT\n",
      "dep_head:  [2, 0, 10, 6, 6, 3, 10, 10, 10, 0, 12, 0, 12, 15, 12, 15, 15, 19, 15, 19, 22, 19, 22, 22, 22, 0, 26, 29, 26, 26, 26, 0, 32, 35, 33, 35, 39, 39, 36, 32]\n",
      "*** Full sentence: ung disease is now the most common cause of scleroderma-related deaths .1 Prior to the newer therapies for PAH , the 2-year cumulative survival was 50 % . 2 These newer drugs have improved func- tion as well as survival in scleroderma patients with PAH.3 There are several known risk factors for the development of pulmonary arterial hypertension ( PAH ) in patients with systemic sclerosis ( SSc ) .\n",
      "ROOT [ 2 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 24 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 28 ]:  2 , pos tag:  LS , dep:  ROOT\n",
      "ROOT [ 33 ]:  improved , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 46 ]:  are , pos tag:  VBP , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 3, 8, 7, 8, 3, 8, 11, 9, 3, 25, 13, 17, 17, 14, 17, 18, 19, 24, 24, 24, 19, 0, 27, 25, 25, 0, 32, 32, 34, 34, 0, 36, 34, 39, 39, 36, 36, 40, 43, 41, 40, 44, 47, 0, 51, 51, 51, 47, 51, 54, 52, 54, 58, 58, 55, 58, 58, 58, 54, 62, 63, 66, 64, 66, 66, 66, 47]\n",
      "*** Full sentence: The definition of PAH is the presence of a mean pulmonary artery pressure ( PAP ) on right-heart catheterization > 25 mm Hg at rest or 30 mm Hg with exercise .\n",
      "ROOT [ 4 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 22 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 5, 2, 3, 0, 7, 5, 7, 13, 13, 13, 13, 8, 13, 13, 13, 7, 19, 17, 19, 22, 23, 0, 23, 24, 23, 29, 29, 23, 29, 30, 23]\n",
      "*** Full sentence: With- out adequate therapy , the mean survival of patients with primary pulmonary hypertension ( PPH ) is < 3 years.1 The use of prostanoids has become an effec- tive therapy lately .\n",
      "ROOT [ 17 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 26 ]:  become , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [4, 1, 4, 18, 18, 8, 8, 18, 8, 9, 10, 14, 14, 11, 14, 14, 14, 0, 20, 18, 20, 23, 27, 23, 24, 27, 0, 31, 31, 31, 27, 27, 27]\n",
      "*** Full sentence: Bosen- tan , 125 mg bid , in combination with beraprost or inhaled iloprost improved 6-min walking distance ( 6MWD ) , and parameters of cardiopulmonary exer- cise testing ( oxygen consumption as well as anaerobic threshold ) in patients with PPH during a 12- week trial.18 The Tei index is a practical echocardiographic index that combines information on systolic and diastolic RV function .\n",
      "ROOT [ 0 ]:  Bosen- , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 5 ]:  bid , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 46 ]:  trial.18 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 50 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [0, 6, 6, 6, 6, 0, 6, 15, 8, 9, 10, 8, 8, 15, 6, 18, 18, 15, 18, 18, 18, 15, 15, 15, 24, 29, 26, 29, 25, 32, 32, 29, 35, 35, 29, 37, 29, 29, 24, 39, 40, 41, 15, 46, 46, 43, 0, 50, 50, 51, 0, 55, 55, 55, 51, 57, 55, 57, 57, 64, 60, 60, 64, 59, 51]\n",
      "*** Full sentence: This noninvasive parameter is correlated well with prognosis in patients with chronic pulmonary heart disease due to PPH.19 We hypothesized that adding bosentan may improve Tei index and 6MWD in patients with progressive pul- monary hypertension receiving prostanoid therapy .\n",
      "ROOT [ 4 ]:  correlated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 19 ]:  hypothesized , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [3, 3, 5, 5, 0, 5, 5, 7, 8, 9, 10, 15, 15, 15, 11, 5, 16, 16, 20, 0, 25, 25, 22, 25, 20, 27, 25, 27, 27, 29, 30, 31, 34, 36, 36, 32, 36, 39, 37, 20]\n",
      "*** Full sentence: It has been suggested that oral prosta- noid therapy may also be switched to inhaled pro- stanoids because the effects of oral beraprost may be attenuated with increasing treatment time.12 This observation , however , was not available at the time of initialization of our study .\n",
      "ROOT [ 3 ]:  suggested , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 35 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 0, 13, 9, 8, 9, 13, 13, 13, 13, 4, 15, 13, 15, 15, 26, 20, 26, 20, 23, 21, 26, 26, 13, 26, 29, 27, 4, 32, 36, 36, 36, 36, 0, 36, 36, 36, 41, 39, 41, 42, 43, 46, 44, 36]\n",
      "*** Full sentence: These authors reported an increase in 6MWD of 58 ± 43 m and an improvement in oxygen consump- tion as well as anaerobic threshold 3 months after the start of combination therapy in 20 patients with PPH exclusively.18 The improvement was maintained in 11 patients at 6 months .\n",
      "ROOT [ 2 ]:  reported , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 41 ]:  maintained , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 5, 3, 5, 6, 7, 12, 12, 12, 8, 5, 15, 5, 15, 16, 17, 16, 22, 22, 19, 24, 19, 26, 27, 15, 29, 27, 29, 32, 30, 29, 35, 33, 35, 38, 36, 40, 42, 42, 0, 42, 45, 43, 42, 48, 46, 42]\n",
      "*** Full sentence: The increases in 6MWD of 42.5 ± 66 m at 6 months and of 44.6 ± 66 m at maximum follow-up are comparable to that reported by Hoeper et al.18 However , the duration of follow-up in this study exceeds that of the study of Hoeper et al18 consid- erably ( mean , 13.5 ± 9 months ; range , 9 to 22 months ) .\n",
      "ROOT [ 21 ]:  are , pos tag:  VBP , dep:  ROOT\n",
      "ROOT [ 39 ]:  exceeds , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 49 ]:  erably , pos tag:  RB , dep:  ROOT\n",
      "dep_head:  [2, 22, 2, 3, 4, 9, 9, 9, 5, 2, 12, 10, 2, 2, 16, 18, 18, 14, 18, 21, 19, 0, 22, 23, 24, 25, 26, 29, 27, 22, 40, 40, 34, 40, 34, 35, 36, 39, 37, 0, 40, 41, 44, 42, 44, 48, 48, 45, 40, 0, 59, 59, 59, 55, 59, 57, 55, 59, 50, 59, 63, 63, 64, 59, 59, 59]\n",
      "*** Full sentence: Conclusion The present study reports our experience of addi- tional bosentan therapy in patients with progressive pulmonary hypertension despite maximally tolerated prostanoid therapy .\n",
      "ROOT [ 0 ]:  Conclusion , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 4 ]:  reports , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [0, 4, 4, 5, 0, 7, 5, 7, 12, 12, 12, 8, 7, 13, 14, 18, 18, 15, 7, 21, 23, 23, 19, 5]\n",
      "*** Full sentence: At week 6 , the mean dose of treprostinil was 52 6 28 ng/kg/min ( range , 24 – 133 ng/kg/min ) ; at week 12 , it was 62 6 30 ng /kg/min ( range , 23–139 ng/kg/min ) .\n",
      "ROOT [ 28 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 32 ]:  ng , pos tag:  RB , dep:  ROOT\n",
      "ROOT [ 35 ]:  range , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [10, 1, 2, 10, 7, 7, 10, 7, 8, 29, 13, 13, 14, 10, 16, 10, 16, 16, 16, 21, 16, 16, 29, 29, 24, 25, 29, 29, 0, 29, 30, 30, 0, 33, 36, 0, 36, 39, 36, 36, 36]\n",
      "*** Full sentence: Exercise Capacity There was no statistically significant difference between the mean 6MW distance at baseline on IV epoprostenol ( 561 6 89 m ) and that performed at either week 6 ( 543 6 91 m ; P = 0.06 ) or week 12 on IV treprostinil ( 576 6 96 m ; P = 0.13 ) .\n",
      "ROOT [ 1 ]:  Capacity , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 3 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 4, 0, 8, 7, 8, 4, 8, 13, 13, 13, 9, 13, 14, 13, 18, 16, 8, 23, 23, 23, 8, 23, 4, 27, 4, 27, 30, 28, 30, 30, 30, 36, 36, 30, 27, 27, 40, 38, 38, 27, 27, 43, 43, 47, 45, 47, 52, 52, 52, 47, 47, 56, 56, 47, 47, 27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: The mean pulmonary artery pres- sure was 68 ± 18 mm Hg , the mean pulmonary vascular resistance was 16.6 ± 11 Wood units , the mean pulmonary arterial oxygen saturation on room air was 61 ± 9 % , and the mean cardiac index was 2.8 ± 1.04 L/min/m2 .\n",
      "ROOT [ 3 ]:  artery , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 6 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 34 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 0, 4, 7, 0, 12, 12, 11, 12, 7, 19, 18, 18, 18, 18, 19, 35, 24, 24, 24, 24, 19, 35, 31, 31, 31, 30, 31, 35, 31, 34, 32, 0, 39, 39, 39, 35, 35, 35, 45, 45, 45, 46, 35, 50, 47, 50, 46, 46]\n",
      "*** Full sentence: The mean dose of treprostinil used in that study was 9 ng/kg/min , and the mean increase in 6-minute walking distance was 16 m . It was also noted that there was a relation between the improvement in 6-minute walking distance and the dose of treprostinil .\n",
      "ROOT [ 9 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 28 ]:  noted , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 3, 10, 3, 4, 5, 6, 9, 7, 0, 12, 10, 10, 10, 17, 17, 22, 17, 20, 21, 18, 10, 24, 22, 22, 29, 29, 29, 0, 32, 32, 29, 34, 32, 34, 37, 35, 37, 40, 41, 38, 41, 44, 41, 44, 45, 29]\n",
      "*** Full sentence: Differential Effects of Terlipressin on Pulmonary and Systemic Hemodynamics in Patients With Cirrhosis and Pulmonary Hypertension : An Echo Study Georgios N. Kalambokis , MD1 , Konstantinos Pappas , MD2 , and Epameinondas V. Tsianos , MD , PhD , AGAF , FEBG1 Introduction The development of pulmonary hypertension ( PH ) in the set- ting of portal hypertension is known as portopulmonary hyper- tension ( POPH ) .\n",
      "ROOT [ 1 ]:  Effects , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 60 ]:  known , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 3, 2, 5, 6, 9, 6, 6, 10, 11, 16, 13, 13, 12, 2, 21, 20, 21, 23, 23, 2, 23, 23, 25, 28, 25, 28, 28, 30, 30, 35, 35, 30, 35, 35, 37, 37, 39, 39, 41, 44, 41, 46, 61, 46, 49, 47, 49, 49, 49, 46, 56, 56, 53, 56, 59, 57, 61, 0, 61, 65, 65, 62, 65, 65, 65, 61]\n",
      "*** Full sentence: Portal hypertension is thought to predispose patients to disturbances in the homeostatic regulation of numer- ous neurohumoral and vasoactive mediators , including increased production of endothelin 1 ( ET-1 ) and interleukin 6 and decreased synthesis of prostacyclin and nitric oxide , which result in pulmonary arterial vasoconstriction.1-4 Portopulmon- ary hypertension is pathologically indistinguishable from idio- pathic PH and is mainly characterized by the development of vasoconstriction within the pulmonary arterial vasculature .\n",
      "ROOT [ 3 ]:  thought , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 51 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 6, 4, 6, 9, 6, 9, 13, 13, 10, 13, 20, 20, 20, 17, 17, 14, 20, 20, 24, 22, 24, 25, 26, 26, 30, 26, 26, 26, 32, 32, 32, 35, 36, 41, 38, 38, 37, 41, 44, 41, 44, 47, 48, 45, 48, 51, 52, 0, 54, 52, 54, 58, 58, 55, 52, 62, 62, 52, 62, 65, 63, 65, 66, 67, 72, 72, 72, 68, 52]\n",
      "*** Full sentence: Due to its vasocon- strictor effects , terlipressin is considered the most efficient therapy for variceal bleeding and hepatorenal syndrome ( HRS ) .12,13 Yet , previous data suggested a pulmonary arterial vasodilating effect of terlipressin in patients with cirrhosis hav- ing PH.14,15 The aim of the present echocardiographic study was to eval- uate the effects of terlipressin on PVR in patients with and without PH concomitantly with its effects on systemic hemody- namics .\n",
      "ROOT [ 9 ]:  considered , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 28 ]:  suggested , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 50 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [10, 1, 6, 6, 6, 1, 10, 10, 10, 0, 14, 13, 14, 10, 14, 17, 15, 17, 20, 17, 20, 20, 20, 10, 29, 29, 28, 29, 0, 34, 34, 34, 34, 29, 34, 35, 34, 37, 38, 41, 42, 43, 39, 45, 51, 45, 50, 50, 50, 46, 0, 54, 54, 51, 56, 54, 56, 57, 56, 59, 56, 61, 62, 63, 63, 65, 54, 54, 70, 68, 70, 73, 74, 71, 51]\n",
      "*** Full sentence: Pulmonary and systemic hemodynamics were evaluated at baseline and 30 minutes after the administration of 2 mg of terlipressin in patients with PVR < 120 dyne s cm-5 ( group 1 , n 20 ) or > 120 dyne s cm-5 ( group 2 , n 10 ) .\n",
      "ROOT [ 5 ]:  evaluated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 29 ]:  group , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 39 ]:  s , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [6, 1, 4, 1, 6, 0, 6, 7, 8, 11, 8, 6, 14, 12, 14, 17, 15, 17, 18, 17, 20, 21, 22, 27, 27, 27, 23, 27, 30, 0, 30, 30, 30, 30, 30, 30, 38, 39, 30, 0, 40, 40, 40, 43, 43, 43, 43, 40, 40]\n",
      "*** Full sentence: Eight patients ( 33 % ) were being treated with a prostacyclin analog , treprostinil ( mean dose , 28 . 4 ng /kg/min ) for a mean duration of 864 ± 317 days .\n",
      "ROOT [ 8 ]:  treated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 22 ]:  ng , pos tag:  RB , dep:  ROOT\n",
      "ROOT [ 25 ]:  for , pos tag:  IN , dep:  ROOT\n",
      "dep_head:  [2, 9, 2, 5, 2, 2, 9, 9, 0, 9, 13, 13, 10, 13, 13, 18, 18, 15, 18, 18, 18, 23, 0, 23, 23, 0, 29, 29, 26, 29, 32, 30, 34, 30, 26]\n",
      "*** Full sentence: Long-term prostacyclin therapy has been found to improve hemodynamics and quality of life in patients ranging in age from 2 to 51 years with congenital heart defects.9 These investigators noted that prostacyclin therapy did not result in a significant increase in exercise capacity as measured by the 6-min walk test , a finding that is similar to that of the present study .\n",
      "ROOT [ 5 ]:  found , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 29 ]:  noted , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [3, 3, 6, 6, 6, 0, 8, 6, 8, 9, 9, 11, 12, 8, 14, 15, 16, 17, 16, 19, 19, 23, 21, 19, 27, 27, 24, 29, 30, 0, 36, 33, 36, 36, 36, 30, 36, 40, 40, 37, 40, 43, 41, 45, 40, 45, 50, 50, 50, 46, 50, 53, 50, 55, 53, 55, 56, 57, 58, 62, 62, 59, 30]\n",
      "*** Full sentence: Bosentan therapy was demonstrated to be effec- tive in improving functional capacity and cardiopul- monary hemodynamics with a significant reduction in PVR and mPAP in patients with primary pulmo - nary hypertension and other forms of PAH.17,28 The large published trial28 with bosentan in PAH patients did not , however , include patients with CHD .\n",
      "ROOT [ 3 ]:  demonstrated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 51 ]:  include , pos tag:  VBP , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 6, 4, 8, 6, 8, 9, 12, 10, 12, 16, 16, 12, 10, 20, 20, 17, 20, 21, 22, 22, 20, 25, 26, 32, 31, 31, 32, 27, 32, 35, 32, 35, 36, 41, 40, 41, 52, 41, 42, 43, 46, 44, 52, 52, 52, 52, 52, 0, 52, 53, 54, 52]\n",
      "*** Full sentence: This 12-month study provides follow-up data for the longest time period on bosentan therapy in adults with CHD to date and demonstrates that therapy with the oral ET receptor antagonist bosen- tan may significantly improve the overall cardiopul- monary hemodynamics and NYHA functional class in patients with this form of PAH .\n",
      "ROOT [ 3 ]:  provides , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 30 ]:  bosen- , pos tag:  XX , dep:  ROOT\n",
      "ROOT [ 34 ]:  improve , pos tag:  VB , dep:  ROOT\n",
      "dep_head:  [3, 3, 4, 0, 6, 4, 4, 11, 11, 11, 7, 11, 14, 12, 14, 15, 16, 17, 4, 19, 4, 4, 24, 22, 24, 29, 29, 29, 30, 25, 0, 35, 35, 35, 0, 40, 40, 40, 40, 35, 40, 44, 44, 40, 40, 45, 46, 49, 47, 49, 50, 35]\n",
      "*** Full sentence: BRIEF REPORT Long-term trial of bosentan monotherapy for pulmonary arterial hypertension in Japanese patients Shigetake Sasayama a , toru Satoh b , tohru izumi c , Shunji Yoshida d , Shingo Kyotani e and nobuhiro tahara f Introduction Pulmonary arterial hypertension ( PAH ) is a devastating disease , characterised by a progressive increase in pulmonary vascular resistance that ultimately leads to right heart failure1 .\n",
      "ROOT [ 14 ]:  Shigetake , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 44 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 15, 4, 7, 5, 4, 11, 11, 8, 11, 14, 12, 0, 15, 21, 21, 21, 21, 15, 21, 25, 25, 21, 25, 29, 29, 25, 29, 36, 36, 36, 33, 33, 29, 41, 41, 41, 41, 45, 41, 41, 41, 0, 48, 48, 45, 45, 45, 50, 54, 54, 51, 54, 58, 58, 55, 61, 61, 58, 61, 65, 65, 62, 45]\n",
      "*** Full sentence: Exposure to bosentan treatment The median duration of bosentan treatment was 2.7 years ( range : 0.4–3.0 years ) .\n",
      "ROOT [ 0 ]:  Exposure , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 10 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 1, 4, 2, 7, 7, 11, 7, 10, 8, 0, 13, 11, 15, 11, 15, 18, 15, 18, 15]\n",
      "*** Full sentence: At 2.5–3.0 years after the start of the long‑term trial 2/12 patients ( 16.7 % ) were in class I , 7/12 patients ( 58.3 % ) were in class II , and 2/12 patients ( 16.7 % ) were in class III .\n",
      "ROOT [ 16 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 27 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [17, 3, 1, 17, 6, 17, 6, 12, 12, 12, 12, 7, 12, 15, 12, 12, 0, 17, 18, 28, 20, 23, 28, 23, 26, 23, 23, 0, 28, 31, 29, 28, 28, 35, 40, 35, 38, 35, 35, 28, 40, 43, 41, 40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: In luciferase reporter gene assays in HUVECs , levosimendan dose-depen- dently attenuated the TNF-a- stimulated increase of proinflamma - tory transcription factors activator protein 1 ( AP1 ) , hypoxia-inducible factor-1a ( HIF-1a ) , and nuclear fac- tor-jB ( NF-jB ) .\n",
      "ROOT [ 11 ]:  attenuated , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 38 ]:  tor-jB , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [12, 3, 5, 5, 1, 5, 6, 12, 12, 12, 12, 0, 16, 15, 16, 12, 16, 20, 20, 21, 22, 24, 24, 17, 24, 24, 24, 24, 12, 31, 9, 31, 31, 31, 31, 31, 38, 31, 0, 41, 39, 41, 39]\n",
      "*** Full sentence: Afterwards , levosimendan ( 100 nmol/l , 1 lmol/l , 10 lmol/ l , 100 lmol/l final concentrations ) or solvent [ dimethyl sulfoxide ( DMSO ) ] was added , and proliferation was subsequently stimulated by platelet-derived growth factor- BB ( 30 ng/ml ) and further proceeded as described previ- ously [ ] .\n",
      "ROOT [ 29 ]:  added , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 40 ]:  BB , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [30, 30, 21, 6, 6, 3, 6, 9, 3, 9, 13, 13, 9, 13, 18, 18, 18, 13, 18, 18, 30, 30, 24, 30, 24, 24, 24, 24, 30, 0, 30, 30, 36, 36, 36, 30, 36, 39, 37, 30, 0, 44, 44, 41, 44, 41, 48, 41, 50, 48, 48, 48, 48, 41, 41]\n",
      "*** Full sentence: Fetal calf serum ( FCS ) was reduced to 0.5 % for a period of 4 h. Then , levosimendan ( 10 lmol/l , 1 lmol/l , 100 nmol/l ) , glibenclamide ( 10 lmol/l ; Sigma , Deisenhofen , Germany ) or solvent ( DMSO ) was added , and cells were stimulated with tumor necrosis factor-a ( 10 ng/ml ) .\n",
      "ROOT [ 7 ]:  reduced , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 48 ]:  added , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 3, 8, 3, 3, 3, 8, 0, 8, 11, 9, 8, 14, 12, 14, 17, 18, 49, 49, 49, 23, 23, 20, 23, 26, 20, 26, 29, 26, 26, 49, 49, 32, 35, 32, 32, 49, 37, 37, 39, 39, 41, 37, 37, 37, 37, 37, 49, 0, 49, 49, 54, 54, 49, 54, 57, 58, 55, 58, 61, 58, 58, 54]\n",
      "*** Full sentence: 0 25 * * 15 Levosimendan inhibits PASMC proliferation in vivo To elucidate the mechanism underlying the levosimen- dan- associated reduction of vascular remodeling , an in vivo BrdU assay for quantification of cell proliferation was performed 7 days after MCT challenge ( Fig .\n",
      "ROOT [ 0 ]:  0 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 1 ]:  25 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 36 ]:  performed , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 0, 4, 7, 4, 7, 30, 9, 7, 9, 10, 13, 11, 15, 13, 15, 21, 19, 21, 21, 16, 21, 24, 22, 21, 7, 30, 27, 30, 37, 30, 31, 32, 35, 33, 37, 0, 39, 37, 37, 42, 40, 42, 42, 37]\n",
      "*** Full sentence: Combination therapy with bosentan and sildenafil in idiopathic pulmonary arterial hypertension ABSTRACT : It has been proposed that targeted treatments should be combined for patients with idiopathic pulmonary arterial hypertension ( IPAH ) responding insuffi- ciently to monotherapy .\n",
      "ROOT [ 1 ]:  therapy , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 16 ]:  proposed , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 3, 2, 2, 6, 11, 11, 11, 12, 7, 2, 17, 17, 17, 0, 23, 20, 23, 23, 23, 17, 23, 24, 25, 30, 30, 30, 26, 32, 30, 34, 17, 36, 34, 34, 37, 17]\n",
      "*** Full sentence: However , this effect was not sustained and , after an interval of 11¡5 months , the walk distance had declined to 277¡80 m. At this point , sildenafil was added to bosentan .\n",
      "ROOT [ 6 ]:  sustained , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 30 ]:  added , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [7, 7, 4, 7, 7, 7, 0, 7, 7, 21, 12, 10, 12, 15, 13, 21, 19, 19, 21, 21, 7, 21, 24, 22, 31, 27, 25, 31, 31, 31, 0, 31, 32, 31]\n",
      "*** Full sentence: Treatment efficacy was monitored primarily by the 6-min walk distance ( 6MWD ) and cardiopulmonary exercise testing ( CPET ) , and was targeted to reach pre-defined goals based on prognostically relevant variables : a 6MWD w380 m [ 6 ] and a peak oxygen uptake measured during CPET Results w10.4 mL ? min-1 ? kg-1 [ 17 ] .\n",
      "ROOT [ 3 ]:  monitored , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 51 ]:  mL , pos tag:  FW , dep:  ROOT\n",
      "ROOT [ 57 ]:  17 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 4, 4, 10, 10, 10, 6, 12, 10, 12, 10, 17, 17, 10, 17, 17, 17, 4, 4, 24, 4, 26, 24, 28, 26, 28, 29, 32, 33, 30, 28, 40, 38, 38, 40, 40, 28, 40, 40, 46, 45, 46, 40, 46, 47, 50, 48, 40, 0, 52, 55, 58, 55, 58, 0, 58, 58]\n",
      "*** Full sentence: How- ever , this effect was not sustained and , after an interval of 11¡5 months , the walk distance had declined to 277¡80 m. At this point , sildenafil was added to bosentan .\n",
      "ROOT [ 7 ]:  sustained , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 31 ]:  added , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [8, 8, 8, 5, 8, 8, 8, 0, 8, 8, 22, 13, 11, 13, 16, 14, 22, 20, 20, 22, 22, 8, 22, 25, 23, 32, 28, 26, 32, 32, 32, 0, 32, 33, 32]\n",
      "*** Full sentence: Before targeted treatment was initiated , all patients were in functional class NYHA III or IV , and haemodynamic assessment confirmed the presence of advanced pulmonary hypertension with a mean pulmonary artery pressure of 62¡12 mmHg , a cardiac index of 1.6¡0.3 L ? min-1 ? m-2 and a pulmonary vascular resistance of 1,549¡440 dynes ? s ? cm-5 .\n",
      "ROOT [ 8 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 51 ]:  resistance , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 56 ]:  s , pos tag:  LS , dep:  ROOT\n",
      "dep_head:  [5, 3, 5, 5, 9, 9, 8, 9, 0, 9, 12, 14, 14, 10, 14, 14, 9, 9, 20, 21, 9, 23, 21, 23, 27, 27, 24, 23, 33, 33, 33, 33, 28, 33, 36, 34, 36, 40, 40, 36, 40, 43, 41, 21, 46, 47, 48, 52, 52, 52, 52, 0, 52, 55, 53, 52, 0, 57, 57, 57]\n",
      "*** Full sentence: To assess the relationship between 6MWT and MET , generalized estimating equation ( GEE ) models with identity link were used for all patients , and for FC I/II patients and FC III/IV patients separately .\n",
      "ROOT [ 9 ]:  generalized , pos tag:  JJ , dep:  ROOT\n",
      "ROOT [ 20 ]:  used , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 10, 4, 2, 4, 5, 6, 6, 10, 0, 12, 10, 16, 16, 16, 21, 16, 19, 17, 21, 0, 21, 24, 22, 21, 21, 21, 30, 30, 27, 30, 34, 34, 30, 27, 21]\n",
      "*** Full sentence: Between- subject coefficient of variation was calculated to de- scribe the relationships between 6MWT and MET for FC I/II vs FC III/IV .\n",
      "ROOT [ 0 ]:  Between- , pos tag:  JJ , dep:  ROOT\n",
      "ROOT [ 6 ]:  calculated , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 3, 7, 3, 4, 7, 0, 7, 8, 7, 12, 10, 12, 13, 14, 14, 16, 17, 10, 10, 22, 20, 7]\n",
      "*** Full sentence: Among patients in FC I or II , the correlation between MET and 6MWT was relatively weak ( r 0.36 ; B ) ; there was also more variability in MET ( between-subject coeffi- cient of variation [ CV ] 0.260 ) than in 6MWT ( CV 0.154 ) .\n",
      "ROOT [ 25 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 40 ]:  0.260 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 42 ]:  than , pos tag:  IN , dep:  ROOT\n",
      "dep_head:  [15, 1, 2, 5, 3, 5, 5, 15, 10, 15, 10, 11, 12, 12, 26, 17, 15, 17, 20, 17, 22, 15, 22, 26, 26, 0, 26, 29, 26, 29, 30, 31, 31, 35, 31, 35, 36, 39, 41, 41, 0, 41, 0, 43, 44, 45, 45, 47, 45, 43]\n",
      "*** Full sentence: When MET was < 4 or 6MWT distance was < 400 m , 1 unit in MET was equivalent to 110 m in 6MWT , whereas 1 unit in MET was only equivalent to 19 m in 6MWT when MET was > 4 or 6MWT distance was > 400 m. The 4-MET cut-off was chosen based on the fit of the data .\n",
      "ROOT [ 46 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 54 ]:  chosen , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 3, 9, 8, 8, 5, 5, 3, 47, 12, 12, 15, 12, 15, 18, 15, 16, 9, 18, 19, 22, 20, 18, 23, 9, 31, 28, 31, 28, 29, 9, 33, 31, 33, 36, 34, 31, 37, 41, 41, 47, 46, 46, 43, 43, 47, 0, 50, 50, 47, 53, 53, 55, 55, 0, 55, 56, 59, 57, 59, 62, 60, 55]\n",
      "*** Full sentence: Considering the 2 major determinants of PVRi , Qpi and mean PAP , virtually no changes were observed in Qpi in either group , and a reduction in mean PAP was reported only in the bosentan group , resulting in a significant treatment effect of -5.5 mm Hg ( P=0.0363 ) ( Table 2 ) .\n",
      "ROOT [ 17 ]:  observed , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 47 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [18, 5, 5, 5, 1, 5, 6, 7, 7, 9, 9, 9, 18, 16, 16, 18, 18, 0, 18, 19, 18, 23, 21, 18, 18, 27, 32, 27, 30, 28, 32, 18, 34, 32, 37, 37, 34, 32, 32, 39, 44, 44, 44, 40, 44, 45, 48, 0, 48, 48, 48, 53, 48, 53, 53, 48]\n",
      "*** Full sentence: Exercise and Functional Capacity The 6-minute walk distance decreased in the placebo group by 9.7±22 .3 m and increased in the bosentan-treated patients by 43.4±8.1 m , resulting in a treatment effect of 53.1 m ( P=0.008 ) ( Figure 2 ) .\n",
      "ROOT [ 0 ]:  Exercise , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 7 ]:  distance , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [0, 1, 4, 1, 8, 8, 8, 0, 8, 9, 13, 13, 10, 9, 17, 17, 14, 9, 9, 19, 23, 23, 20, 19, 26, 24, 19, 19, 28, 32, 32, 29, 32, 35, 33, 37, 35, 37, 40, 8, 40, 40, 8]\n",
      "*** Full sentence: No changes in SpO2 also were observed in an open multicenter study that included patients with Eisenmenger syndrome,17 and limited improve- ments were observed in smaller series.14 –16 In the present study , baseline 6-minute walk distance was similar to that of other studies on bosentan in PAH,12,13 although the patient population was about a decade younger ( Table 1 ) .\n",
      "ROOT [ 6 ]:  observed , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 37 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 7, 2, 3, 7, 7, 0, 7, 12, 12, 12, 8, 14, 12, 14, 15, 18, 22, 18, 18, 22, 16, 24, 7, 24, 27, 25, 24, 38, 32, 32, 29, 38, 37, 37, 37, 38, 0, 38, 39, 40, 41, 44, 42, 44, 45, 44, 47, 53, 52, 52, 53, 38, 56, 56, 57, 59, 59, 53, 59, 59, 38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: With our results , we can confirm the data of Gessler et al [ 16 ] : 20 lg of iloprost , administered by ultrasonic nebulization , induced a maximum MPAP reduc- tion of 15.6 % ; the administration of 50 lg iloprost by jet nebulization in our previous study induced a reduction of 16.4 % [ 12 ] .\n",
      "ROOT [ 6 ]:  confirm , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 50 ]:  induced , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [7, 3, 1, 7, 7, 7, 0, 9, 7, 9, 13, 13, 10, 15, 9, 15, 9, 19, 28, 19, 20, 19, 19, 23, 26, 24, 28, 7, 33, 33, 33, 33, 28, 33, 36, 34, 28, 39, 43, 39, 42, 40, 28, 43, 46, 44, 43, 50, 50, 47, 0, 53, 51, 53, 56, 54, 58, 56, 56, 51]\n",
      "*** Full sentence: Moreover , we included patients aged between 18 and 65 years diagnosed with PAH ( group I PH ) according to European Society of Cardiology ( ESC ) /European Respiratory Society ( ERS ) crite- ria , who were clinically stable and under optimal medical therapy for at least three months before participating in this study .\n",
      "ROOT [ 3 ]:  included , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 35 ]:  ria , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 0, 4, 5, 8, 11, 8, 11, 6, 11, 12, 13, 14, 14, 18, 14, 4, 4, 20, 23, 21, 23, 24, 27, 25, 27, 31, 31, 23, 33, 31, 33, 36, 0, 36, 39, 36, 41, 39, 41, 41, 46, 46, 43, 39, 49, 50, 51, 47, 39, 52, 53, 56, 54, 36]\n",
      "*** Full sentence: ProBNP ( Δ=446.85 pg/ml ) and troponin T levels ( Δ=1.38 ng/l ) were signiﬁcantly increased within CG ( p < 0.05 , ) .\n",
      "ROOT [ 0 ]:  ProBNP , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 15 ]:  increased , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 4, 4, 16, 4, 4, 4, 9, 7, 9, 12, 13, 9, 16, 16, 0, 16, 17, 22, 22, 22, 18, 18, 16, 16]\n",
      "*** Full sentence: Nine patients who had severe residual pulmonary hypertension ( MPAP > 70 mm Hg or mean PAP/mean systemic blood pressure ratio > 0.75 ) were also excluded as these patients required intensive antihypertensive management including intravenous infusion of PGE1 .\n",
      "ROOT [ 1 ]:  patients , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 9 ]:  MPAP , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 13 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 26 ]:  excluded , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 0, 4, 2, 8, 8, 8, 4, 10, 0, 13, 13, 10, 0, 14, 14, 16, 21, 20, 21, 27, 21, 22, 21, 27, 27, 0, 31, 30, 31, 27, 34, 34, 31, 34, 37, 35, 37, 38, 27]\n",
      "*** Full sentence: However , the reported hypotension rates were low , ranging from 10 % to 15 % . 26 In our study , at a daily dose of approximately 200 mg , we did not observe hypotension in the 16 patients treated with PGE1 .\n",
      "ROOT [ 6 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 17 ]:  26 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 34 ]:  observe , pos tag:  VB , dep:  ROOT\n",
      "dep_head:  [7, 7, 6, 6, 6, 7, 0, 7, 7, 7, 10, 13, 15, 15, 16, 11, 7, 0, 35, 21, 19, 35, 35, 26, 26, 23, 26, 29, 30, 27, 35, 35, 35, 35, 0, 35, 35, 40, 40, 37, 40, 41, 42, 35]\n",
      "*** Full sentence: Imatinib in Pulmonary Arterial Hypertension Patients with Inadequate Response to Established Therapy Pulmonary arterial hypertension ( PAH ) ( defined as a mean pulmonary artery pressure [ PAPm ] of > 25 mm Hg at rest and mean pulmonary capillary wedge pressure < 15 mm Hg ) leads to progressive increases in pulmonary vascular resistance ( PVR ) , right ventricular failure , and death if untreated ( 1 ) .\n",
      "ROOT [ 0 ]:  Imatinib , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 47 ]:  leads , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 66 ]:  untreated , pos tag:  JJ , dep:  ROOT\n",
      "dep_head:  [0, 1, 6, 5, 6, 2, 1, 9, 7, 9, 15, 15, 15, 15, 10, 17, 15, 17, 20, 15, 20, 26, 26, 26, 26, 21, 28, 26, 28, 26, 34, 33, 34, 30, 34, 35, 20, 20, 42, 42, 42, 38, 46, 45, 46, 48, 46, 0, 48, 51, 49, 51, 55, 55, 52, 55, 55, 55, 48, 62, 62, 48, 62, 62, 62, 67, 0, 69, 67, 69, 67]\n",
      "*** Full sentence: The oral phosphodiesterase type 5 ( PDE5 ) inhibitors sildenafil and tadalafil ( 4 ) , oral endothelin receptor antagonists ( ERAs ) bosentan , ambrisentan , and sitaxsentan ( not in the United States ) , and prostacyclin analogs epoprostenol ( intravenous ) , iloprost ( inhaled ) , and treprostinil ( inhaled , subcutaneous , or intravenous ) are approved for patients in FC II–IV .\n",
      "ROOT [ 9 ]:  sildenafil , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 23 ]:  bosentan , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 61 ]:  approved , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [4, 3, 4, 10, 4, 8, 8, 4, 10, 0, 10, 10, 12, 12, 12, 10, 20, 19, 20, 10, 20, 20, 20, 0, 24, 24, 26, 26, 26, 32, 32, 24, 35, 35, 32, 32, 24, 24, 40, 41, 24, 43, 41, 43, 41, 62, 48, 46, 48, 48, 48, 46, 54, 52, 54, 54, 56, 56, 56, 52, 62, 0, 62, 63, 64, 65, 66, 62]\n",
      "*** Full sentence: The mean ( 6 SD ) 6MWD did not change significantly in the imatinib group versus placebo ( 122 6 63 versus 21 6 53 m ; mean treatment difference 21.7 m ; 95 % confidence interval [ CI ] , 213 to 56.5 ; P 5 0.21 ) ( Table 2 , Figure 2 ) .\n",
      "ROOT [ 1 ]:  mean , pos tag:  JJ , dep:  ROOT\n",
      "ROOT [ 9 ]:  change , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 27 ]:  mean , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 50 ]:  Table , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 0, 5, 5, 2, 5, 10, 10, 10, 0, 10, 10, 15, 15, 12, 10, 16, 17, 17, 21, 19, 21, 22, 26, 26, 22, 10, 0, 30, 28, 32, 30, 30, 35, 37, 37, 30, 39, 37, 39, 37, 37, 42, 43, 30, 30, 46, 46, 30, 51, 0, 51, 51, 51, 54, 51, 51]\n",
      "*** Full sentence: Compared with the intention-to-treat population , the increase in 6MWD for the group with PVR greater than or equal to 1,000 dynes s cm25 was statistically significant with a mean treatment difference of 74 m ( 95 % CI , 3–144.1 ) .\n",
      "ROOT [ 0 ]:  Compared , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 24 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 1, 5, 5, 2, 1, 8, 6, 8, 9, 8, 13, 11, 13, 14, 13, 16, 17, 17, 19, 22, 20, 24, 25, 0, 27, 25, 27, 32, 32, 32, 28, 32, 35, 33, 39, 38, 39, 32, 39, 39, 39, 25]\n",
      "*** Full sentence: DISCUSSION This is the first randomized , double-blind , placebo-controlled trial to assess the safety , tolerability , and efficacy of the tyrosine kinase inhibitor imatinib in patients with PAH .\n",
      "ROOT [ 0 ]:  DISCUSSION , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 2 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [0, 3, 0, 11, 11, 11, 6, 11, 11, 11, 3, 13, 11, 15, 13, 15, 15, 17, 17, 17, 15, 26, 24, 26, 26, 21, 26, 27, 28, 29, 3]\n",
      "*** Full sentence: In patients with CML receiving imatinib , 0.4 % of patients per year develop congestive cardiac failure com- pared with 0 . 75 % per year for patients receiving interferon-g plus Ara-C ( 32 ) .\n",
      "ROOT [ 13 ]:  develop , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 23 ]:  % , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [14, 1, 2, 5, 3, 5, 14, 9, 14, 9, 10, 11, 12, 0, 17, 17, 18, 14, 14, 19, 22, 20, 24, 0, 24, 25, 24, 27, 28, 29, 30, 30, 30, 30, 30, 24]\n",
      "*** Full sentence: News Pharmacotherapy for pulmonary arterial hypertension A has concluded that adding selexipag to macitentan and tadalafil does not improve pulmonary vascular resistance in patients with pulmonary arterial hypertension ( PAH ) .\n",
      "ROOT [ 1 ]:  Pharmacotherapy , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 8 ]:  concluded , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 6, 6, 3, 9, 9, 0, 19, 19, 11, 11, 13, 14, 14, 19, 19, 9, 22, 22, 19, 22, 23, 24, 28, 28, 25, 28, 28, 28, 9]\n",
      "*** Full sentence: The patients given double combination therapy saw an improvement in PVR of 52 % , relative to baseline levels , whereas those assigned the triple combination saw an improvement of 54 % .\n",
      "ROOT [ 2 ]:  given , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 6 ]:  saw , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 6, 6, 3, 0, 9, 7, 9, 10, 11, 14, 12, 9, 9, 16, 19, 17, 7, 27, 27, 22, 26, 26, 23, 7, 29, 27, 29, 32, 30, 7]\n",
      "*** Full sentence: The Treatment of Pulmonary Hypertension and Sickle Cell Disease with Sildenafil Treatment ( Walk-PHaSST ) study sought to enroll subjects with any form of SCD with PH defined by an elevated TRV > 2.7 m/s and a 6MWD of 150-500 m.15 Subjects were stratified by TRV into 2 groups : those with TRV of 2.7-2.9 m/s and those with > 3.0 m/s .\n",
      "ROOT [ 16 ]:  sought , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 43 ]:  stratified , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 17, 2, 5, 3, 5, 9, 9, 5, 2, 12, 16, 16, 15, 16, 10, 0, 19, 17, 19, 20, 23, 21, 23, 24, 23, 26, 27, 28, 32, 32, 29, 19, 35, 19, 35, 38, 35, 38, 39, 17, 44, 44, 0, 44, 45, 44, 49, 47, 49, 49, 51, 52, 53, 56, 54, 53, 53, 58, 62, 62, 59, 44]\n",
      "*** Full sentence: The mPAP was 28 ± 11 mmHg at baseline , with a PVR of 157 ± 122 dynes/s/cm,5 and CO of 8.6 L/min .\n",
      "ROOT [ 2 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 6 ]:  mmHg , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 7 ]:  at , pos tag:  IN , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 3, 4, 7, 0, 0, 8, 8, 8, 13, 11, 13, 16, 14, 18, 13, 20, 13, 20, 23, 21, 8]\n",
      "*** Full sentence: DOI 10.1007/s12325-013-0029-0 Evaluation of a New Formulation of Epoprostenol Sodium in Japanese Patients with Pulmonary Arterial Hypertension ( EPITOME4 ) ABSTRACT Introduction : Pulmonary arterial hypertension ( PAH ) is associated with poor prognosis despite significant recent advances in its treatment .\n",
      "ROOT [ 2 ]:  Evaluation , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 21 ]:  Introduction , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 30 ]:  associated , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 1, 0, 3, 7, 7, 4, 7, 10, 8, 7, 13, 11, 7, 17, 17, 14, 19, 17, 19, 22, 0, 22, 26, 26, 31, 26, 26, 26, 31, 0, 31, 34, 32, 31, 38, 38, 35, 38, 41, 39, 31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: The efficacy endpoints were changes in pulmonary hemodynamic factors , WHO functional class , and N-terminal prohormone of brain natriuretic peptide ( NT-proBNP ) concentrations from baseline ( within 60 min before the first dose of epoprostenol AS ) to immediately after switching ( within 60 min after the first dose of epoprostenol AS ) or week 12 .\n",
      "ROOT [ 3 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 24 ]:  concentrations , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 41 ]:  after , pos tag:  IN , dep:  ROOT\n",
      "dep_head:  [3, 3, 4, 0, 4, 5, 9, 9, 6, 9, 13, 13, 9, 13, 13, 17, 13, 17, 21, 21, 18, 25, 25, 25, 0, 25, 26, 25, 25, 31, 29, 25, 35, 35, 32, 35, 38, 36, 25, 42, 42, 0, 42, 43, 43, 47, 45, 43, 51, 51, 48, 51, 54, 52, 43, 42, 42, 57, 42]\n",
      "*** Full sentence: Epoprostenol AS was started at the same dose of epoprostenol GM , with a mean ( range ) dose of 40 . 13 ( 17.0–61.0 ) ng/ kg/ min .\n",
      "ROOT [ 3 ]:  started , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 24 ]:  17.0–61.0 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 28 ]:  min , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 4, 8, 8, 5, 8, 11, 9, 4, 4, 17, 17, 17, 19, 17, 13, 19, 22, 20, 25, 25, 0, 25, 28, 29, 0, 29]\n",
      "*** Full sentence: The mean ( range ) change from baseline to week 12 was -–43.3 ( -196 to 43 ) pg/mL , which was not clinically significant ( Wilcoxon signed rank sum test : P = 0.5781 ) ( Fig .\n",
      "ROOT [ 11 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 37 ]:  Fig , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [6, 6, 4, 2, 6, 12, 6, 7, 6, 9, 10, 0, 19, 15, 19, 15, 16, 15, 12, 19, 22, 19, 22, 25, 22, 19, 28, 19, 30, 31, 28, 19, 35, 35, 19, 19, 38, 0, 38]\n",
      "*** Full sentence: Continuous intravenous infusion of epoprostenol ( pros- tacyclin ) has been shown to result in improved exercise capac- ity , hemodynamics , and quality of life in patients with pulmo- nary arterial hypertension ( PAH ) .1–6 Despite these positive results , epoprostenol treatment is limited by its very short half- life ( 1–2 minutes ) and chemical instability , which requires continuous intravenous infusion via a permanently implanted central venous catheter and a portable infusion pump .\n",
      "ROOT [ 11 ]:  shown , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 45 ]:  limited , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 3, 12, 3, 4, 5, 8, 5, 5, 12, 12, 0, 14, 12, 14, 17, 15, 19, 17, 19, 19, 21, 21, 21, 24, 25, 24, 27, 28, 33, 33, 33, 29, 33, 33, 33, 12, 46, 41, 41, 38, 46, 44, 46, 46, 0, 46, 52, 50, 52, 52, 47, 52, 55, 52, 55, 55, 59, 55, 59, 62, 59, 65, 65, 62, 62, 72, 69, 72, 71, 72, 66, 72, 77, 77, 77, 72, 46]\n",
      "*** Full sentence: Retrospective reviews have also indicated improved survival in patients treated with warfarin . 13,19 Given the likelihood for treprostinil and warfarin coad- ministration during PAH therapy , this drug interaction study was conducted in adult volunteers to investigate the effect of treprostinil coadministration on the pharmacodynamic and pharmacokinetics of warfarin and to assess the safety of this coadministration .\n",
      "ROOT [ 4 ]:  indicated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 13 ]:  13,19 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 32 ]:  conducted , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 5, 5, 5, 0, 7, 5, 7, 8, 9, 10, 11, 5, 0, 33, 17, 15, 17, 18, 19, 19, 23, 19, 19, 26, 24, 33, 31, 30, 31, 33, 33, 0, 33, 36, 34, 38, 33, 40, 38, 40, 43, 41, 43, 46, 44, 46, 46, 46, 49, 38, 53, 38, 55, 53, 55, 58, 56, 33]\n",
      "*** Full sentence: METHODS This was a single-center , single-blind , vehicle-con- trolled , two-period crossover study assessing the effects of continuous subcutaneous treprostinil therapy on single-dose warfarin ( 25 mg ) pharmacodynamics and pharmacokinetics and the safety of this coadministration in healthy male and fe- male volunteers .\n",
      "ROOT [ 0 ]:  METHODS , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 2 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 3, 0, 7, 7, 7, 3, 7, 10, 7, 10, 14, 14, 7, 14, 17, 15, 17, 22, 22, 22, 18, 17, 25, 23, 25, 28, 25, 28, 25, 30, 30, 25, 35, 25, 35, 38, 36, 35, 41, 45, 41, 41, 45, 39, 3]\n",
      "*** Full sentence: Two sub- jects , both receiving treprostinil , discontinued the study be- cause of intolerable treatment-related events ; ie , moderate emesis and moderate infusion-site pain .\n",
      "ROOT [ 8 ]:  discontinued , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 12 ]:  cause , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [3, 1, 9, 3, 6, 3, 6, 3, 0, 11, 9, 9, 0, 13, 17, 17, 14, 22, 22, 22, 22, 13, 22, 26, 26, 22, 13]\n",
      "*** Full sentence: DISCUSSION This study evaluated the effect of a continuous 9-day subcutaneous infusion of treprostinil on the pharmacodynam- ics and pharmacokinetics of a single oral dose ( 25 mg ) of war- farin and assessed the safety of this coadministration .\n",
      "ROOT [ 0 ]:  DISCUSSION , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 3 ]:  evaluated , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 3, 4, 0, 6, 4, 6, 12, 12, 12, 12, 7, 12, 13, 12, 18, 18, 15, 18, 18, 18, 25, 25, 25, 21, 25, 28, 25, 25, 25, 32, 30, 4, 4, 36, 34, 36, 39, 37, 4]\n",
      "*** Full sentence: The most common adverse events were headache and infusion-site reactions , which are well-known side effects of treprostinil . 10 Two subjects discon- tinued the study prematurely because of intolerable adverse events of emesis and infusion -site pain , respectively .\n",
      "ROOT [ 5 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 23 ]:  tinued , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [5, 3, 5, 5, 6, 0, 6, 7, 10, 7, 10, 13, 10, 16, 16, 13, 16, 17, 6, 22, 22, 24, 22, 0, 26, 24, 28, 24, 28, 32, 32, 28, 32, 33, 34, 38, 38, 34, 32, 32, 24]\n",
      "*** Full sentence: Information from literature search on other SLE patients with PAH treated with bosentan was also included for analysis.11,12 Patients and methods Four SLE patients who were symptomatic for their underlying PAH were recruited from a Chinese lupus cohort that satisfied the 1982 revised American College of Rheumatology ( ACR ) classification crite- ria13 and has been followed up at a university affiliated lupus clinic in Hong Kong .\n",
      "ROOT [ 15 ]:  included , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 32 ]:  recruited , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 52 ]:  ria13 , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [16, 1, 4, 2, 4, 8, 8, 5, 1, 9, 1, 11, 12, 16, 16, 0, 16, 17, 33, 19, 19, 24, 24, 21, 26, 24, 26, 33, 31, 31, 33, 33, 0, 33, 38, 38, 38, 34, 40, 38, 45, 45, 45, 45, 40, 45, 49, 49, 46, 51, 40, 38, 0, 53, 57, 57, 53, 57, 57, 64, 62, 64, 64, 59, 64, 67, 65, 53]\n",
      "*** Full sentence: To examine the differential effect of bosentan on patients with different levels of exercise tolerance , analysis was made on patients in functional class III/IV ( n = 5 , patients A , B , D , E , F ) and those in func- tional class I/II ( n = 3 , patients C , G , H ) .\n",
      "ROOT [ 18 ]:  made , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 28 ]:  5 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 51 ]:  3 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [2, 19, 5, 5, 2, 5, 6, 5, 8, 9, 12, 10, 12, 15, 13, 19, 19, 19, 0, 19, 20, 21, 24, 22, 19, 29, 29, 29, 0, 29, 32, 30, 32, 32, 34, 34, 36, 36, 38, 32, 32, 32, 32, 43, 47, 47, 44, 32, 52, 51, 52, 0, 52, 55, 53, 55, 55, 57, 55, 55, 52]\n",
      "*** Full sentence: The 6MWD were generally higher at different time points ( +33.7 m , +30.3 m , +67.3 m , +74 m at 3 , 6 , 9 , and 12 months , respectively ) for patients in functional class III/IV compared to those ( +16 m , +22 m , +51.3 m at three , six , and 12 months ) of patients in functional class I/II but the differences were not statistically significant .\n",
      "ROOT [ 2 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 70 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 5, 3, 3, 9, 9, 6, 9, 12, 41, 12, 15, 12, 12, 18, 12, 21, 21, 12, 12, 22, 23, 23, 23, 23, 23, 12, 31, 12, 31, 31, 31, 12, 35, 36, 39, 37, 12, 9, 41, 42, 46, 46, 49, 46, 49, 43, 49, 52, 49, 52, 53, 54, 54, 56, 56, 60, 56, 60, 54, 62, 63, 66, 64, 49, 71, 70, 71, 0, 71, 74, 71, 71]\n",
      "*** Full sentence: Bosentan is a receptor antagonist to endothelin-1 which is found to be pathogenic in PAH due to its vasoconstrictive and fibrogenic actions.15 This case series showed that long-term bosentan treatment resulted in significant improvement in exercise toler- ance in SLE patients with PAH and the effect was sus- tained over 12 months .\n",
      "ROOT [ 1 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 21 ]:  actions.15 , pos tag:  . , dep:  ROOT\n",
      "ROOT [ 25 ]:  showed , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 5, 5, 2, 5, 6, 10, 10, 5, 12, 10, 12, 13, 14, 12, 16, 19, 16, 19, 19, 0, 25, 25, 26, 0, 31, 30, 30, 31, 26, 31, 34, 32, 34, 35, 38, 36, 38, 41, 39, 38, 42, 43, 46, 43, 49, 49, 26, 49, 52, 49, 26]\n",
      "*** Full sentence: Deranged liver function was the most commonly reported adverse effect of bosentan but was not frequent ( two out of 21 patients on bosentan 125 mg twice daily ) and was transient.8 We found liver dyssfunction in two SLE patients in this series .\n",
      "ROOT [ 3 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 33 ]:  found , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [3, 3, 4, 0, 10, 7, 8, 10, 10, 4, 10, 11, 4, 4, 14, 14, 28, 22, 21, 21, 22, 28, 22, 23, 26, 24, 28, 14, 28, 14, 14, 31, 34, 0, 36, 34, 36, 40, 40, 37, 34, 43, 41, 34]\n",
      "*** Full sentence: Treatment of Pulmonary Arterial Hypertension Using Initial Combination Therapy of Bosentan and Iloprost BACKGROUND : Monotherapy and sequential combination therapy have been widely used in the treatment of pulmonary arterial hypertension ( PAH ) .\n",
      "ROOT [ 0 ]:  Treatment , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 23 ]:  used , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 1, 5, 5, 2, 1, 9, 9, 6, 9, 14, 11, 11, 10, 1, 24, 16, 19, 20, 16, 24, 24, 24, 0, 24, 27, 25, 27, 31, 31, 28, 31, 31, 31, 24]\n",
      "*** Full sentence: METHODS : Twenty-seven consecutive treatment-naive PAH subjects with WHO func- tional class III or IV PAH were randomized into 3 groups with a 1:1:1 ratio : a combination therapy group with 125 mg of bosentan twice daily plus 10 µg of iloprost 4 – 6 times/d ; a bosentan monotherapy group with 125 mg of bosentan twice daily ; and a iloprost monotherapy group with 10 µg of iloprost 4 – 6 times/d .\n",
      "ROOT [ 0 ]:  METHODS , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 17 ]:  randomized , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 1, 7, 7, 7, 7, 18, 7, 8, 13, 13, 13, 9, 9, 16, 9, 18, 0, 18, 21, 19, 18, 25, 25, 22, 37, 30, 29, 30, 37, 30, 33, 31, 33, 34, 37, 18, 37, 40, 37, 40, 46, 42, 42, 42, 41, 37, 51, 51, 51, 18, 51, 54, 52, 54, 55, 58, 51, 51, 51, 64, 64, 64, 51, 64, 67, 65, 67, 73, 69, 69, 69, 68, 18]\n",
      "*** Full sentence: CONCLUSIONS : Initial combination therapy in treatment-naive PAH subjects with WHO functional class III or IV can significantly improve 6MWD , hemodynamics , and quality of life compared with monotherapy .\n",
      "ROOT [ 0 ]:  CONCLUSIONS , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 18 ]:  improve , pos tag:  VB , dep:  ROOT\n",
      "dep_head:  [0, 1, 5, 5, 19, 5, 9, 9, 6, 5, 14, 13, 14, 10, 14, 14, 19, 19, 0, 19, 20, 20, 22, 22, 22, 25, 26, 19, 28, 29, 19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: Iloprost was administered at an increas- ing dose to a target of 10 µg 4 – 6 times/ d ( Fig .\n",
      "ROOT [ 2 ]:  administered , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 20 ]:  Fig , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 6, 4, 6, 7, 7, 11, 9, 11, 14, 12, 14, 14, 14, 19, 21, 19, 0, 21]\n",
      "*** Full sentence: Outcome Measures The primary end point was changes in peak 6-min walk distance ( 6MWD ) that were defined as within 10 – 60 min after iloprost inhalation at week 6 and 3 months after the initiation of treatment compared with baseline values .\n",
      "ROOT [ 1 ]:  Measures , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 6 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 6, 6, 6, 7, 0, 7, 8, 13, 13, 13, 9, 13, 13, 13, 19, 19, 13, 19, 20, 21, 24, 25, 19, 19, 28, 26, 28, 29, 30, 28, 34, 35, 19, 37, 35, 37, 38, 37, 40, 43, 41, 7]\n",
      "*** Full sentence: An increase of 133.75 ± 25.6 m was observed in subjects receiving combination therapy , 0.86 ± 17.6 m in value ( Table 2 ) .\n",
      "ROOT [ 1 ]:  increase , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 8 ]:  observed , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 5, 3, 7, 9, 9, 0, 9, 10, 11, 14, 12, 14, 19, 19, 19, 11, 19, 20, 19, 19, 23, 11, 9]\n",
      "*** Full sentence: N-terminal pro-BNP in the combination therapy group sig- nificantly decreased at both week 6 ( P = .03 ) and 3 months ( P = .01 ) compared with the monotherapy groups ( Table 3 ) .\n",
      "ROOT [ 1 ]:  pro-BNP , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 9 ]:  decreased , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 17 ]:  .03 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 7, 6, 7, 3, 2, 10, 0, 10, 13, 11, 13, 16, 18, 18, 0, 18, 18, 22, 18, 24, 22, 22, 22, 22, 22, 28, 32, 32, 29, 32, 32, 34, 32, 22]\n",
      "*** Full sentence: Quality of life significantly improved in the combination therapy group compared with the monotherapy groups , with Minnesota Living with Heart Failure questionnaire scores decreasing at week 6 ( P < .001 ) and 3 months ( P = .002 ) ( Table 3 ) .\n",
      "ROOT [ 4 ]:  improved , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 31 ]:  .001 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [5, 1, 2, 5, 0, 5, 10, 10, 10, 6, 5, 11, 15, 15, 12, 5, 5, 19, 17, 19, 22, 23, 24, 20, 24, 25, 26, 27, 30, 32, 32, 0, 32, 32, 36, 32, 38, 36, 36, 36, 36, 43, 36, 43, 43, 36]\n",
      "*** Full sentence: There were no significant differences in the incidence of adverse events in the com- naive incident PAH who were randomized to therapy with ambrisentan plus tadalafil versus ambrisentan or tadalafil monotherapy.15 The study concluded that ini- tial combination therapy with ambrisentan and tadalafil results in a significantly lower risk of clinical failure events than the risk with ambrisentan or tadalafil mono- therapy.15 Thus , the AMBITION study presented a new treatment paradigm for PAH .\n",
      "ROOT [ 1 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 33 ]:  concluded , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 61 ]:  therapy.15 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 67 ]:  presented , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 5, 5, 2, 5, 8, 6, 8, 11, 9, 11, 16, 16, 16, 12, 16, 20, 20, 16, 20, 21, 22, 23, 24, 24, 22, 27, 20, 20, 34, 33, 34, 0, 34, 39, 38, 39, 35, 39, 40, 39, 39, 43, 43, 49, 48, 49, 45, 49, 52, 53, 50, 53, 56, 54, 56, 57, 43, 43, 62, 0, 68, 68, 67, 67, 68, 0, 72, 72, 72, 68, 72, 73, 68]\n",
      "*** Full sentence: There is indeed a cogent pharmacoeconomic argument for upfront combination ther- apy : If patients with PAH are at high risk of immediate clinical deterioration or death , first-line upfront combina- tion therapy should be preferred .\n",
      "ROOT [ 1 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 35 ]:  preferred , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 7, 7, 7, 2, 7, 10, 8, 12, 36, 36, 18, 18, 15, 16, 36, 18, 21, 19, 21, 25, 25, 22, 25, 25, 36, 33, 33, 32, 33, 36, 36, 36, 0, 36]\n",
      "*** Full sentence: Conclusions In this study , first-line upfront combination therapy us- ing bosentan and iloprost in PAH subjects with WHO functional class III or IV can significantly improve 6MWD , hemodynamics , and quality of life compared with mono- therapy comprising bosentan or iloprost .\n",
      "ROOT [ 8 ]:  therapy , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 26 ]:  improve , pos tag:  VB , dep:  ROOT\n",
      "dep_head:  [9, 1, 4, 2, 9, 9, 9, 9, 0, 9, 27, 11, 12, 12, 12, 17, 15, 17, 22, 21, 22, 18, 22, 22, 27, 27, 0, 27, 28, 28, 30, 30, 30, 33, 34, 27, 36, 40, 40, 37, 40, 41, 41, 27]\n",
      "*** Full sentence: The hemodynamic parameters and the oxygenation status at baseline and at the end of inhalation with NO and iloprost are shown in Table 1 and in Fig .\n",
      "ROOT [ 2 ]:  parameters , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 20 ]:  shown , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 7, 7, 3, 3, 8, 8, 8, 13, 11, 13, 14, 15, 16, 21, 21, 21, 0, 21, 22, 23, 22, 22, 26, 21]\n",
      "*** Full sentence: 1A to C. During inhalation of NO , the mean pulmonary arterial pressure declined by 4.3 ± 8.8 mm Hg ( range , +11 to -27 mm Hg ; p = 0.008 vs. baseline ) , whereas the systemic arterial pressure and the systemic vascular resistance remained unchanged .\n",
      "ROOT [ 13 ]:  declined , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 19 ]:  Hg , pos tag:  FW , dep:  ROOT\n",
      "ROOT [ 31 ]:  0.008 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [14, 1, 2, 1, 4, 5, 6, 14, 13, 13, 13, 13, 14, 0, 14, 19, 19, 17, 15, 0, 20, 20, 22, 20, 20, 27, 25, 32, 32, 32, 32, 0, 32, 33, 33, 32, 47, 41, 41, 41, 47, 41, 46, 46, 46, 41, 32, 47, 32]\n",
      "*** Full sentence: The arterial PO2 was minimally affected during inhalation of NO ( +1.8 ± 14.1 mm Hg ; p = 0.84 ) .\n",
      "ROOT [ 5 ]:  affected , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 19 ]:  0.84 , pos tag:  CD , dep:  ROOT\n",
      "dep_head:  [3, 3, 6, 6, 6, 0, 6, 7, 8, 9, 16, 16, 16, 15, 16, 20, 20, 19, 20, 0, 20, 20]\n",
      "*** Full sentence: Aerosolized iloprost caused a decline of the mean pulmonary arterial pressure by -8.3 ± 7.5 mm Hg ( range , -33 to ±0 mm Hg ; p < 0.001 vs. baseline ) .\n",
      "ROOT [ 2 ]:  caused , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 16 ]:  Hg , pos tag:  FW , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 5, 3, 5, 11, 11, 11, 11, 6, 5, 12, 17, 16, 17, 0, 17, 17, 19, 17, 23, 24, 25, 21, 17, 17, 29, 27, 27, 30, 27, 17]\n",
      "*** Full sentence: Furthermore , iloprost caused an increase of the CO by 0.7 ± 0.6 liter/min ( range , -0.1 to +3.3 liter/min ) .\n",
      "ROOT [ 3 ]:  caused , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 13 ]:  liter/min , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 15 ]:  range , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 0, 6, 4, 6, 9, 7, 6, 12, 10, 12, 0, 16, 0, 16, 16, 16, 19, 22, 16, 16]\n",
      "*** Full sentence: The right atrial pressure fell by 1.5 ± 2.5 mm Hg ( range , -9 to +3 mm Hg ; p = 0.002 vs. baseline ) , which was not significantly different from the change of the right atrial pressure during inhalation of NO .\n",
      "ROOT [ 4 ]:  fell , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 10 ]:  Hg , pos tag:  FW , dep:  ROOT\n",
      "ROOT [ 15 ]:  to , pos tag:  TO , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 5, 0, 5, 10, 10, 10, 6, 0, 13, 11, 13, 13, 0, 18, 19, 16, 23, 22, 23, 16, 23, 24, 23, 23, 29, 23, 29, 32, 29, 32, 35, 33, 35, 40, 40, 40, 36, 35, 41, 42, 43, 23]\n",
      "*** Full sentence: Furthermore , aerosolized iloprost resulted in an increase of the arterial PO2 by 6.9 ± 14.8 mm Hg ( range , -15 to +62 mm Hg ) that was statistically significant in comparison to baseline ( p = 0.01 ) and in comparison to NO ( p = 0.02 ) .\n",
      "ROOT [ 4 ]:  resulted , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 17 ]:  Hg , pos tag:  FW , dep:  ROOT\n",
      "ROOT [ 28 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [5, 5, 4, 5, 0, 5, 8, 6, 8, 12, 12, 9, 5, 17, 17, 17, 13, 0, 18, 18, 20, 18, 18, 25, 26, 23, 18, 29, 0, 31, 29, 29, 32, 33, 34, 39, 39, 39, 35, 29, 29, 29, 42, 43, 44, 49, 49, 49, 45, 45, 29]\n",
      "*** Full sentence: Inhalation of NO and of iloprost was generally well toler- ated .\n",
      "ROOT [ 0 ]:  Inhalation , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 6 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 1, 2, 7, 7, 5, 0, 7, 7, 7, 7, 7]\n",
      "*** Full sentence: In summary , aerosolized iloprost exhibited a favorable hemodynamic response during acute drug testing in our PPH patients : pulmonary artery pressure and pulmonary vascular resistance decreased , CO and arterial PO2 in- creased , and systemic blood pressure remained stable .\n",
      "ROOT [ 5 ]:  exhibited , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 26 ]:  decreased , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [6, 1, 6, 5, 6, 0, 10, 10, 10, 6, 10, 14, 14, 11, 14, 18, 18, 15, 10, 22, 22, 27, 22, 26, 26, 22, 0, 27, 34, 29, 29, 29, 29, 27, 34, 34, 39, 39, 40, 34, 40, 27]\n",
      "*** Full sentence: Methods : Fifty-three patients scheduled for valvular heart surgery with mean pulmonary arterial pressure greater than 30 mm Hg were randomly treated with either 50 mg oral sildenafil ( n 26 ) or placebo ( n 27 ) 10 minutes before induction of anesthesia .\n",
      "ROOT [ 0 ]:  Methods , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 21 ]:  treated , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 1, 4, 1, 4, 5, 9, 9, 6, 9, 14, 14, 14, 10, 17, 17, 18, 19, 22, 22, 22, 0, 22, 28, 26, 28, 28, 23, 28, 28, 30, 30, 28, 28, 34, 34, 36, 34, 40, 41, 22, 41, 42, 43, 22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: Eligible patients also had a mean pulmonary artery pressure ( mPAP ) .30 mmHg , a pulmonary capillary wedge pressure ( PCWP ) ,15 mmHg and a pulmonary vascular resistance ( PVR ) .400 dyn ? s-1 ? cm-5 , as demonstrated by right heart catheterisa- tion within 3 months prior to the commencement of the study .\n",
      "ROOT [ 3 ]:  had , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 13 ]:  mmHg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 23 ]:  ,15 , pos tag:  NFP , dep:  ROOT\n",
      "ROOT [ 24 ]:  mmHg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 34 ]:  dyn , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 37 ]:  ? , pos tag:  . , dep:  ROOT\n",
      "ROOT [ 38 ]:  cm-5 , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 9, 9, 9, 9, 4, 9, 9, 9, 9, 0, 14, 20, 20, 20, 20, 14, 20, 20, 20, 0, 0, 25, 30, 30, 30, 25, 32, 30, 32, 30, 0, 35, 38, 0, 0, 39, 42, 39, 42, 45, 47, 45, 43, 42, 50, 48, 50, 51, 54, 52, 54, 57, 55, 39]\n",
      "*** Full sentence: During the long-term extension , a further six patients discontinued therapy permanently : one patient died from disease progression and respiratory failure ; one patient under- went lung transplantation ; two withdrew due to adverse c events ( increasing dyspnoea , leg oedema ) ; and two for unspecified reasons .\n",
      "ROOT [ 15 ]:  died , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 26 ]:  went , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [10, 4, 4, 1, 10, 9, 9, 9, 10, 16, 10, 10, 16, 15, 16, 0, 16, 19, 17, 19, 22, 19, 16, 25, 27, 25, 0, 29, 27, 27, 32, 27, 32, 35, 33, 37, 35, 37, 40, 37, 40, 43, 40, 40, 37, 37, 37, 47, 50, 48, 27]\n",
      "*** Full sentence: These included dyspnoea ( n54 ) , pancytopenia ( n52 ) , right ventricular failure , chest pain , bronchitis , diverticulitis , hypo- natraemia , hypokalaemia , peripheral oedema , syncope ( n52 ) , progression of ILD , progressive massive fibrosis , pulmonary fibrosis ( n52 ) , viral-triggered exacerbation of lung fibrosis , respiratory disorder , coronary artery disease , closed perfora- tion of a gastric ulcer and respiratory failure ( n52 ) .\n",
      "ROOT [ 1 ]:  included , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 65 ]:  tion , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 3, 3, 3, 3, 3, 10, 8, 10, 8, 15, 15, 8, 15, 18, 15, 18, 18, 20, 20, 22, 25, 64, 25, 25, 27, 30, 27, 30, 30, 32, 32, 32, 25, 25, 37, 38, 39, 43, 43, 39, 43, 46, 43, 46, 46, 46, 25, 52, 25, 52, 55, 53, 55, 58, 55, 58, 62, 62, 58, 25, 66, 66, 0, 66, 70, 70, 67, 70, 73, 70, 73, 73, 73, 66]\n",
      "*** Full sentence: A total of eight serious adverse events were considered possibly related to the study drug : syncope ( n51 ) ; dyspnoea ( n53 ) ; pancyto- penia ( n52 ) ; respiratory disorder ( n51 ) ; and respiratory failure ( n51 ) .\n",
      "ROOT [ 8 ]:  considered , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 21 ]:  dyspnoea , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 9, 2, 7, 7, 7, 3, 9, 0, 11, 9, 11, 15, 15, 12, 15, 15, 17, 17, 17, 15, 0, 22, 22, 22, 22, 28, 22, 28, 28, 28, 28, 34, 28, 34, 34, 34, 34, 34, 41, 34, 41, 41, 41, 22]\n",
      "*** Full sentence: Serum NT-proBNP did not change from baseline ( 1061.6¡1511.9 pg ? mL-1 at baseline ( n519 ) versus 1227.6¡1352.0 pg ? mL-1 ( n516 ) at week 12 ) .\n",
      "ROOT [ 4 ]:  change , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 11 ]:  mL-1 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 21 ]:  mL-1 , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 5, 5, 5, 0, 5, 6, 10, 10, 7, 5, 0, 12, 13, 16, 12, 16, 12, 20, 18, 12, 0, 22, 22, 22, 22, 26, 27, 22, 22]\n",
      "*** Full sentence: The 6MWD increased slightly to 351 ¡ 111 m after 12 weeks , from a corresponding baseline of 325¡96 m ( n517 ) ( differ- ence : +25¡64 m , 95 % CI -8–58 m ) .\n",
      "ROOT [ 2 ]:  increased , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 25 ]:  ence , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 28 ]:  m , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 3, 3, 7, 9, 9, 5, 3, 12, 10, 3, 3, 17, 17, 14, 17, 20, 18, 22, 20, 20, 26, 26, 0, 26, 29, 0, 29, 32, 35, 35, 35, 29, 29, 29]\n",
      "*** Full sentence: Background- Ambrisentan is a propanoic acid- based , A-selective endothelin receptor antagonist for the once-daily treatment of pulmonary arterial hypertension .\n",
      "ROOT [ 0 ]:  Background- , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 2 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 11 ]:  antagonist , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [0, 3, 0, 6, 6, 3, 12, 12, 11, 11, 12, 0, 12, 16, 16, 13, 16, 20, 20, 17, 12]\n",
      "*** Full sentence: Methods and Results- Ambrisentan in Pulmonary Arterial Hypertension , Randomized , Double-Blind , Placebo-Controlled , Multicenter , Efficacy Study 1 and 2 ( ARIES-1 and ARIES-2 ) were concurrent , double-blind , placebo-controlled studies that randomized 202 and 192 patients with pulmonary arterial hypertension , respectively , to placebo or ambrisentan ( ARIES- 1 , 5 or 10 mg ; ARIES-2 , 2.5 or 5 mg ) orally once daily for 12 weeks .\n",
      "ROOT [ 0 ]:  Methods , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 27 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 1, 1, 28, 4, 8, 8, 5, 4, 4, 10, 10, 12, 12, 28, 28, 16, 19, 16, 19, 19, 19, 16, 16, 24, 24, 16, 0, 28, 29, 34, 34, 34, 28, 36, 34, 40, 37, 37, 36, 40, 44, 44, 41, 36, 36, 36, 36, 48, 49, 49, 53, 51, 53, 53, 53, 56, 56, 48, 34, 34, 61, 66, 63, 63, 61, 34, 70, 70, 28, 70, 73, 71, 28]\n",
      "*** Full sentence: In 280 patients completing 48 weeks of treatment with ambrisentan monotherapy , the improvement from baseline in 6-minute walk at 48 weeks was 39 m. Conclusions- Ambrisentan improves exercise capacity in patients with pulmonary arterial hypertension .\n",
      "ROOT [ 22 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 27 ]:  improves , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [23, 3, 1, 3, 6, 4, 6, 7, 8, 11, 9, 23, 14, 23, 14, 15, 14, 19, 17, 19, 22, 20, 0, 25, 23, 23, 28, 0, 30, 28, 30, 31, 32, 36, 36, 33, 28]\n",
      "*** Full sentence: For the ARIES- 1 and 10 mg ambrisentan Placebo 60 ) 50 m ( ec 40 n ta 30 s i kD 20 l aW10 e tu 0 n i -M-10 6 -20 ARIES-2 combined 5-mg group , the distribution of WHO functional class improved from baseline to week 12 compared with the combined placebo group ( P 0 .025 ) .\n",
      "ROOT [ 0 ]:  For , pos tag:  IN , dep:  ROOT\n",
      "ROOT [ 12 ]:  m , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 21 ]:  kD , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 23 ]:  l , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 24 ]:  aW10 , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 25 ]:  e , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 26 ]:  tu , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 29 ]:  i , pos tag:  PRP , dep:  ROOT\n",
      "ROOT [ 36 ]:  group , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [0, 8, 8, 3, 3, 3, 8, 9, 1, 9, 9, 13, 0, 13, 18, 18, 18, 20, 18, 13, 22, 0, 24, 0, 0, 0, 0, 27, 27, 0, 30, 34, 34, 30, 36, 37, 0, 37, 40, 37, 40, 44, 44, 41, 40, 45, 46, 46, 48, 49, 45, 51, 56, 56, 56, 52, 56, 56, 58, 56, 56, 37]\n",
      "*** Full sentence: In ARIES-2 , the SF-36 Health Survey Physical Functioning scale significantly improved ( P 0.005 ) in the combined ambrisentan group ( 3.41 6.96 ) compared with the placebo group ( 0.20 7.14 ) ; improvements in this parameter also were noted in the individual 2.5-mg ( P 0.005 ) and 5-mg ( P 0.040 ) dose groups .\n",
      "ROOT [ 41 ]:  noted , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 45 ]:  2.5-mg , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [12, 1, 12, 7, 7, 7, 10, 10, 10, 12, 12, 42, 12, 12, 14, 12, 12, 21, 21, 21, 17, 21, 24, 21, 21, 12, 26, 30, 30, 27, 30, 33, 30, 30, 42, 42, 36, 39, 37, 42, 42, 0, 42, 45, 43, 0, 48, 46, 48, 48, 46, 46, 54, 52, 54, 54, 58, 52, 46]\n",
      "*** Full sentence: For these patients , the mean change from baseline in 6-minute walk distance for this combined ambrisentan group was 40 m ( 95 % CI , 33 to 48 m ) at week 12 and 39 m ( 95 % CI , 29 to 49 m ) at week 48 .\n",
      "ROOT [ 18 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 24 ]:  CI , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [19, 3, 1, 19, 7, 7, 19, 7, 8, 7, 13, 13, 10, 13, 18, 18, 18, 14, 0, 21, 19, 21, 24, 25, 0, 30, 29, 29, 30, 25, 30, 25, 32, 33, 32, 37, 32, 37, 40, 41, 37, 37, 45, 45, 46, 37, 37, 25, 48, 49, 25]\n",
      "*** Full sentence: The 6-minute walk distance is an independent predictor of mortality in patients with idiopathic PAH17 and has been used as the primary end point in most clinical trials in PAH .5,6,10,18 –28 In both studies , a 2-fold increase in ambrisentan dose was associated with a notable ( 20 to 27 m ) increase in 6-minute walk distance , suggesting a possible dose response .\n",
      "ROOT [ 4 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 43 ]:  associated , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 5, 0, 8, 8, 5, 8, 9, 8, 11, 12, 15, 13, 5, 19, 19, 5, 19, 24, 24, 24, 20, 19, 28, 28, 25, 28, 31, 29, 5, 44, 35, 33, 44, 39, 39, 44, 39, 42, 40, 44, 0, 44, 52, 52, 52, 51, 51, 52, 54, 52, 45, 54, 58, 58, 55, 54, 54, 64, 64, 64, 60, 44]\n",
      "*** Full sentence: The change in 6-minute walk distance appeared different for the 5-mg ambrisentan dose group in ARIES -1 ( 31 m ) and in ARIES -2 ( 59 m ) , yet the substantial overlap of 95 % CIs for 8 8 12 12 these data supports the conclusion that these results were not discordant .\n",
      "ROOT [ 6 ]:  appeared , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 19 ]:  m , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 45 ]:  supports , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [2, 7, 2, 6, 6, 3, 0, 7, 7, 14, 14, 13, 14, 9, 14, 15, 7, 20, 20, 0, 20, 46, 46, 23, 23, 28, 28, 23, 28, 23, 23, 34, 34, 23, 34, 37, 38, 35, 34, 43, 40, 43, 39, 45, 46, 0, 48, 46, 52, 51, 52, 48, 52, 52, 46]\n",
      "*** Full sentence: Similar variability between studies for 6-minute walk distance also has been observed for bosentan at the same dose ( 70 versus 35 m ) .10,21 Nevertheless , examination of the dose response for ambrisentan should be limited to comparisons within the individual studies .\n",
      "ROOT [ 11 ]:  observed , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 36 ]:  limited , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 12, 2, 3, 4, 8, 8, 5, 12, 12, 12, 0, 12, 13, 12, 18, 18, 15, 18, 22, 22, 23, 18, 18, 12, 37, 37, 37, 28, 32, 32, 29, 32, 33, 37, 37, 0, 37, 38, 39, 43, 43, 40, 37]\n",
      "*** Full sentence: This finding differs from other studies in which smaller improvements were observed in less compromised patients.5,10,19 The effect of ambrisentan on exercise capacity was maintained after 48 weeks of treatment in those continuing with ambrisentan monotherapy , reinforcing the clinical significance of the exercise improvements observed in the 12-week study .\n",
      "ROOT [ 2 ]:  differs , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 24 ]:  maintained , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 6, 4, 12, 7, 10, 12, 12, 6, 12, 15, 16, 13, 18, 25, 18, 19, 18, 23, 21, 25, 0, 25, 28, 26, 28, 29, 28, 31, 32, 33, 36, 34, 25, 25, 41, 41, 38, 41, 45, 45, 42, 45, 46, 50, 50, 47, 25]\n",
      "*** Full sentence: The reduction in B-type natriuretic peptide concentrations may be related to hemodynamic improvement , as was observed in a previous phase 2 study with ambrisentan in PAH patients.12 All ambrisentan doses were well tolerated , with most adverse events mild to moderate in severity ( Table 2 ) .\n",
      "ROOT [ 9 ]:  related , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 33 ]:  tolerated , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 10, 2, 7, 7, 7, 3, 10, 10, 0, 10, 13, 11, 10, 17, 17, 10, 17, 21, 21, 23, 23, 18, 23, 24, 23, 26, 27, 31, 31, 34, 34, 34, 0, 34, 34, 38, 39, 36, 39, 42, 40, 42, 43, 44, 44, 46, 44, 34]\n",
      "*** Full sentence: Dr Frost has served on the advisory Placebo 2.5 mg ambrisentan 5 mg ambrisentan Baseline The rates of clinical worsening ranged from 4 % to 5 % in all ambrisentan dose groups and were reduced compared with those of the placebo groups ( Table 2 and Figure 3 ) in both ARIES-1 and ARIES-2 .\n",
      "ROOT [ 3 ]:  served , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 13 ]:  ambrisentan , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 14 ]:  Baseline , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 20 ]:  ranged , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 4, 8, 8, 5, 10, 11, 8, 14, 14, 0, 0, 17, 21, 17, 20, 18, 0, 21, 24, 26, 26, 27, 22, 27, 32, 31, 32, 28, 21, 35, 21, 35, 36, 37, 38, 42, 42, 39, 42, 42, 44, 44, 44, 47, 42, 38, 52, 50, 52, 52, 21]\n",
      "*** Full sentence: The data of the Ambrisentan in Pulmonary Arterial Hypertension , Randomized , Double-Blind , Placebo-Controlled , Multicenter , Efficacy Study 1 and 2 ( ARIES-1 and ARIES-2 ) trials show that the once-daily oral active A-selective endothelin receptor antagonist ambrisentan is effective in improving exercise capacity , functional class , and clinical outcome of patients with symptomatic pulmonary arterial hypertension .\n",
      "ROOT [ 1 ]:  data , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 29 ]:  show , pos tag:  VBP , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 5, 3, 5, 9, 9, 6, 2, 2, 11, 11, 13, 13, 2, 2, 17, 20, 17, 20, 20, 20, 2, 2, 25, 25, 2, 30, 0, 41, 38, 38, 38, 38, 38, 38, 41, 40, 38, 30, 41, 42, 43, 46, 44, 46, 49, 46, 49, 49, 53, 49, 53, 54, 55, 60, 60, 60, 56, 30]\n",
      "===============  #docs with multiroot =  211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: After 16 weeks of therapy , PA mean pressure fell signiﬁcantly for the bosentan vs the placebo group ( 24 ± 6.6 mm Hg vs 1 ± 3.7 mm Hg , respectively ; P , .05 ) .\n",
      "ROOT [ 9 ]:  fell , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 23 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [10, 3, 1, 3, 4, 10, 9, 9, 10, 0, 10, 10, 14, 12, 14, 18, 18, 15, 10, 23, 23, 21, 10, 0, 24, 27, 30, 29, 30, 25, 24, 24, 24, 24, 34, 34, 34, 24]\n",
      "*** Full sentence: ORIGINAL ARTICLE Haemodynamic effects of riociguat in inoperable/ recurrent chronic thromboembolic pulmonary hypertension Methods Patients with inoperable or persistent/ recurrent CTEPH ( n=261 ; mean± SD age 59±14 years ; 66 % women ) were randomised to riociguat ( up to −213 ) ; p < 0.0001 ) and persistent/recurrent ( n=72 ; patients .\n",
      "ROOT [ 3 ]:  effects , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 35 ]:  randomised , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [4, 3, 4, 0, 4, 5, 4, 4, 13, 13, 13, 13, 15, 15, 36, 15, 21, 17, 17, 17, 16, 23, 21, 23, 27, 27, 21, 29, 27, 15, 32, 33, 15, 33, 36, 0, 38, 36, 38, 38, 40, 41, 38, 36, 46, 47, 36, 47, 36, 36, 52, 50, 50, 50, 36]\n",
      "*** Full sentence: The STRIDE-2 randomization for patients randomized to sitaxsentan 50 mg or 100 mg in the STRIDE-2 study remained double-blind until the STRIDE-2 study database was locked ; the STRIDE-2 randomization for patients randomized to pla- cebo in the STRIDE-2 study and randomized to sitaxsen- tan 100 mg in the extension study remained double-blind until the STRIDE-2 study database was locked .\n",
      "ROOT [ 17 ]:  remained , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 51 ]:  remained , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [3, 3, 18, 3, 4, 5, 6, 7, 10, 8, 10, 13, 10, 13, 17, 17, 14, 0, 18, 26, 24, 24, 24, 26, 26, 18, 18, 30, 30, 27, 30, 31, 32, 33, 36, 34, 36, 40, 40, 37, 30, 30, 42, 43, 47, 47, 52, 47, 51, 51, 48, 0, 52, 60, 58, 58, 58, 60, 60, 52, 52]\n",
      "*** Full sentence: Decreases in hemoglobin in sitaxsentan- and OL bosentan-treated patients were observed as early as week two and remained stable throughout the 18-week study ( mean change from baseline to week 18 : placebo , +0.2 g/dl ; 50 mg sitaxsentan , -0.2 g/dl ; 100 mg sitaxsentan , -0.5 g/dl ; and OL bosentan , -0.5 g/dl ) .\n",
      "ROOT [ 0 ]:  Decreases , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 10 ]:  observed , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 1, 2, 1, 4, 11, 9, 9, 11, 11, 0, 13, 11, 13, 14, 15, 11, 11, 18, 18, 23, 23, 20, 23, 26, 23, 26, 27, 26, 29, 30, 23, 23, 33, 36, 33, 23, 40, 40, 23, 40, 43, 40, 40, 47, 47, 40, 47, 50, 47, 47, 47, 54, 47, 54, 57, 54, 54, 54]\n",
      "*** Full sentence: All 10 acute responders to aerosolised iloprost ( eight of whom also responded to infused adenosine ) were treated with incremental high-dose CCB therapy ( diltiazem : mean¡SD ( range ) dosage 433¡119 ( 360–720 ) mg ? day-1 ) .\n",
      "ROOT [ 18 ]:  treated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 27 ]:  mean¡SD , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 31 ]:  dosage , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 36 ]:  mg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 38 ]:  day-1 , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 13, 6, 4, 6, 13, 13, 9, 10, 13, 19, 15, 13, 15, 13, 19, 0, 19, 24, 24, 24, 20, 26, 19, 26, 0, 30, 28, 30, 0, 32, 35, 37, 35, 0, 37, 0, 39, 39]\n",
      "*** Full sentence: Treprostinil can also be administered IV , offering convenience and safety advantages over IV epoprostenol , but line sepsis remains a threat.9 Thus , the transition from prosta- cyclin infusion to oral therapy is an appealing option for many patients with PAH .\n",
      "ROOT [ 4 ]:  administered , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 33 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [5, 5, 5, 5, 0, 5, 5, 5, 12, 9, 9, 8, 12, 15, 13, 5, 5, 19, 20, 5, 22, 20, 34, 34, 26, 34, 26, 30, 30, 27, 26, 33, 31, 0, 37, 37, 34, 37, 40, 38, 40, 41, 34]\n",
      "*** Full sentence: Treatment with selexipag resulted in a 39 % reduction in the risk of death due to PAH or hospitalization due to PAH worsening compared with placebo ( HR 0.61 ; 95 % CI 0.39–0.96 ) , consistent with the overall GRIPHON popu- lation [ ] ( Fig .\n",
      "ROOT [ 3 ]:  resulted , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 46 ]:  Fig , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 1, 2, 0, 4, 9, 8, 9, 5, 9, 12, 10, 12, 13, 9, 15, 15, 17, 17, 9, 20, 23, 20, 9, 24, 25, 26, 26, 28, 26, 32, 34, 34, 26, 26, 4, 4, 37, 43, 43, 43, 43, 38, 43, 4, 47, 0, 47]\n",
      "*** Full sentence: Long-term functional , QOL and hemodynamic changes : Mean 6MWD improved significantly over the follow up period ( Figure 3A ; p=0.006 ) , with a mean increase of 65 meters at 1 month ( p < 0.001 ) , persisting at 3 and 6 months ( 48 and 47 meters , respectively ; p < 0.01 ) .\n",
      "ROOT [ 6 ]:  changes , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 10 ]:  improved , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 7, 2, 2, 4, 4, 0, 7, 10, 11, 0, 11, 11, 15, 13, 11, 11, 17, 20, 17, 20, 20, 22, 17, 11, 28, 28, 25, 28, 31, 29, 31, 34, 32, 38, 38, 38, 28, 28, 11, 11, 41, 46, 43, 46, 42, 41, 41, 48, 51, 48, 41, 41, 41, 57, 57, 41, 57, 11]\n",
      "*** Full sentence: Preliminary data from a recent randomized , placebo- controlled trial ( STEP [ Randomized , Double-Blind , Placebo-Controlled Study of Iloprost Inhalation as Add-On Therapy to Bosentan in Pulmonary Arterial Hy- pertension ] ) ( ) in PAH patients already on bosentan compared another inhaled prostacyclin derivative , iloprost , with placebo .\n",
      "ROOT [ 1 ]:  data , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 42 ]:  compared , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 10, 10, 10, 10, 9, 10, 3, 10, 10, 14, 12, 14, 14, 16, 19, 16, 19, 22, 20, 16, 25, 23, 16, 26, 27, 31, 31, 32, 28, 14, 2, 43, 43, 43, 39, 37, 41, 39, 41, 0, 47, 47, 47, 43, 47, 47, 49, 49, 51, 43]\n",
      "*** Full sentence: Clinical worsening ( in all cases due to a decline in 6MWD by . 20 % from baseline or to ,150 m , and/or a deterioration in functional class ) occurred in four patients in the control group and in three patients in the iloprost group .\n",
      "ROOT [ 1 ]:  worsening , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 30 ]:  occurred , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 2, 6, 4, 6, 7, 10, 7, 10, 11, 10, 2, 16, 31, 16, 17, 17, 17, 20, 20, 16, 16, 26, 16, 26, 29, 27, 26, 0, 31, 34, 32, 34, 38, 38, 35, 31, 31, 42, 40, 42, 46, 46, 43, 31]\n",
      "*** Full sentence: This finding was unexpected since theoretical considerations and clinical experience provide a strong rationale for combina- tion therapy , and previous publications have yielded promis- ing results with the combination of bosentan and inhaled iloprost [ 7 , 14 ] .\n",
      "ROOT [ 3 ]:  unexpected , pos tag:  JJ , dep:  ROOT\n",
      "ROOT [ 25 ]:  ing , pos tag:  VBG , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 11, 7, 11, 7, 10, 7, 4, 14, 14, 11, 14, 18, 18, 15, 4, 4, 22, 24, 24, 4, 24, 0, 26, 27, 30, 28, 30, 31, 26, 26, 34, 37, 34, 37, 37, 37, 26]\n",
      "*** Full sentence: The percent decrease in PAP and PVR under inhaled iloprost in every single IPAH patient is given in Three of 18 patients with IPAH ( patients 1 , 5 , and 10 in showed a decrease in mean PAP of z10 mm Hg to a value of V40 mm Hg and were thus fulfilling the common American College of Chest Physicians ( ACCP ) /Europe- an Society of Cardiology ( ESC ) definition of a positive acute responder .\n",
      "ROOT [ 16 ]:  given , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 72 ]:  definition , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [3, 3, 17, 3, 4, 5, 5, 3, 10, 8, 3, 15, 15, 15, 11, 17, 0, 17, 21, 21, 22, 18, 22, 23, 24, 24, 26, 26, 26, 24, 24, 34, 32, 17, 36, 34, 36, 39, 37, 39, 43, 43, 40, 34, 46, 44, 46, 49, 50, 47, 34, 54, 54, 34, 58, 58, 58, 54, 58, 61, 59, 61, 61, 61, 34, 67, 73, 67, 73, 73, 73, 73, 0, 73, 78, 78, 78, 74, 73]\n",
      "*** Full sentence: Patients were included in the study if they had a resting mean pulmonary artery pressure ( mPAP ) 425 mm Hg , a pulmonary artery wedge pressure ( PAWP ) p15 mm Hg , and no evidence of active ischemic heart disease based on symptoms and echocardiography , and if indicated , cardiac stress test , or left heart catheterization .\n",
      "ROOT [ 2 ]:  included , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 57 ]:  left , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 6, 4, 9, 9, 3, 15, 15, 15, 15, 15, 9, 15, 15, 15, 20, 21, 58, 21, 27, 27, 27, 27, 21, 27, 27, 27, 33, 33, 21, 33, 33, 37, 33, 37, 42, 41, 42, 38, 37, 43, 44, 45, 45, 21, 21, 51, 55, 55, 54, 55, 21, 55, 55, 0, 60, 58, 58]\n",
      "*** Full sentence: Bosentan , an oral endothelin ( ET ) -A/ET-B recep- tor antagonist , has been shown to be effective and safe in the short-term and long-term treatment of nonthromboembolic pulmonary arterial hyperten - sion.6–8 The aim of the present study was to evaluate its safety and efficacy in patients with inoperable CTEPH .\n",
      "ROOT [ 15 ]:  shown , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 40 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [16, 1, 9, 5, 9, 7, 5, 7, 1, 12, 12, 1, 1, 16, 16, 0, 18, 16, 18, 19, 19, 21, 24, 22, 24, 27, 24, 27, 31, 31, 28, 34, 34, 35, 36, 41, 36, 40, 40, 37, 0, 43, 41, 45, 43, 45, 45, 45, 48, 49, 52, 50, 41]\n",
      "*** Full sentence: In a small uncontrolled series , sildenafil had a ben- eficial effect on exercise capacity and hemodynamics in patients with inoperable CTEPH . 12 Application of IV prostacyclin ( epoprostenol ) may improve exercise capacity and hemodynamic parameters in CTEPH patients listed for PEA.13 The major drawback of prostacyclin therapy in this patient population is the need for a permanent IV access that increases the risk of infection and thrombosis , potentially complicated by new thromboembolism .\n",
      "ROOT [ 7 ]:  had , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 32 ]:  improve , pos tag:  VB , dep:  ROOT\n",
      "ROOT [ 54 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [8, 5, 5, 5, 1, 8, 8, 0, 12, 12, 12, 8, 12, 15, 13, 15, 15, 15, 18, 19, 22, 20, 8, 25, 33, 25, 28, 26, 25, 25, 25, 33, 0, 35, 33, 35, 38, 35, 35, 41, 39, 41, 42, 43, 47, 47, 55, 47, 50, 48, 47, 54, 54, 51, 0, 57, 55, 57, 62, 62, 62, 58, 64, 62, 66, 64, 66, 67, 68, 68, 64, 73, 64, 73, 76, 74, 55]\n",
      "*** Full sentence: The adjusted mean change from baseline to end of treatment in mean pulmonary arterial pressure was -2.8 mm Hg in the combination therapy group com- pared with 1.1 mm Hg in the placebo group , for an adjusted treatment difference of -3.8 mm Hg ( CI , -5.6 to -2.1 mm Hg ) .\n",
      "ROOT [ 15 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 25 ]:  pared , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 43 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 3, 4, 16, 4, 5, 5, 7, 8, 9, 10, 15, 15, 15, 11, 0, 16, 19, 26, 19, 24, 24, 24, 20, 20, 0, 26, 29, 30, 27, 30, 34, 34, 31, 30, 26, 40, 40, 40, 36, 40, 43, 41, 0, 46, 44, 46, 44, 44, 49, 52, 49, 44, 44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: In contrast , among the patients who received placebo , only 1 ( 5 % ) of 20 patients improved from class III to class II , 15 ( 75 % ) remained stable , two ( 10 % ) de- teriorated from class II to class III , and two deteriorated from class III to class IV ( the two patients who died were classified as class IV ) ( P 0.585 ) ( Figure 3 ) .\n",
      "ROOT [ 19 ]:  improved , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 41 ]:  teriorated , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 71 ]:  P , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 75 ]:  Figure , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [20, 1, 20, 15, 6, 4, 8, 6, 8, 15, 12, 15, 15, 15, 20, 15, 15, 19, 17, 0, 20, 23, 21, 25, 20, 25, 25, 31, 31, 31, 25, 25, 20, 33, 33, 39, 39, 39, 33, 39, 42, 0, 42, 45, 43, 47, 42, 47, 42, 42, 52, 42, 52, 55, 53, 57, 52, 57, 58, 62, 62, 66, 64, 62, 66, 52, 66, 69, 67, 52, 72, 0, 72, 72, 76, 0, 76, 76, 76]\n",
      "*** Full sentence: DISCUSSION The current study is the first randomized , placebo-controlled tri- al to evaluate the efficacy and safety of the phosphodiesterase type 5 inhibitor vardenafil in patients with PAH .\n",
      "ROOT [ 0 ]:  DISCUSSION , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 4 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [0, 4, 4, 5, 0, 5, 8, 6, 10, 8, 10, 10, 14, 10, 16, 14, 16, 16, 16, 25, 22, 25, 22, 25, 19, 25, 26, 27, 28, 10]\n",
      "*** Full sentence: Following 12 weeks of treprostinil , there was evidence of decreased RV afterload , as per signiﬁcant reductions in mPAP , TPG , PVR , and increased pulmonary capacitance ( table 4 ; see online supple- mentary repository ﬁgure S2 ) .\n",
      "ROOT [ 7 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 39 ]:  S2 , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [8, 3, 1, 3, 4, 8, 8, 0, 8, 9, 13, 13, 10, 8, 34, 15, 18, 16, 18, 19, 20, 20, 22, 22, 24, 24, 29, 29, 24, 29, 29, 31, 29, 8, 36, 34, 40, 40, 40, 0, 40, 40]\n",
      "*** Full sentence: The therapeutic effect of Fasudil on pulmonary hypertension was reported in 2006 in Japan14 and by subsequent studies.15‐18 Xiao et al also reported the therapeutic effect of Fasudil on congenital heart disease accompanied with severe pulmonary hypertension . 19 Nevertheless , the optimal dose of Fasudil is still controversial .\n",
      "ROOT [ 9 ]:  reported , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 22 ]:  reported , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 38 ]:  19 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 46 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [3, 3, 10, 3, 4, 3, 8, 6, 10, 0, 10, 11, 10, 13, 13, 13, 19, 19, 21, 21, 23, 23, 0, 26, 26, 23, 26, 27, 26, 32, 32, 29, 26, 33, 37, 37, 34, 23, 0, 47, 47, 44, 44, 47, 44, 45, 0, 47, 47, 47]\n",
      "*** Full sentence: In the 30‐mg and 60‐mg groups , 30 or 60 mg of Fasudil ( Tianjin Chase Sun Pharmaceutical Co. , Ltd. , Tianjin , China ; approval num‐ ber : H20020356 ) in 50 ml of saline was intravenously infused over 30 minutes .\n",
      "ROOT [ 0 ]:  In , pos tag:  IN , dep:  ROOT\n",
      "ROOT [ 28 ]:  ber , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 39 ]:  infused , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 6, 6, 3, 3, 1, 6, 11, 8, 8, 1, 11, 12, 21, 19, 19, 19, 19, 21, 19, 11, 11, 11, 23, 23, 11, 11, 29, 0, 29, 40, 31, 40, 35, 33, 35, 36, 40, 40, 0, 42, 43, 40, 40]\n",
      "*** Full sentence: Discussion PAH is now the most common cause of scleroderma- related deaths .1 The availability of effective treatments for PAH makes it necessary to search more rigor- ously to identify patients who are at increased risk for PAH since early identification and early ther- apeutic intervention has the best chance of im- proving survival .\n",
      "ROOT [ 2 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 20 ]:  makes , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 52 ]:  proving , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [2, 3, 0, 3, 8, 7, 8, 3, 8, 11, 12, 9, 3, 15, 21, 15, 18, 16, 18, 19, 0, 23, 21, 25, 23, 27, 25, 27, 30, 27, 30, 33, 31, 33, 36, 34, 36, 37, 33, 41, 39, 41, 44, 46, 46, 41, 21, 50, 50, 47, 50, 51, 0, 53, 53]\n",
      "*** Full sentence: This strategy has been shown to improve exercise capacity in patients with PPH and with PAH associated with systemic sclerosis.14,15 Bosentan also led to improvements in right ventric- ular ( RV ) function.14 This compound induced hemo- dynamic effects and exercise capacity improvements that were stable at 12 months of follow-up.16 An experimental study17 as well as a clinical trial18 demonstrated an additive effect of the combination of prostanoid treatment with oral bosentan .\n",
      "ROOT [ 22 ]:  led , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 28 ]:  ular , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 35 ]:  induced , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 60 ]:  demonstrated , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 5, 5, 5, 23, 7, 5, 9, 7, 9, 10, 11, 12, 12, 12, 17, 15, 17, 18, 23, 23, 23, 0, 23, 24, 25, 28, 26, 0, 31, 29, 31, 36, 35, 36, 0, 39, 39, 36, 36, 43, 43, 36, 45, 43, 45, 45, 49, 47, 49, 50, 54, 54, 61, 57, 57, 54, 60, 60, 54, 0, 64, 64, 61, 64, 67, 65, 67, 70, 68, 70, 73, 71, 61]\n",
      "*** Full sentence: For ex- ample , the greatest incremental benefit in PVR was noted in the first 3 months of bosentan therapy ( a reduction of 99 dyne · s · cm-5 from 663 ± 386 to 564 ± 343 ; p = 0.03 ) .\n",
      "ROOT [ 0 ]:  For , pos tag:  IN , dep:  ROOT\n",
      "ROOT [ 11 ]:  noted , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 1, 12, 12, 8, 8, 8, 12, 8, 9, 12, 0, 12, 17, 17, 17, 13, 17, 20, 18, 12, 23, 8, 23, 26, 24, 26, 26, 26, 12, 12, 33, 31, 36, 36, 37, 33, 37, 33, 41, 42, 33, 33, 12]\n",
      "*** Full sentence: Improvements were noted in hemodynamics and functional capacity in PAH patients who had been treated with treprostinil . 8 This study included pa- tients with CHD and noted no difference in treat- ment effects by diagnosis .\n",
      "ROOT [ 2 ]:  noted , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 18 ]:  8 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 21 ]:  included , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [3, 3, 0, 3, 4, 5, 8, 5, 3, 11, 9, 15, 15, 15, 11, 15, 16, 3, 0, 21, 22, 0, 24, 22, 24, 25, 22, 22, 30, 28, 30, 31, 34, 31, 34, 35, 22]\n",
      "*** Full sentence: The percentage of patients in whom LFT abnormalities developed ( 12 % ) was similar to that in other classes of PAH patients treated with bosentan . 28,33 This rate of LFT changes emphasizes the need for the routine surveillance of patients receiving bosentan and is a recognized shortcoming of this class of agent .\n",
      "ROOT [ 13 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 27 ]:  28,33 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 33 ]:  emphasizes , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [2, 14, 2, 3, 9, 5, 8, 9, 4, 12, 12, 9, 12, 0, 14, 15, 16, 14, 20, 18, 20, 23, 21, 20, 24, 25, 14, 0, 30, 34, 30, 33, 31, 0, 36, 34, 36, 40, 40, 37, 40, 41, 42, 43, 34, 34, 49, 49, 46, 49, 52, 50, 52, 53, 34]\n",
      "*** Full sentence: In this group , mean PAP ( mean change , -7.6 ± 2.32 mm Hg ; p < 0.01 ) , mean PVR ( mean change , -206.31 ± 103.97 dyne · s · cm-5 ) , and mean RAP ( mean change , -2 ± 0.88 mm Hg ; p < 0.05 ) declined significantly compared to base- line levels .\n",
      "ROOT [ 0 ]:  In , pos tag:  IN , dep:  ROOT\n",
      "ROOT [ 14 ]:  Hg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 21 ]:  mean , pos tag:  VB , dep:  ROOT\n",
      "dep_head:  [0, 3, 1, 6, 6, 1, 9, 9, 6, 9, 9, 14, 14, 6, 0, 15, 19, 19, 15, 19, 22, 0, 22, 26, 26, 23, 26, 26, 28, 31, 26, 31, 31, 33, 33, 26, 22, 22, 22, 39, 43, 43, 40, 43, 43, 48, 48, 43, 43, 43, 55, 53, 51, 51, 22, 55, 55, 57, 60, 61, 58, 55]\n",
      "*** Full sentence: Indeed , levosimendan increased cyclic gua- nosine monophosphate ( cGMP ) in human umbilical vein endothelial cells ( HUVECs ) and impaired the tumor necrosis factor-a ( TNF-a ) - induced inflammatory expression of E-selectin , intercellular adhesion molecule-1 ( ICAM-1 ) , cyclooxygen- ase-2 ( COX-2 ) , and monocyte chemotactic protein-1 ( MCP-1 ) .\n",
      "ROOT [ 3 ]:  increased , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 7 ]:  monophosphate , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 47 ]:  ) , pos tag:  -RRB- , dep:  ROOT\n",
      "ROOT [ 54 ]:  MCP-1 , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [4, 4, 4, 0, 4, 4, 8, 0, 8, 8, 8, 8, 15, 15, 17, 17, 12, 17, 17, 17, 8, 8, 33, 25, 23, 25, 26, 26, 26, 31, 33, 33, 22, 33, 34, 35, 39, 39, 41, 41, 35, 41, 35, 8, 47, 47, 48, 0, 48, 52, 52, 48, 52, 55, 0, 55, 55]\n",
      "*** Full sentence: Afterwards , isobutylmethyl xanthine ( 0.5 mmol/l ) was added to the medium followed by addition of levosim- endan ( 100 nmol/l , 1 lmol/ l , 10 lmol /l ) or solvent ( dimethyl sulfoxide ) .\n",
      "ROOT [ 9 ]:  added , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 25 ]:  l , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [10, 10, 4, 10, 7, 7, 4, 7, 10, 0, 10, 13, 11, 13, 14, 15, 16, 19, 17, 22, 22, 26, 22, 26, 26, 0, 26, 29, 30, 26, 26, 26, 26, 33, 36, 33, 33, 26]\n",
      "*** Full sentence: 1009 Six-min walk distance As shown in figure 1 and table 2 , the 6MWD at baseline was 346¡66 m . After introduction of bosentan treatment , the 6MWD improved by 57 m to 403¡80 m ( p=0.0003 ) .\n",
      "ROOT [ 0 ]:  1009 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 17 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 29 ]:  improved , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 4, 4, 18, 6, 4, 6, 7, 8, 8, 8, 11, 18, 15, 18, 15, 16, 0, 20, 18, 18, 30, 22, 23, 26, 24, 30, 29, 30, 0, 30, 31, 32, 30, 34, 34, 38, 30, 38, 30]\n",
      "*** Full sentence: In comparison , in Germany , the annual treatment costs for intravenous epoprostenol are J230,000 ( at a dose of 25 ng ? kg- 1 ? min- 1 , which is a typical average dose in patients receiving epoprostenol for w1 yr [ 5 , 6 ] ) and those for intravenous iloprost are J180,000 ( at a dose of 2 ng ? kg-1 ? min-1 , a typical average dose for intravenous iloprost [ 16 ] ) .\n",
      "ROOT [ 13 ]:  are , pos tag:  VBP , dep:  ROOT\n",
      "ROOT [ 23 ]:  kg- , pos tag:  FW , dep:  ROOT\n",
      "ROOT [ 24 ]:  1 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 26 ]:  min- , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 41 ]:  yr , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 63 ]:  kg-1 , pos tag:  ADD , dep:  ROOT\n",
      "ROOT [ 70 ]:  dose , pos tag:  NN , dep:  ROOT\n",
      "dep_head:  [14, 1, 14, 14, 4, 14, 10, 10, 10, 14, 10, 13, 11, 0, 14, 15, 15, 19, 17, 19, 22, 20, 14, 0, 0, 25, 0, 27, 27, 31, 27, 35, 35, 35, 31, 35, 36, 37, 38, 39, 40, 0, 44, 42, 44, 44, 44, 44, 42, 54, 50, 53, 51, 42, 54, 55, 55, 59, 57, 59, 62, 60, 54, 0, 64, 71, 71, 71, 71, 71, 0, 71, 74, 72, 76, 71, 76, 71, 71]\n",
      "*** Full sentence: Methods : Serial 6MWTs followed by Naughton–Balke treadmill tests were performed in 47 PAH patients initiating a change in PAH therapy at baseline ( i.e. , Time 0 ) and Weeks 6 , 12 , 24 and 48 ; New York Heart Association ( NYHA ) functional class ( FC ) was also assessed .\n",
      "ROOT [ 0 ]:  Methods , pos tag:  NNS , dep:  ROOT\n",
      "ROOT [ 53 ]:  assessed , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [0, 1, 4, 11, 4, 5, 9, 9, 6, 11, 1, 11, 15, 15, 12, 15, 18, 16, 18, 21, 19, 18, 22, 27, 27, 27, 23, 27, 27, 18, 54, 31, 31, 31, 31, 31, 36, 36, 31, 43, 43, 43, 31, 43, 43, 43, 48, 43, 48, 48, 48, 54, 54, 0, 54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full sentence: Olschewski et al [ 9 ] described the use of aerosolized iloprost , a carbacyclin analogue of PGI2 , for severe pulmonary hypertension .\n",
      "ROOT [ 0 ]:  Olschewski , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 2 ]:  al , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 6 ]:  described , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [0, 3, 0, 5, 7, 5, 0, 9, 7, 9, 12, 10, 12, 16, 16, 12, 16, 17, 12, 7, 23, 23, 20, 7]\n",
      "*** Full sentence: 6MWT distance ( 52.53 m , 95 % CI =3.65 93.24 m , Cohen ’ s d = 0.45 moderate , power= 35 % ) and % 6MWT ( 6 . 73 % , 95 % CI =0.43 12 . 08 % ) sig- niﬁcantly and clinically ( > 33 m ) increased within TG after AET ( p < 0.05 ) .\n",
      "ROOT [ 1 ]:  distance , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 9 ]:  =3.65 , pos tag:  NFP , dep:  ROOT\n",
      "ROOT [ 11 ]:  m , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 16 ]:  d , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 19 ]:  moderate , pos tag:  JJ , dep:  ROOT\n",
      "ROOT [ 32 ]:  % , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 38 ]:  12 , pos tag:  CD , dep:  ROOT\n",
      "ROOT [ 41 ]:  % , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 43 ]:  sig- , pos tag:  XX , dep:  ROOT\n",
      "ROOT [ 52 ]:  increased , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 2, 5, 2, 5, 8, 9, 10, 0, 12, 0, 12, 12, 14, 14, 0, 17, 17, 0, 20, 24, 24, 20, 20, 20, 20, 27, 27, 27, 27, 33, 0, 33, 36, 37, 33, 33, 0, 39, 42, 0, 42, 0, 51, 45, 45, 51, 51, 51, 53, 51, 0, 53, 54, 53, 56, 57, 61, 61, 57, 57, 53]\n",
      "*** Full sentence: The mean ( range ) NT-proBNP concentration was 139 ( 57–240 ) pg/mL at baseline and 106 ( 41–243 ) pg/mL at week 12 .\n",
      "ROOT [ 1 ]:  mean , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 7 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "dep_head:  [2, 0, 4, 2, 4, 7, 8, 0, 8, 13, 13, 13, 9, 13, 14, 13, 21, 19, 21, 21, 13, 21, 22, 23, 8]\n",
      "*** Full sentence: Changes in peak 6MWD from baseline to week 6 were significantly different among the 3 groups ( P = .001 ) with 95.6 ± 20.3 m for the combination therapy group , Secondary Efficacy End Points There was no difference in changes in pulmonary vas- cular resistance ( P = .62 ) and cardiac output ( P = 0.074 ) from baseline to 3 months among the 3 groups ( Table 3 ) .\n",
      "ROOT [ 9 ]:  were , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 37 ]:  was , pos tag:  VBD , dep:  ROOT\n",
      "ROOT [ 60 ]:  from , pos tag:  IN , dep:  ROOT\n",
      "dep_head:  [10, 1, 2, 1, 4, 5, 5, 7, 8, 0, 12, 10, 12, 16, 16, 13, 16, 19, 20, 16, 16, 10, 24, 26, 26, 22, 26, 31, 30, 31, 27, 10, 36, 35, 36, 10, 38, 0, 40, 38, 40, 41, 42, 43, 38, 47, 38, 47, 51, 51, 47, 51, 47, 55, 47, 55, 55, 59, 55, 55, 0, 61, 61, 65, 63, 61, 69, 69, 66, 69, 69, 71, 69, 61]\n",
      "*** Full sentence: Silde- nafil citrate ( INN sildenafil ) is a selective phosphodiesterase type 5 inhibitor that is being increasingly recognized as a treatment modality for pulmonary hypertension .\n",
      "ROOT [ 0 ]:  Silde- , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 1 ]:  nafil , pos tag:  VBZ , dep:  ROOT\n",
      "dep_head:  [0, 0, 8, 6, 6, 3, 6, 2, 14, 14, 14, 14, 12, 8, 19, 19, 19, 19, 14, 19, 23, 23, 20, 23, 26, 24, 8]\n",
      "*** Full sentence: Even though 50 mg oral sildenafil has been proved to be sufficient to produce significant pulmonary vasodilatio studies with oral sildenafil anesthetized animals have reported reduced plasma concen- tration rather than delayed apwtihoinc , h could be verified in this study and is a limitation of this study .\n",
      "ROOT [ 8 ]:  proved , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 37 ]:  verified , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [9, 9, 6, 6, 6, 9, 9, 9, 0, 11, 9, 11, 14, 12, 18, 18, 18, 14, 18, 21, 19, 23, 25, 25, 38, 27, 29, 27, 25, 31, 29, 33, 29, 38, 38, 38, 38, 0, 38, 41, 39, 38, 38, 45, 43, 45, 48, 46, 38]\n",
      "*** Full sentence: mPAP was virtually unchanged ( 40¡10 mmHg at baseline versus 41¡7 mmHg at week 12 ; difference : +1¡10 mmHg , 95 % CI -5–6 mmHg ) .\n",
      "ROOT [ 16 ]:  difference , pos tag:  NN , dep:  ROOT\n",
      "ROOT [ 19 ]:  mmHg , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 25 ]:  mmHg , pos tag:  NNP , dep:  ROOT\n",
      "dep_head:  [2, 17, 4, 7, 7, 7, 2, 2, 8, 9, 12, 10, 2, 13, 14, 17, 0, 17, 20, 0, 20, 23, 26, 26, 26, 0, 26, 26]\n",
      "*** Full sentence: drugs endothelin hypertension , pulmonary receptors P disease characterized by an elevated pulmonary vas ulmonary arterial hypertension ( PAH ) is a progressive cular resistance leading to right ventricular failure and premature death . 1 Current therapies approved for the treatment of PAH in the United States and/or Europe include prostacyclin analogues administered by intravenous , inhaled , and subcutaneous routes ; the oral endothelin-receptor antagonists bosentan and sitaxsentan ; and the oral phosphodiesterase type 5 inhibitor sildenafil .\n",
      "ROOT [ 20 ]:  is , pos tag:  VBZ , dep:  ROOT\n",
      "ROOT [ 49 ]:  include , pos tag:  VBP , dep:  ROOT\n",
      "dep_head:  [3, 3, 21, 21, 6, 8, 8, 21, 8, 9, 17, 17, 17, 17, 17, 17, 10, 17, 17, 17, 0, 25, 25, 25, 21, 25, 26, 30, 30, 27, 30, 33, 30, 21, 37, 37, 50, 37, 38, 41, 39, 41, 42, 41, 47, 47, 44, 37, 37, 0, 52, 50, 52, 53, 54, 53, 53, 57, 57, 61, 57, 50, 66, 65, 66, 50, 66, 67, 67, 66, 66, 78, 78, 78, 78, 75, 78, 66, 50]\n",
      "*** Full sentence: ARIES-1 patients were randomized to placebo ( n 67 ) or 5 mg ( n 67 ) or 10 mg ( n 68 ) ambrisentan once daily ; ARIES-2 patients were randomized to placebo ( n 65 ) or 2.5 mg ( n 64 ) or 5 mg ( n 63 ) ambrisentan once daily .\n",
      "ROOT [ 3 ]:  randomized , pos tag:  VBN , dep:  ROOT\n",
      "ROOT [ 24 ]:  ambrisentan , pos tag:  NNP , dep:  ROOT\n",
      "ROOT [ 31 ]:  randomized , pos tag:  VBN , dep:  ROOT\n",
      "dep_head:  [2, 4, 4, 0, 4, 5, 6, 6, 8, 8, 8, 13, 8, 13, 13, 15, 13, 8, 20, 8, 20, 20, 22, 20, 0, 27, 25, 32, 30, 32, 32, 0, 32, 33, 34, 34, 36, 36, 36, 41, 36, 41, 41, 43, 41, 36, 48, 36, 48, 48, 50, 48, 34, 55, 32, 32]\n",
      "===============  #docs with multiroot =  41\n"
     ]
    }
   ],
   "source": [
    "#augmentation from Santosh et. al\n",
    "augmenter = JsonInputAugmenter()\n",
    "augmenter.augment_docs_in_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_sci_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_sci_sm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_sci_sm-3.0.0/en_core_sci_sm-3.0.0.tar.gz\n",
    "# !pip install spacy==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>Selective pulmonary vasodilation is an advanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>We hypothesized that milrinone, an adenosine-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>Consequently, we investigated the hemodynamic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>During mechanical ventilation and using a conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>In the same manner, 11 patients received iPGI2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id                      filename  \\\n",
       "0            1  00000539-200112000-00018.txt   \n",
       "1            2  00000539-200112000-00018.txt   \n",
       "2            3  00000539-200112000-00018.txt   \n",
       "3            4  00000539-200112000-00018.txt   \n",
       "4            5  00000539-200112000-00018.txt   \n",
       "\n",
       "                                            sentence  \n",
       "0  Selective pulmonary vasodilation is an advanta...  \n",
       "1  We hypothesized that milrinone, an adenosine-3...  \n",
       "2  Consequently, we investigated the hemodynamic ...  \n",
       "3  During mechanical ventilation and using a conv...  \n",
       "4  In the same manner, 11 patients received iPGI2...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = pd.read_csv('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/sentences.csv', delimiter = '\\t')\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "jsonlist_a = []\n",
    "jsonlist_f = []\n",
    "sglobal = 0\n",
    "for i in range(sent.shape[0]):\n",
    "    st = sent['sentence'][i]\n",
    "    file = sent['filename'][i]\n",
    "    s = []\n",
    "    for t in word_tokenize(st):\n",
    "        s.append(pp.sub('\\\"', t))\n",
    "    if len(s)>5 and len(s)<60:\n",
    "        entities = [{\"type\": \"drug\", \"start\":0, \"end\": 1}, {\"type\":\"reason\", \"start\":1, \"end\":2}]\n",
    "        relations = [{\"type\":\"NotRel\", \"head\": 0, \"tail\":1}]\n",
    "        jsonlist_a.append({\"tokens\": s, \"entities\": entities, \"relations\": relations, \"orig_id\": sglobal})\n",
    "        jsonlist_f.append({\"tokens\": s, \"entities\": entities, \"relations\": relations, \"orig_id\": sglobal, \"sentence\": sent['sentence_id'][i], \"file\":file})\n",
    "        sglobal+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/articles_inf_a.json', 'w') as f:\n",
    "    f.write(json.dumps(jsonlist_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_j.healun.2016.12.012.txt\n",
      "ann_1471-2466-11-25.txt\n",
      "ann_222.full.txt\n",
      "ann_chest.10-0969.txt\n",
      "ann_j.rmed.2017.05.008.txt\n",
      "ann_j.healun.2006.11.009.txt\n",
      "ann_chest.130.5.1471.txt\n",
      "ann_03007995.2011.605440.txt\n",
      "ann_j.healun.2008.04.009.txt\n",
      "ann_j.1365-2141.2005.05625.x.txt\n",
      "ann_chest.11-0404.txt\n",
      "ann_s12872-017-0569-3.txt\n",
      "ann_1410.full.txt\n",
      "ann_j.jacc.2005.04.050.txt\n",
      "ann_s40256-017-0262-z.txt\n",
      "ann_s12872-016-0361-9.txt\n",
      "ann_030079905x30680.txt\n",
      "ann_j.jvca.2007.10.015.txt\n",
      "ann_0003-4975_2895_2900408-d.txt\n",
      "ann_bloodadvances.2019000883.txt\n",
      "ann_j.1365-2362.2006.01688.x.txt\n",
      "ann_09031936.04.00111904.txt\n",
      "ann_09031936.00182909.txt\n",
      "ann_j.healun.2013.06.008.txt\n",
      "ann_72.full.txt\n",
      "ann_1602493.full.txt\n",
      "ann_NEJMoa1209655.txt\n",
      "ann_CIRCRESAHA.114.305951.txt\n",
      "ann_jcm-06-00043-v2.txt\n",
      "ann_rccm.200605-694OC.txt\n",
      "ann_hjh.0b013e3282f382ff.txt\n",
      "ann_bcp.14039.txt\n",
      "ann_Phosphodisesterase inhibitors in Pulmonary hypertension.txt\n",
      "ann_2000513.full.txt\n",
      "ann_j.ijcard.2010.10.051.txt\n",
      "ann_s0140-6736_2895_2991504-4.txt\n",
      "ann_691.full.txt\n",
      "ann_j.clinthera.2013.02.013.txt\n",
      "ann_j.ijcard.2017.04.016.txt\n",
      "ann_j.jacc.2006.05.070.txt\n",
      "ann_09031936.04.00028404.txt\n",
      "ann_s0735-1097_2899_2900312-5.txt\n",
      "ann_jac_2Fdkw125.txt\n",
      "ann_circulationaha.106.618397.txt\n",
      "ann_s10157-016-1344-y.txt\n",
      "ann_chest.07-0767.txt\n",
      "ann_s13318-017-0424-z.txt\n",
      "ann_NEJMoa012212.txt\n",
      "ann_rccm.200308-1142OC.txt\n",
      "ann_234.full.txt\n",
      "ann_chest.13-1766.txt\n",
      "ann_s0735-1097_2803_2900121-9.txt\n",
      "ann_j.ijcard.2017.02.094.txt\n",
      "ann_s00246-007-9139-2.txt\n",
      "ann_circulationaha.111.081125.txt\n",
      "ann_CIRCHEARTFAILURE.108.796789.txt\n",
      "ann_11534940-000000000-00000.txt\n",
      "ann_113.full.txt\n",
      "ann_1343.full.txt\n",
      "ann_446.full.txt\n",
      "ann_s00392-005-0266-6.txt\n",
      "ann_rccm.201506-1091le.txt\n",
      "ann_bf02044111.txt\n",
      "ann_j.healun.2018.07.001.txt\n",
      "ann_j.pupt.2014.04.007.txt\n",
      "ann_j.ijcard.2015.08.080.txt\n",
      "ann_pedsPAH.txt\n",
      "ann_chest.129.6.1636.txt\n",
      "ann_j.healun.2012.04.005.txt\n",
      "ann_j.pupt.2018.02.005.txt\n",
      "ann_jcph.152.txt\n",
      "ann_ajh_2Fhps057.txt\n",
      "ann_1477-7525-11-31.txt\n",
      "ann_s00277-018-3518-z.txt\n",
      "ann_chest.14-0193.txt\n",
      "ann_11539110-000000000-00000.txt\n",
      "ann_ajrccm.163.4.2007116.txt\n",
      "ann_09031936.05.00075305.txt\n",
      "ann_s0100-879x2008000800003.txt\n",
      "ann_200330.full.txt\n",
      "ann_fjc.0b013e31827e0fa9.txt\n",
      "ann_heartjnl-2016-309621.txt\n",
      "ann_j.jacc.2006.12.037.txt\n",
      "ann_0091270011398241.txt\n",
      "ann_13993003.00364-2015.txt\n",
      "ann_ham.2010.1075.txt\n",
      "ann_circj.cj-11-0473.txt\n",
      "ann_j.clinthera.2016.03.014.txt\n",
      "ann_405.full.txt\n",
      "ann_j.jacc.2006.01.057.txt\n",
      "ann_09031936.00105914.txt\n",
      "ann_jcph.193.txt\n",
      "ann_j.echo.2005.07.019.txt\n",
      "ann_13993003.01886-2017.txt\n",
      "ann_hrt.79.2.175.txt\n",
      "ann_j.pupt.2008.11.009.txt\n",
      "ann_j.jpeds.2012.05.050.txt\n",
      "ann_journal.pone.0114309.txt\n",
      "ann_ppul.23032.txt\n",
      "ann_0091270004270833.txt\n",
      "ann_j.hlc.2018.12.005.txt\n",
      "ann_hrt.2004.051961.txt\n",
      "ann_s00228-007-0408-z.txt\n",
      "ann_000446583.txt\n",
      "ann_j.1540-8175.2012.01809.x.txt\n",
      "ann_rccm.200603-358OC.txt\n",
      "ann_j.jacc.2004.06.060.txt\n",
      "ann_S2213-2600_2814_2970013-X.txt\n",
      "ann_09031936.02.02572001.txt\n",
      "ann_j.vph.2005.03.003.txt\n",
      "ann_s00408-014-9667-5.txt\n",
      "ann_0091270008319793.txt\n",
      "ann_ehf2.12630.txt\n",
      "ann_j.rmed.2016.06.018.txt\n",
      "ann_v058p00797.txt\n",
      "ann_0961203307076509.txt\n",
      "ann_j.hlc.2018.04.299.txt\n",
      "ann_jps.21789.txt\n",
      "ann_1701214.full.txt\n",
      "ann_chest.12-3023.txt\n",
      "ann_1465-9921-6-92.txt\n",
      "ann_j.healun.2006.09.016.txt\n",
      "ann_498.full.txt\n",
      "ann_mtc.2003.107.txt\n",
      "ann_thx.2005.041954.txt\n",
      "ann_hr.2009.113.txt\n",
      "ann_503.full.txt\n",
      "ann_j.jtcvs.2003.11.035.txt\n",
      "ann_j.echo.2005.07.010.txt\n",
      "ann_s002469900463.txt\n",
      "ann_j.1365-2125.2009.03384.x.txt\n",
      "ann_biomolecules-10-01261-v3.txt\n",
      "ann_pah-novinky-bwv.txt\n",
      "ann_rccm.201809-1631LE.txt\n",
      "ann_psp4.12202.txt\n",
      "ann_09031936.00176312.txt\n",
      "ann_rccm.200410-1411OC.txt\n",
      "ann_944.full.txt\n",
      "ann_j.healun.2009.09.005.txt\n",
      "ann_s00296-004-0439-z.txt\n",
      "ann_j.jclinane.2005.08.018.txt\n",
      "ann_67.full.txt\n",
      "ann_13993003.00090-2016.txt\n",
      "ann_1753465808103499.txt\n",
      "ann_170134.full.txt\n",
      "ann_j.cardfail.2009.09.008.txt\n",
      "ann_000242498.txt\n",
      "ann_j.hlc.2012.06.013.txt\n",
      "ann_1303.full.txt\n",
      "ann_j.vph.2021.106840.txt\n",
      "ann_S2213-2600_2816_2930307-1.txt\n",
      "ann_j.hrtlng.2020.04.006.txt\n",
      "ann_bmjopen-2016-011028.txt\n",
      "ann_rccm.201001-0123oc.txt\n",
      "ann_03007990903210066.txt\n",
      "ann_1074248411429966.txt\n",
      "ann_bcp.13267.txt\n",
      "ann_gut.2005.077453.txt\n",
      "ann_030079905x46232.txt\n",
      "ann_chest.06-2690.txt\n",
      "ann_j.ijcard.2019.07.004.txt\n",
      "ann_s0735-1097_2802_2901786-2.txt\n",
      "ann_circj.cj-16-0254.txt\n",
      "ann_j.healun.2014.02.019.txt\n",
      "ann_s00296-006-0222-4.txt\n",
      "ann_ajh_2Fhpt018.txt\n",
      "ann_j.ijcard.2004.09.002.txt\n",
      "ann_fjc.0b013e3182893d90.txt\n",
      "ann_09031936.98.12040932.txt\n",
      "ann_j.rmed.2017.03.025.txt\n",
      "ann_S2213-2600_2820_2930394-5.txt\n",
      "ann_CIRCULATIONAHA.107.742510.txt\n",
      "ann_circj.69.335.txt\n",
      "ann_09031936.06.00030206.txt\n",
      "ann_j.ejcts.2010.01.045.txt\n",
      "ann_chest.06-2903.txt\n",
      "ann_j.1939-1676.2010.0517.x.txt\n",
      "ann_S2213-2600_2816_2930019-4.txt\n",
      "ann_ar3883.txt\n",
      "ann_01.jcm.0000203850.97890.fe.txt\n",
      "ann_000107977.txt\n",
      "ann_heart.86.6.661.txt\n",
      "ann_787.full.txt\n",
      "ann_s11748-016-0724-2.txt\n",
      "ann_circj.cj-10-1310.txt\n",
      "ann_j.amjcard.2013.04.051.txt\n",
      "ann_13993003.02044-2014.txt\n",
      "ann_chest.128.2.709.txt\n",
      "ann_13993003.01030-2019.txt\n",
      "ann_s00296-008-0789-z.txt\n",
      "ann_chest.07-2324.txt\n",
      "ann_circulationaha.110.016667.txt\n",
      "ann_1745-6215-14-91.txt\n",
      "ann_s00330-012-2606-z.txt\n",
      "ann_j.transproceed.2007.08.069.txt\n",
      "ann_s00380-009-1176-8.txt\n",
      "ann_j.rmed.2016.11.001.txt\n",
      "ann_5-MVD_PAHT_Stepien_JSAP2009.txt\n",
      "ann_Sutendra-2013-Pulmonary arterial h.txt\n",
      "ann_7.full.txt\n"
     ]
    }
   ],
   "source": [
    "#inference for my own model based on Roberta\n",
    "\n",
    "pp = re.compile('(?<!\\\\\\\\)\\'')\n",
    "\n",
    "nfiles = 0\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "bio_annotation = []\n",
    "entity = []\n",
    "index = []\n",
    "sentence = []\n",
    "entity_unique = []\n",
    "\n",
    "dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/DownloadToInception/'\n",
    "# dir_ = '/Users/aelitta/Documents/DMMatrix/PAH/готовые файлы tsv/'\n",
    "#listfiles = os.listdir(\"Documents/Cancer/CLL/Разметка/\")\n",
    "listfiles = os.listdir(dir_)\n",
    "\n",
    "entities = [] #entities for joint NER and RE model\n",
    "relations = [] #relationss for joint NER and RE model\n",
    "\n",
    "jsonlist_inf = [] #list of jsons for joint NER+RE model\n",
    "sglobal = 0 #sentence counter\n",
    "\n",
    "for file in listfiles:\n",
    "    print(file)\n",
    "    if file[-3:]=='txt' :\n",
    "        nfiles+=1\n",
    "        entities = []\n",
    "        tokens = []\n",
    "        relations = []\n",
    "        token_counter = 0 \n",
    "        ent_token_counter = 0\n",
    "        sglobal = 0\n",
    "        with open(dir_+file) as tsvfile:\n",
    "            tsvreader = csv.reader(tsvfile, delimiter=\" \")\n",
    "            for line in tsvreader:\n",
    "#                 print(line)\n",
    "                if len(line) == 0:\n",
    "                    relations = []\n",
    "                    for h in range(len(entities)):\n",
    "                        for t in range(len(entities)):\n",
    "                            if h!=t:\n",
    "                                relations.append({\"type\":\"NotRel\", \"head\": h, \"tail\":t})\n",
    "                    jsonlist_inf.append({\"tokens\": tokens, \"entities\": entities, \"relations\": relations, \"orig_id\": sglobal})\n",
    "                    tokens = []\n",
    "                    relations = []\n",
    "                    entities = []\n",
    "                    token_counter = 0 \n",
    "                    ent_token_counter = 0\n",
    "                    sglobal+=1\n",
    "                \n",
    "                else:\n",
    "                    if line[1].find('B-')==0:\n",
    "                        ent_start = token_counter\n",
    "                        ent_token_counter+=1\n",
    "                        ent_type = line[1].split('-')[1].lower().replace('ntprobpn','nt-probnp').\\\n",
    "                        replace('progression_rate','progression').replace('hospitalization_rate','hospitalization').\\\n",
    "                        replace('death', 'death_rate').replace('death_rate_rate','death_rate')\n",
    "                    if line[1].find('I-')==0 and ent_token_counter>0:\n",
    "                        ent_token_counter+=1\n",
    "                    if line[1] == 'O' and ent_token_counter >0:\n",
    "                            ent_end = ent_start+ent_token_counter\n",
    "                            ent_token_counter = 0 \n",
    "                            entities.append({\"type\": ent_type, \"start\": ent_start, \"end\": ent_end})\n",
    "                    token_counter+=1\n",
    "                    tokens.append(line[0])\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/articles_inf_aa.json', 'w') as f:\n",
    "    f.write(json.dumps(jsonlist_inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "9\n",
      "15\n",
      "16\n",
      "18\n",
      "20\n",
      "26\n",
      "28\n",
      "31\n",
      "45\n",
      "48\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "56\n",
      "57\n",
      "61\n",
      "70\n",
      "73\n",
      "76\n",
      "82\n",
      "87\n",
      "90\n",
      "128\n",
      "145\n",
      "177\n",
      "185\n",
      "189\n",
      "192\n",
      "200\n",
      "219\n",
      "224\n",
      "233\n",
      "236\n",
      "242\n",
      "246\n",
      "247\n",
      "248\n",
      "251\n",
      "262\n",
      "267\n",
      "268\n",
      "279\n",
      "285\n",
      "286\n",
      "289\n",
      "290\n",
      "301\n",
      "303\n",
      "335\n",
      "374\n",
      "375\n",
      "376\n",
      "378\n",
      "379\n",
      "403\n",
      "433\n",
      "434\n",
      "458\n",
      "494\n",
      "495\n",
      "497\n",
      "504\n",
      "508\n",
      "512\n",
      "524\n",
      "525\n",
      "528\n",
      "532\n",
      "533\n",
      "534\n",
      "546\n",
      "569\n",
      "570\n",
      "582\n",
      "602\n",
      "603\n",
      "605\n",
      "606\n",
      "607\n",
      "610\n",
      "611\n",
      "613\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "621\n",
      "623\n",
      "627\n",
      "632\n",
      "633\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "658\n",
      "659\n",
      "661\n",
      "663\n",
      "666\n",
      "667\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "687\n",
      "690\n",
      "693\n",
      "696\n",
      "698\n",
      "700\n",
      "701\n",
      "702\n",
      "710\n",
      "711\n",
      "712\n",
      "714\n",
      "716\n",
      "720\n",
      "722\n",
      "724\n",
      "761\n",
      "762\n",
      "770\n",
      "774\n",
      "778\n",
      "779\n",
      "782\n",
      "783\n",
      "784\n",
      "792\n",
      "801\n",
      "806\n",
      "810\n",
      "811\n",
      "817\n",
      "825\n",
      "829\n",
      "830\n",
      "831\n",
      "836\n",
      "837\n",
      "838\n",
      "843\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "858\n",
      "860\n",
      "878\n",
      "879\n",
      "882\n",
      "883\n",
      "888\n",
      "889\n",
      "893\n",
      "897\n",
      "902\n",
      "910\n",
      "911\n",
      "920\n",
      "921\n",
      "924\n",
      "926\n",
      "927\n",
      "932\n",
      "934\n",
      "940\n",
      "943\n",
      "945\n",
      "949\n",
      "956\n",
      "958\n",
      "964\n",
      "972\n",
      "973\n",
      "974\n",
      "976\n",
      "979\n",
      "980\n",
      "981\n",
      "986\n",
      "990\n",
      "991\n",
      "995\n",
      "997\n",
      "1002\n",
      "1005\n",
      "1006\n",
      "1008\n",
      "1010\n",
      "1013\n",
      "1014\n",
      "1017\n",
      "1020\n",
      "1032\n",
      "1033\n",
      "1058\n",
      "1068\n",
      "1071\n",
      "1080\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1090\n",
      "1093\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1117\n",
      "1118\n",
      "1120\n",
      "1121\n",
      "1124\n",
      "1130\n",
      "1133\n",
      "1137\n",
      "1139\n",
      "1141\n",
      "1142\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1156\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1163\n",
      "1164\n",
      "1166\n",
      "1169\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1179\n",
      "1182\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1220\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1248\n",
      "1250\n",
      "1253\n",
      "1257\n",
      "1258\n",
      "1264\n",
      "1267\n",
      "1270\n",
      "1275\n",
      "1277\n",
      "1283\n",
      "1284\n",
      "1288\n",
      "1292\n",
      "1294\n",
      "1295\n",
      "1297\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1303\n",
      "1304\n",
      "1315\n",
      "1319\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1330\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1343\n",
      "1348\n",
      "1354\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1369\n",
      "1371\n",
      "1375\n",
      "1376\n",
      "1385\n",
      "1389\n",
      "1391\n",
      "1394\n",
      "1397\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1411\n",
      "1425\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1438\n",
      "1452\n",
      "1454\n",
      "1455\n",
      "1459\n",
      "1464\n",
      "1479\n",
      "1488\n",
      "1493\n",
      "1559\n",
      "1560\n",
      "1563\n",
      "1564\n",
      "1567\n",
      "1570\n",
      "1571\n",
      "1575\n",
      "1576\n",
      "1579\n",
      "1580\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1588\n",
      "1591\n",
      "1592\n",
      "1599\n",
      "1614\n",
      "1626\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1640\n",
      "1643\n",
      "1647\n",
      "1648\n",
      "1650\n",
      "1656\n",
      "1660\n",
      "1663\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1686\n",
      "1687\n",
      "1691\n",
      "1692\n",
      "1696\n",
      "1702\n",
      "1716\n",
      "1718\n",
      "1722\n",
      "1723\n",
      "1735\n",
      "1745\n",
      "1747\n",
      "1748\n",
      "1753\n",
      "1754\n",
      "1762\n",
      "1769\n",
      "1772\n",
      "1791\n",
      "1795\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1836\n",
      "1841\n",
      "1842\n",
      "1846\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1853\n",
      "1856\n",
      "1857\n",
      "1864\n",
      "1871\n",
      "1872\n",
      "1887\n",
      "1888\n",
      "1897\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1903\n",
      "1906\n",
      "1923\n",
      "1925\n",
      "1935\n",
      "1939\n",
      "1942\n",
      "1944\n",
      "1945\n",
      "1947\n",
      "1970\n",
      "1973\n",
      "1985\n",
      "1986\n",
      "1991\n",
      "1994\n",
      "1997\n",
      "2015\n",
      "2019\n",
      "2027\n",
      "2028\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2035\n",
      "2037\n",
      "2049\n",
      "2051\n",
      "2052\n",
      "2057\n",
      "2060\n",
      "2063\n",
      "2066\n",
      "2069\n",
      "2073\n",
      "2078\n",
      "2083\n",
      "2088\n",
      "2094\n",
      "2098\n",
      "2103\n",
      "2104\n",
      "2118\n",
      "2119\n",
      "2121\n",
      "2124\n",
      "2126\n",
      "2130\n",
      "2133\n",
      "2135\n",
      "2136\n",
      "2138\n",
      "2141\n",
      "2143\n",
      "2144\n",
      "2146\n",
      "2152\n",
      "2155\n",
      "2156\n",
      "2158\n",
      "2161\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2171\n",
      "2177\n",
      "2178\n",
      "2181\n",
      "2189\n",
      "2201\n",
      "2204\n",
      "2205\n",
      "2207\n",
      "2211\n",
      "2212\n",
      "2215\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2222\n",
      "2223\n",
      "2228\n",
      "2234\n",
      "2235\n",
      "2237\n",
      "2249\n",
      "2251\n",
      "2252\n",
      "2255\n",
      "2258\n",
      "2265\n",
      "2268\n",
      "2270\n",
      "2271\n",
      "2274\n",
      "2280\n",
      "2282\n",
      "2283\n",
      "2288\n",
      "2289\n",
      "2292\n",
      "2296\n",
      "2298\n",
      "2299\n",
      "2301\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2310\n",
      "2312\n",
      "2317\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2325\n",
      "2328\n",
      "2339\n",
      "2345\n",
      "2347\n",
      "2348\n",
      "2351\n",
      "2352\n",
      "2354\n",
      "2356\n",
      "2362\n",
      "2364\n",
      "2365\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2372\n",
      "2377\n",
      "2378\n",
      "2381\n",
      "2382\n",
      "2391\n",
      "2393\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2403\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2409\n",
      "2416\n",
      "2421\n",
      "2430\n",
      "2432\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2467\n",
      "2470\n",
      "2473\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2480\n",
      "2487\n",
      "2490\n",
      "2491\n",
      "2493\n",
      "2494\n",
      "2497\n",
      "2505\n",
      "2507\n",
      "2509\n",
      "2511\n",
      "2517\n",
      "2519\n",
      "2527\n",
      "2532\n",
      "2535\n",
      "2536\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2547\n",
      "2551\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2561\n",
      "2562\n",
      "2564\n",
      "2565\n",
      "2571\n",
      "2574\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2585\n",
      "2587\n",
      "2588\n",
      "2599\n",
      "2601\n",
      "2603\n",
      "2630\n",
      "2632\n",
      "2635\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2646\n",
      "2658\n",
      "2665\n",
      "2668\n",
      "2670\n",
      "2674\n",
      "2679\n",
      "2688\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2698\n",
      "2699\n",
      "2706\n",
      "2708\n",
      "2709\n",
      "2711\n",
      "2717\n",
      "2722\n",
      "2724\n",
      "2728\n",
      "2732\n",
      "2742\n",
      "2745\n",
      "2754\n",
      "2858\n",
      "2862\n",
      "2869\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2877\n",
      "2879\n",
      "2899\n",
      "2901\n",
      "2921\n",
      "2947\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2972\n",
      "2973\n",
      "3033\n",
      "3115\n",
      "3123\n",
      "3128\n",
      "3139\n",
      "3141\n",
      "3142\n",
      "3144\n",
      "3146\n",
      "3147\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3160\n",
      "3164\n",
      "3166\n",
      "3168\n",
      "3169\n",
      "3287\n",
      "3293\n",
      "3298\n",
      "3301\n",
      "3370\n",
      "3372\n",
      "3373\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3380\n",
      "3390\n",
      "3396\n",
      "3401\n",
      "3402\n",
      "3404\n",
      "3405\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3423\n",
      "3428\n",
      "3431\n",
      "3434\n",
      "3438\n",
      "3439\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3456\n",
      "3457\n",
      "3459\n",
      "3460\n",
      "3464\n",
      "3466\n",
      "3468\n",
      "3470\n",
      "3472\n",
      "3474\n",
      "3477\n",
      "3480\n",
      "3483\n",
      "3486\n",
      "3490\n",
      "3491\n",
      "3497\n",
      "3505\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3514\n",
      "3589\n",
      "3601\n",
      "3602\n",
      "3607\n",
      "3612\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3621\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3636\n",
      "3638\n",
      "3639\n",
      "3641\n",
      "3645\n",
      "3647\n",
      "3651\n",
      "3655\n",
      "3662\n",
      "3663\n",
      "3672\n",
      "3674\n",
      "3675\n",
      "3678\n",
      "3680\n",
      "3683\n",
      "3684\n",
      "3702\n",
      "3707\n",
      "3712\n",
      "3713\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3723\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3738\n",
      "3740\n",
      "3749\n",
      "3753\n",
      "3755\n",
      "3757\n",
      "3758\n",
      "3767\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3776\n",
      "3777\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3790\n",
      "3792\n",
      "3795\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3808\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3823\n",
      "3832\n",
      "3834\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3845\n",
      "3846\n",
      "3850\n",
      "3855\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3866\n",
      "3867\n",
      "3872\n",
      "3874\n",
      "3876\n",
      "3886\n",
      "3888\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3898\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3914\n",
      "3916\n",
      "3921\n",
      "3939\n",
      "3941\n",
      "3951\n",
      "3956\n",
      "3962\n",
      "3975\n",
      "3977\n",
      "4012\n",
      "4034\n",
      "4041\n",
      "4065\n",
      "4094\n",
      "4114\n",
      "4136\n",
      "4148\n",
      "4156\n",
      "4181\n",
      "4201\n",
      "4213\n",
      "4215\n",
      "4217\n",
      "4219\n",
      "4221\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4228\n",
      "4231\n",
      "4248\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4279\n",
      "4281\n",
      "4283\n",
      "4285\n",
      "4286\n",
      "4290\n",
      "4296\n",
      "4298\n",
      "4299\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4311\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4324\n",
      "4327\n",
      "4328\n",
      "4331\n",
      "4336\n",
      "4343\n",
      "4344\n",
      "4354\n",
      "4355\n",
      "4357\n",
      "4360\n",
      "4361\n",
      "4368\n",
      "4370\n",
      "4372\n",
      "4373\n",
      "4376\n",
      "4379\n",
      "4382\n",
      "4384\n",
      "4387\n",
      "4388\n",
      "4391\n",
      "4394\n",
      "4398\n",
      "4401\n",
      "4405\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4422\n",
      "4427\n",
      "4429\n",
      "4430\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4440\n",
      "4442\n",
      "4448\n",
      "4449\n",
      "4451\n",
      "4452\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4459\n",
      "4460\n",
      "4463\n",
      "4465\n",
      "4466\n",
      "4468\n",
      "4469\n",
      "4474\n",
      "4480\n",
      "4482\n",
      "4487\n",
      "4491\n",
      "4501\n",
      "4505\n",
      "4533\n",
      "4534\n",
      "4546\n",
      "4554\n",
      "4556\n",
      "4561\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4570\n",
      "4574\n",
      "4760\n",
      "4762\n",
      "4764\n",
      "4767\n",
      "4770\n",
      "4772\n",
      "4835\n",
      "4851\n",
      "4861\n",
      "4865\n",
      "4873\n",
      "4896\n",
      "4897\n",
      "4909\n",
      "4917\n",
      "4919\n",
      "4924\n",
      "4925\n",
      "4926\n",
      "4927\n",
      "4933\n",
      "4937\n",
      "5094\n",
      "5096\n",
      "5098\n",
      "5101\n",
      "5104\n",
      "5106\n",
      "5158\n",
      "5161\n",
      "5166\n",
      "5183\n",
      "5189\n",
      "5192\n",
      "5196\n",
      "5198\n",
      "5209\n",
      "5211\n",
      "5218\n",
      "5242\n",
      "5266\n",
      "5273\n",
      "5295\n",
      "5306\n",
      "5313\n",
      "5321\n",
      "5363\n",
      "5381\n",
      "5387\n",
      "5395\n",
      "5399\n",
      "5402\n",
      "5471\n",
      "5585\n",
      "5594\n",
      "5601\n",
      "5623\n",
      "5669\n",
      "5718\n",
      "5763\n",
      "5773\n",
      "5806\n",
      "5837\n",
      "5850\n",
      "5858\n",
      "5875\n",
      "5884\n",
      "5896\n",
      "5899\n",
      "5900\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5906\n",
      "5907\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5917\n",
      "5918\n",
      "5919\n",
      "5920\n",
      "5921\n",
      "5923\n",
      "5926\n",
      "5930\n",
      "5931\n",
      "5934\n",
      "5935\n",
      "5940\n",
      "5941\n",
      "5945\n",
      "5946\n",
      "5949\n",
      "5955\n",
      "5958\n",
      "5963\n",
      "5968\n",
      "5970\n",
      "5971\n",
      "5972\n",
      "5973\n",
      "5975\n",
      "5984\n",
      "5986\n",
      "5987\n",
      "5989\n",
      "5990\n",
      "5991\n",
      "5998\n",
      "5999\n",
      "6000\n",
      "6001\n",
      "6002\n",
      "6003\n",
      "6004\n",
      "6005\n",
      "6011\n",
      "6014\n",
      "6015\n",
      "6017\n",
      "6030\n",
      "6037\n",
      "6038\n",
      "6052\n",
      "6053\n",
      "6054\n",
      "6056\n",
      "6057\n",
      "6059\n",
      "6061\n",
      "6064\n",
      "6065\n",
      "6067\n",
      "6068\n",
      "6069\n",
      "6070\n",
      "6076\n",
      "6078\n",
      "6080\n",
      "6082\n",
      "6083\n",
      "6084\n",
      "6085\n",
      "6087\n",
      "6089\n",
      "6090\n",
      "6091\n",
      "6092\n",
      "6093\n",
      "6100\n",
      "6101\n",
      "6103\n",
      "6105\n",
      "6106\n",
      "6107\n",
      "6109\n",
      "6111\n",
      "6112\n",
      "6116\n",
      "6117\n",
      "6119\n",
      "6120\n",
      "6121\n",
      "6124\n",
      "6130\n",
      "6139\n",
      "6141\n",
      "6142\n",
      "6155\n",
      "6177\n",
      "6180\n",
      "6181\n",
      "6182\n",
      "6186\n",
      "6188\n",
      "6189\n",
      "6202\n",
      "6208\n",
      "6221\n",
      "6238\n",
      "6247\n",
      "6251\n",
      "6252\n",
      "6253\n",
      "6254\n",
      "6258\n",
      "6259\n",
      "6262\n",
      "6266\n",
      "6268\n",
      "6269\n",
      "6270\n",
      "6271\n",
      "6272\n",
      "6273\n",
      "6277\n",
      "6278\n",
      "6279\n",
      "6280\n",
      "6302\n",
      "6309\n",
      "6311\n",
      "6316\n",
      "6318\n",
      "6319\n",
      "6321\n",
      "6322\n",
      "6329\n",
      "6331\n",
      "6332\n",
      "6335\n",
      "6336\n",
      "6337\n",
      "6341\n",
      "6345\n",
      "6346\n",
      "6347\n",
      "6349\n",
      "6350\n",
      "6353\n",
      "6356\n",
      "6358\n",
      "6359\n",
      "6376\n",
      "6380\n",
      "6384\n",
      "6385\n",
      "6386\n",
      "6387\n",
      "6390\n",
      "6392\n",
      "6395\n",
      "6396\n",
      "6397\n",
      "6399\n",
      "6412\n",
      "6415\n",
      "6416\n",
      "6419\n",
      "6435\n",
      "6438\n",
      "6444\n",
      "6447\n",
      "6449\n",
      "6453\n",
      "6455\n",
      "6458\n",
      "6459\n",
      "6460\n",
      "6461\n",
      "6462\n",
      "6465\n",
      "6466\n",
      "6467\n",
      "6474\n",
      "6475\n",
      "6476\n",
      "6477\n",
      "6478\n",
      "6480\n",
      "6481\n",
      "6482\n",
      "6483\n",
      "6485\n",
      "6486\n",
      "6489\n",
      "6490\n",
      "6491\n",
      "6492\n",
      "6498\n",
      "6500\n",
      "6502\n",
      "6503\n",
      "6504\n",
      "6506\n",
      "6508\n",
      "6511\n",
      "6512\n",
      "6514\n",
      "6517\n",
      "6526\n",
      "6527\n",
      "6529\n",
      "6530\n",
      "6532\n",
      "6533\n",
      "6536\n",
      "6538\n",
      "6540\n",
      "6541\n",
      "6543\n",
      "6544\n",
      "6546\n",
      "6547\n",
      "6548\n",
      "6551\n",
      "6554\n",
      "6563\n",
      "6566\n",
      "6568\n",
      "6570\n",
      "6571\n",
      "6574\n",
      "6579\n",
      "6587\n",
      "6589\n",
      "6590\n",
      "6591\n",
      "6598\n",
      "6604\n",
      "6606\n",
      "6607\n",
      "6611\n",
      "6633\n",
      "6638\n",
      "6648\n",
      "6656\n",
      "6661\n",
      "6663\n",
      "6666\n",
      "6667\n",
      "6668\n",
      "6669\n",
      "6673\n",
      "6675\n",
      "6676\n",
      "6678\n",
      "6679\n",
      "6680\n",
      "6681\n",
      "6682\n",
      "6683\n",
      "6684\n",
      "6685\n",
      "6689\n",
      "6691\n",
      "6693\n",
      "6700\n",
      "6701\n",
      "6702\n",
      "6719\n",
      "6721\n",
      "6722\n",
      "6732\n",
      "6740\n",
      "6748\n",
      "6749\n",
      "6750\n",
      "6753\n",
      "6778\n",
      "6781\n",
      "6782\n",
      "6786\n",
      "6804\n",
      "6807\n",
      "6809\n",
      "6814\n",
      "6820\n",
      "6824\n",
      "6829\n",
      "6838\n",
      "6842\n",
      "6846\n",
      "6865\n",
      "6869\n",
      "6875\n",
      "6876\n",
      "6877\n",
      "6878\n",
      "6881\n",
      "6882\n",
      "6883\n",
      "6884\n",
      "6886\n",
      "6887\n",
      "6890\n",
      "6900\n",
      "6901\n",
      "6902\n",
      "6904\n",
      "6907\n",
      "6914\n",
      "6915\n",
      "6921\n",
      "6935\n",
      "6936\n",
      "6944\n",
      "6945\n",
      "6948\n",
      "6949\n",
      "6950\n",
      "6951\n",
      "6953\n",
      "6963\n",
      "6964\n",
      "6966\n",
      "6968\n",
      "6970\n",
      "6971\n",
      "6972\n",
      "6976\n",
      "6977\n",
      "6982\n",
      "6994\n",
      "6995\n",
      "6998\n",
      "6999\n",
      "7002\n",
      "7003\n",
      "7005\n",
      "7007\n",
      "7008\n",
      "7011\n",
      "7012\n",
      "7013\n",
      "7014\n",
      "7017\n",
      "7018\n",
      "7019\n",
      "7020\n",
      "7021\n",
      "7024\n",
      "7029\n",
      "7030\n",
      "7031\n",
      "7032\n",
      "7035\n",
      "7039\n",
      "7044\n",
      "7051\n",
      "7052\n",
      "7053\n",
      "7056\n",
      "7058\n",
      "7063\n",
      "7065\n",
      "7067\n",
      "7069\n",
      "7077\n",
      "7079\n",
      "7080\n",
      "7084\n",
      "7085\n",
      "7087\n",
      "7091\n",
      "7093\n",
      "7095\n",
      "7100\n",
      "7101\n",
      "7102\n",
      "7104\n",
      "7105\n",
      "7107\n",
      "7108\n",
      "7109\n",
      "7113\n",
      "7116\n",
      "7117\n",
      "7118\n",
      "7125\n",
      "7127\n",
      "7130\n",
      "7131\n",
      "7134\n",
      "7138\n",
      "7139\n",
      "7141\n",
      "7142\n",
      "7144\n",
      "7145\n",
      "7148\n",
      "7177\n",
      "7182\n",
      "7201\n",
      "7225\n",
      "7226\n",
      "7275\n",
      "7284\n",
      "7287\n",
      "7290\n",
      "7295\n",
      "7296\n",
      "7299\n",
      "7310\n",
      "7318\n",
      "7319\n",
      "7325\n",
      "7330\n",
      "7332\n",
      "7333\n",
      "7334\n",
      "7338\n",
      "7339\n",
      "7340\n",
      "7346\n",
      "7350\n",
      "7351\n",
      "7363\n",
      "7365\n",
      "7366\n",
      "7370\n",
      "7371\n",
      "7372\n",
      "7373\n",
      "7374\n",
      "7375\n",
      "7378\n",
      "7383\n",
      "7405\n",
      "7407\n",
      "7411\n",
      "7413\n",
      "7418\n",
      "7419\n",
      "7437\n",
      "7438\n",
      "7442\n",
      "7448\n",
      "7449\n",
      "7455\n",
      "7456\n",
      "7483\n",
      "7487\n",
      "7495\n",
      "7500\n",
      "7501\n",
      "7505\n",
      "7506\n",
      "7507\n",
      "7509\n",
      "7513\n",
      "7515\n",
      "7526\n",
      "7535\n",
      "7539\n",
      "7550\n",
      "7556\n",
      "7566\n",
      "7569\n",
      "7572\n",
      "7574\n",
      "7575\n",
      "7576\n",
      "7580\n",
      "7581\n",
      "7583\n",
      "7584\n",
      "7587\n",
      "7590\n",
      "7591\n",
      "7609\n",
      "7612\n",
      "7613\n",
      "7623\n",
      "7629\n",
      "7635\n",
      "7636\n",
      "7639\n",
      "7641\n",
      "7644\n",
      "7645\n",
      "7646\n",
      "7647\n",
      "7649\n",
      "7653\n",
      "7654\n",
      "7656\n",
      "7657\n",
      "7658\n",
      "7664\n",
      "7669\n",
      "7671\n",
      "7673\n",
      "7676\n",
      "7677\n",
      "7678\n",
      "7680\n",
      "7681\n",
      "7682\n",
      "7683\n",
      "7692\n",
      "7693\n",
      "7694\n",
      "7695\n",
      "7705\n",
      "7708\n",
      "7710\n",
      "7712\n",
      "7714\n",
      "7715\n",
      "7716\n",
      "7718\n",
      "7719\n",
      "7726\n",
      "7727\n",
      "7728\n",
      "7729\n",
      "7730\n",
      "7736\n",
      "7739\n",
      "7740\n",
      "7744\n",
      "7745\n",
      "7746\n",
      "7748\n",
      "7754\n",
      "7755\n",
      "7756\n",
      "7757\n",
      "7759\n",
      "7760\n",
      "7761\n",
      "7763\n",
      "7764\n",
      "7765\n",
      "7767\n",
      "7770\n",
      "7772\n",
      "7773\n",
      "7775\n",
      "7776\n",
      "7777\n",
      "7785\n",
      "7787\n",
      "7789\n",
      "7791\n",
      "7792\n",
      "7793\n",
      "7794\n",
      "7796\n",
      "7797\n",
      "7798\n",
      "7800\n",
      "7801\n",
      "7803\n",
      "7804\n",
      "7805\n",
      "7806\n",
      "7807\n",
      "7809\n",
      "7810\n",
      "7811\n",
      "7814\n",
      "7815\n",
      "7816\n",
      "7817\n",
      "7818\n",
      "7819\n",
      "7822\n",
      "7823\n",
      "7824\n",
      "7825\n",
      "7826\n",
      "7827\n",
      "7830\n",
      "7832\n",
      "7833\n",
      "7834\n",
      "7842\n",
      "7844\n",
      "7848\n",
      "7849\n",
      "7857\n",
      "7862\n",
      "7870\n",
      "7880\n",
      "7917\n",
      "7925\n",
      "7927\n",
      "7932\n",
      "7936\n",
      "7940\n",
      "7941\n",
      "7944\n",
      "7949\n",
      "7960\n",
      "7962\n",
      "7965\n",
      "7967\n",
      "7968\n",
      "7971\n",
      "7972\n",
      "7973\n",
      "7974\n",
      "7978\n",
      "7983\n",
      "7985\n",
      "7988\n",
      "7996\n",
      "7999\n",
      "8005\n",
      "8008\n",
      "8010\n",
      "8013\n",
      "8016\n",
      "8026\n",
      "8029\n",
      "8030\n",
      "8033\n",
      "8035\n",
      "8036\n",
      "8038\n",
      "8039\n",
      "8042\n",
      "8046\n",
      "8048\n",
      "8050\n",
      "8052\n",
      "8054\n",
      "8055\n",
      "8056\n",
      "8058\n",
      "8059\n",
      "8062\n",
      "8071\n",
      "8073\n",
      "8077\n",
      "8080\n",
      "8082\n",
      "8083\n",
      "8087\n",
      "8089\n",
      "8090\n",
      "8096\n",
      "8097\n",
      "8099\n",
      "8124\n",
      "8158\n",
      "8183\n",
      "8184\n",
      "8229\n",
      "8238\n",
      "8239\n",
      "8241\n",
      "8243\n",
      "8257\n",
      "8263\n",
      "8265\n",
      "8291\n",
      "8304\n",
      "8317\n",
      "8319\n",
      "8366\n",
      "8369\n",
      "8371\n",
      "8372\n",
      "8374\n",
      "8380\n",
      "8382\n",
      "8385\n",
      "8408\n",
      "8435\n",
      "8440\n",
      "8446\n",
      "8452\n",
      "8453\n",
      "8455\n",
      "8467\n",
      "8469\n",
      "8471\n",
      "8479\n",
      "8480\n",
      "8490\n",
      "8497\n",
      "8498\n",
      "8501\n",
      "8503\n",
      "8505\n",
      "8506\n",
      "8507\n",
      "8514\n",
      "8516\n",
      "8522\n",
      "8529\n",
      "8543\n",
      "8544\n",
      "8545\n",
      "8548\n",
      "8549\n",
      "8551\n",
      "8552\n",
      "8553\n",
      "8554\n",
      "8555\n",
      "8559\n",
      "8564\n",
      "8617\n",
      "8621\n",
      "8629\n",
      "8630\n",
      "8644\n",
      "8670\n",
      "8672\n",
      "8673\n",
      "8678\n",
      "8708\n",
      "8709\n",
      "8710\n",
      "8713\n",
      "8714\n",
      "8715\n",
      "8717\n",
      "8718\n",
      "8719\n",
      "8720\n",
      "8721\n",
      "8722\n",
      "8723\n",
      "8724\n",
      "8725\n",
      "8726\n",
      "8727\n",
      "8730\n",
      "8733\n",
      "8735\n",
      "8737\n",
      "8739\n",
      "8740\n",
      "8743\n",
      "8755\n",
      "8758\n",
      "8759\n",
      "8760\n",
      "8761\n",
      "8762\n",
      "8764\n",
      "8766\n",
      "8768\n",
      "8770\n",
      "8771\n",
      "8772\n",
      "8773\n",
      "8774\n",
      "8775\n",
      "8776\n",
      "8778\n",
      "8779\n",
      "8780\n",
      "8781\n",
      "8782\n",
      "8783\n",
      "8784\n",
      "8788\n",
      "8790\n",
      "8791\n",
      "8792\n",
      "8793\n",
      "8794\n",
      "8795\n",
      "8796\n",
      "8798\n",
      "8799\n",
      "8804\n",
      "8805\n",
      "8819\n",
      "8825\n",
      "8831\n",
      "8833\n",
      "8835\n",
      "8836\n",
      "8837\n",
      "8838\n",
      "8839\n",
      "8840\n",
      "8841\n",
      "8843\n",
      "8846\n",
      "8854\n",
      "8855\n",
      "8859\n",
      "8869\n",
      "8872\n",
      "8874\n",
      "8875\n",
      "8876\n",
      "8877\n",
      "8879\n",
      "8882\n",
      "8884\n",
      "8886\n",
      "8888\n",
      "8898\n",
      "8910\n",
      "8923\n",
      "8924\n",
      "8926\n",
      "8938\n",
      "8949\n",
      "8953\n",
      "8954\n",
      "8969\n",
      "8972\n",
      "8973\n",
      "8997\n",
      "8998\n",
      "9004\n",
      "9006\n",
      "9052\n",
      "9121\n",
      "9122\n",
      "9125\n",
      "9126\n",
      "9127\n",
      "9128\n",
      "9133\n",
      "9134\n",
      "9135\n",
      "9136\n",
      "9137\n",
      "9138\n",
      "9141\n",
      "9145\n",
      "9147\n",
      "9149\n",
      "9155\n",
      "9167\n",
      "9173\n",
      "9175\n",
      "9176\n",
      "9178\n",
      "9180\n",
      "9187\n",
      "9189\n",
      "9196\n",
      "9200\n",
      "9203\n",
      "9205\n",
      "9208\n",
      "9214\n",
      "9217\n",
      "9225\n",
      "9243\n",
      "9247\n",
      "9248\n",
      "9251\n",
      "9252\n",
      "9254\n",
      "9256\n",
      "9257\n",
      "9258\n",
      "9260\n",
      "9261\n",
      "9269\n",
      "9270\n",
      "9271\n",
      "9273\n",
      "9274\n",
      "9275\n",
      "9283\n",
      "9285\n",
      "9286\n",
      "9287\n",
      "9288\n",
      "9290\n",
      "9291\n",
      "9293\n",
      "9297\n",
      "9301\n",
      "9306\n",
      "9309\n",
      "9310\n",
      "9313\n",
      "9314\n",
      "9315\n",
      "9316\n",
      "9321\n",
      "9326\n",
      "9334\n",
      "9335\n",
      "9338\n",
      "9339\n",
      "9340\n",
      "9341\n",
      "9342\n",
      "9343\n",
      "9344\n",
      "9346\n",
      "9348\n",
      "9349\n",
      "9351\n",
      "9352\n",
      "9355\n",
      "9357\n",
      "9360\n",
      "9363\n",
      "9365\n",
      "9367\n",
      "9368\n",
      "9369\n",
      "9370\n",
      "9372\n",
      "9377\n",
      "9380\n",
      "9383\n",
      "9384\n",
      "9385\n",
      "9386\n",
      "9388\n",
      "9389\n",
      "9390\n",
      "9391\n",
      "9395\n",
      "9404\n",
      "9405\n",
      "9406\n",
      "9408\n",
      "9409\n",
      "9410\n",
      "9411\n",
      "9452\n",
      "9453\n",
      "9454\n",
      "9455\n",
      "9473\n",
      "9477\n",
      "9483\n",
      "9505\n",
      "9506\n",
      "9509\n",
      "9512\n",
      "9513\n",
      "9516\n",
      "9520\n",
      "9523\n",
      "9548\n",
      "9551\n",
      "9552\n",
      "9558\n",
      "9560\n",
      "9563\n",
      "9564\n",
      "9571\n",
      "9572\n",
      "9579\n",
      "9582\n",
      "9584\n",
      "9588\n",
      "9593\n",
      "9624\n",
      "9767\n",
      "9768\n",
      "9772\n",
      "9784\n",
      "9790\n",
      "9795\n",
      "9827\n",
      "9828\n",
      "9833\n",
      "9836\n",
      "9839\n",
      "9840\n",
      "9841\n",
      "9844\n",
      "9848\n",
      "9851\n",
      "9856\n",
      "9858\n",
      "9859\n",
      "9862\n",
      "9863\n",
      "9864\n",
      "9867\n",
      "9877\n",
      "9879\n",
      "9882\n",
      "9883\n",
      "9887\n",
      "9889\n",
      "9897\n",
      "9898\n",
      "9908\n",
      "9910\n",
      "9911\n",
      "9914\n",
      "9916\n",
      "9917\n",
      "9920\n",
      "9921\n",
      "9927\n",
      "9949\n",
      "9955\n",
      "9962\n",
      "9968\n",
      "9973\n",
      "9978\n",
      "9996\n",
      "9998\n",
      "9999\n",
      "10018\n",
      "10032\n",
      "10033\n",
      "10036\n",
      "10037\n",
      "10081\n",
      "10088\n",
      "10098\n",
      "10151\n",
      "10153\n",
      "10162\n",
      "10165\n",
      "10166\n",
      "10167\n",
      "10170\n",
      "10172\n",
      "10173\n",
      "10177\n",
      "10188\n",
      "10194\n",
      "10196\n",
      "10197\n",
      "10211\n",
      "10216\n",
      "10223\n",
      "10225\n",
      "10226\n",
      "10228\n",
      "10231\n",
      "10233\n",
      "10257\n",
      "10259\n",
      "10269\n",
      "10270\n",
      "10275\n",
      "10276\n",
      "10281\n",
      "10283\n",
      "10297\n",
      "10300\n",
      "10301\n",
      "10325\n",
      "10343\n",
      "10357\n",
      "10360\n",
      "10377\n",
      "10386\n",
      "10396\n",
      "10403\n",
      "10404\n",
      "10417\n",
      "10425\n",
      "10435\n",
      "10436\n",
      "10440\n",
      "10456\n",
      "10480\n",
      "10481\n",
      "10482\n",
      "10486\n",
      "10488\n",
      "10493\n",
      "10495\n",
      "10497\n",
      "10499\n",
      "10500\n",
      "10504\n",
      "10505\n",
      "10508\n",
      "10509\n",
      "10510\n",
      "10512\n",
      "10513\n",
      "10515\n",
      "10517\n",
      "10518\n",
      "10519\n",
      "10521\n",
      "10525\n",
      "10528\n",
      "10529\n",
      "10531\n",
      "10532\n",
      "10533\n",
      "10541\n",
      "10547\n",
      "10548\n",
      "10550\n",
      "10554\n",
      "10557\n",
      "10562\n",
      "10563\n",
      "10565\n",
      "10566\n",
      "10572\n",
      "10573\n",
      "10574\n",
      "10575\n",
      "10576\n",
      "10578\n",
      "10580\n",
      "10581\n",
      "10583\n",
      "10591\n",
      "10593\n",
      "10594\n",
      "10595\n",
      "10604\n",
      "10608\n",
      "10611\n",
      "10612\n",
      "10615\n",
      "10623\n",
      "10627\n",
      "10628\n",
      "10630\n",
      "10632\n",
      "10642\n",
      "10665\n",
      "10670\n",
      "10671\n",
      "10807\n",
      "10808\n",
      "10810\n",
      "10813\n",
      "10821\n",
      "10822\n",
      "10825\n",
      "10828\n",
      "10829\n",
      "10831\n",
      "10833\n",
      "10835\n",
      "10836\n",
      "10839\n",
      "10842\n",
      "10852\n",
      "10854\n",
      "10855\n",
      "10867\n",
      "10877\n",
      "10880\n",
      "10887\n",
      "10889\n",
      "10894\n",
      "10895\n",
      "10903\n",
      "10906\n",
      "10908\n",
      "10909\n",
      "10911\n",
      "10912\n",
      "10913\n",
      "10916\n",
      "10917\n",
      "10919\n",
      "10922\n",
      "10923\n",
      "10924\n",
      "10932\n",
      "10933\n",
      "10934\n",
      "10935\n",
      "10945\n",
      "10946\n",
      "10948\n",
      "10949\n",
      "10952\n",
      "10953\n",
      "10958\n",
      "10959\n",
      "10963\n",
      "10965\n",
      "10968\n",
      "10969\n",
      "10970\n",
      "10971\n",
      "10973\n",
      "10974\n",
      "10976\n",
      "10986\n",
      "10992\n",
      "10993\n",
      "10995\n",
      "10996\n",
      "11002\n",
      "11003\n",
      "11004\n",
      "11012\n",
      "11014\n",
      "11015\n",
      "11016\n",
      "11018\n",
      "11019\n",
      "11021\n",
      "11022\n",
      "11028\n",
      "11030\n",
      "11047\n",
      "11052\n",
      "11053\n",
      "11062\n",
      "11063\n",
      "11065\n",
      "11067\n",
      "11068\n",
      "11069\n",
      "11070\n",
      "11076\n",
      "11077\n",
      "11078\n",
      "11084\n",
      "11088\n",
      "11089\n",
      "11090\n",
      "11093\n",
      "11098\n",
      "11103\n",
      "11104\n",
      "11105\n",
      "11111\n",
      "11116\n",
      "11128\n",
      "11152\n",
      "11157\n",
      "11159\n",
      "11170\n",
      "11200\n",
      "11213\n",
      "11220\n",
      "11222\n",
      "11226\n",
      "11227\n",
      "11232\n",
      "11234\n",
      "11235\n",
      "11236\n",
      "11237\n",
      "11238\n",
      "11248\n",
      "11249\n",
      "11266\n",
      "11268\n",
      "11274\n",
      "11282\n",
      "11306\n",
      "11315\n",
      "11319\n",
      "11323\n",
      "11328\n",
      "11337\n",
      "11344\n",
      "11348\n",
      "11349\n",
      "11350\n",
      "11364\n",
      "11366\n",
      "11368\n",
      "11386\n",
      "11409\n",
      "11458\n",
      "11468\n",
      "11475\n",
      "11476\n",
      "11488\n",
      "11494\n",
      "11498\n",
      "11508\n",
      "11510\n",
      "11511\n",
      "11512\n",
      "11514\n",
      "11515\n",
      "11527\n",
      "11535\n",
      "11538\n",
      "11543\n",
      "11546\n",
      "11549\n",
      "11550\n",
      "11552\n",
      "11553\n",
      "11556\n",
      "11572\n",
      "11573\n",
      "11581\n",
      "11582\n",
      "11596\n",
      "11604\n",
      "11617\n",
      "11618\n",
      "11635\n",
      "11636\n",
      "11639\n",
      "11641\n",
      "11642\n",
      "11643\n",
      "11656\n",
      "11660\n",
      "11661\n",
      "11662\n",
      "11664\n",
      "11673\n",
      "11697\n",
      "11701\n",
      "11703\n",
      "11711\n",
      "11712\n",
      "11713\n",
      "11715\n",
      "11716\n",
      "11717\n",
      "11759\n",
      "11762\n",
      "11763\n",
      "11765\n",
      "11767\n",
      "11768\n",
      "11769\n",
      "11770\n",
      "11774\n",
      "11775\n",
      "11777\n",
      "11779\n",
      "11782\n",
      "11783\n",
      "11786\n",
      "11793\n",
      "11799\n",
      "11803\n",
      "11804\n",
      "11806\n",
      "11808\n",
      "11809\n",
      "11810\n",
      "11812\n",
      "11819\n",
      "11821\n",
      "11839\n",
      "11848\n",
      "11850\n",
      "11851\n",
      "11854\n",
      "11856\n",
      "11859\n",
      "11866\n",
      "11880\n",
      "11882\n",
      "11884\n",
      "11885\n",
      "11886\n",
      "11888\n",
      "11889\n",
      "11890\n",
      "11891\n",
      "11892\n",
      "11893\n",
      "11894\n",
      "11895\n",
      "11896\n",
      "11897\n",
      "11898\n",
      "11899\n",
      "11900\n",
      "11901\n",
      "11902\n",
      "11908\n",
      "11910\n",
      "11911\n",
      "11915\n",
      "11921\n",
      "11925\n",
      "11930\n",
      "11933\n",
      "11934\n",
      "11937\n",
      "11941\n",
      "11942\n",
      "11947\n",
      "11951\n",
      "11956\n",
      "11958\n",
      "11962\n",
      "11968\n",
      "11970\n",
      "11972\n",
      "11974\n",
      "11976\n",
      "11978\n",
      "11979\n",
      "11980\n",
      "11981\n",
      "11982\n",
      "11983\n",
      "11984\n",
      "11985\n",
      "11986\n",
      "11987\n",
      "11989\n",
      "11991\n",
      "12000\n",
      "12001\n",
      "12002\n",
      "12004\n",
      "12005\n",
      "12007\n",
      "12008\n",
      "12009\n",
      "12012\n",
      "12015\n",
      "12019\n",
      "12020\n",
      "12021\n",
      "12022\n",
      "12024\n",
      "12057\n",
      "12115\n",
      "12142\n",
      "12144\n",
      "12150\n",
      "12152\n",
      "12153\n",
      "12154\n",
      "12155\n",
      "12156\n",
      "12161\n",
      "12166\n",
      "12171\n",
      "12172\n",
      "12174\n",
      "12175\n",
      "12178\n",
      "12181\n",
      "12183\n",
      "12184\n",
      "12187\n",
      "12188\n",
      "12190\n",
      "12191\n",
      "12193\n",
      "12194\n",
      "12210\n",
      "12212\n",
      "12218\n",
      "12220\n",
      "12221\n",
      "12223\n",
      "12227\n",
      "12228\n",
      "12230\n",
      "12231\n",
      "12236\n",
      "12238\n",
      "12247\n",
      "12250\n",
      "12253\n",
      "12258\n",
      "12264\n",
      "12265\n",
      "12267\n",
      "12271\n",
      "12279\n",
      "12294\n",
      "12295\n",
      "12296\n",
      "12304\n",
      "12305\n",
      "12308\n",
      "12312\n",
      "12328\n",
      "12337\n",
      "12359\n",
      "12363\n",
      "12365\n",
      "12370\n",
      "12376\n",
      "12398\n",
      "12404\n",
      "12407\n",
      "12408\n",
      "12409\n",
      "12411\n",
      "12412\n",
      "12413\n",
      "12414\n",
      "12415\n",
      "12416\n",
      "12419\n",
      "12428\n",
      "12429\n",
      "12430\n",
      "12431\n",
      "12432\n",
      "12436\n",
      "12437\n",
      "12441\n",
      "12443\n",
      "12447\n",
      "12470\n",
      "12474\n",
      "12475\n",
      "12478\n",
      "12484\n",
      "12488\n",
      "12498\n",
      "12507\n",
      "12518\n",
      "12521\n",
      "12523\n",
      "12524\n",
      "12548\n",
      "12550\n",
      "12558\n",
      "12562\n",
      "12565\n",
      "12567\n",
      "12568\n",
      "12575\n",
      "12577\n",
      "12596\n",
      "12607\n",
      "12610\n",
      "12636\n",
      "12648\n",
      "12661\n",
      "12662\n",
      "12664\n",
      "12665\n",
      "12690\n",
      "12692\n",
      "12695\n",
      "12717\n",
      "12723\n",
      "12726\n",
      "12727\n",
      "12742\n",
      "12748\n",
      "12749\n",
      "12750\n",
      "12758\n",
      "12775\n",
      "12776\n",
      "12777\n",
      "12779\n",
      "12780\n",
      "12785\n",
      "12789\n",
      "12800\n",
      "12805\n",
      "12817\n",
      "12818\n",
      "12820\n",
      "12831\n",
      "12832\n",
      "12845\n",
      "12847\n",
      "12853\n",
      "12854\n",
      "12856\n",
      "12863\n",
      "12874\n",
      "12875\n",
      "12876\n",
      "12878\n",
      "12882\n",
      "12886\n",
      "12889\n",
      "12890\n",
      "12891\n",
      "12895\n",
      "12899\n",
      "12900\n",
      "12901\n",
      "12902\n",
      "12910\n",
      "12911\n",
      "12912\n",
      "12913\n",
      "12916\n",
      "12924\n",
      "12932\n",
      "12936\n",
      "12937\n",
      "12938\n",
      "12939\n",
      "12940\n",
      "12942\n",
      "12943\n",
      "12944\n",
      "12945\n",
      "12947\n",
      "12948\n",
      "12950\n",
      "12951\n",
      "12954\n",
      "12958\n",
      "12960\n",
      "12961\n",
      "12962\n",
      "12963\n",
      "12970\n",
      "12972\n",
      "12975\n",
      "13003\n",
      "13007\n",
      "13013\n",
      "13014\n",
      "13019\n",
      "13040\n",
      "13041\n",
      "13045\n",
      "13052\n",
      "13055\n",
      "13103\n",
      "13104\n",
      "13106\n",
      "13171\n",
      "13173\n",
      "13175\n",
      "13177\n",
      "13181\n",
      "13183\n",
      "13184\n",
      "13185\n",
      "13188\n",
      "13189\n",
      "13190\n",
      "13191\n",
      "13196\n",
      "13198\n",
      "13199\n",
      "13203\n",
      "13223\n",
      "13224\n",
      "13225\n",
      "13226\n",
      "13227\n",
      "13228\n",
      "13231\n",
      "13239\n",
      "13241\n",
      "13242\n",
      "13245\n",
      "13246\n",
      "13248\n",
      "13249\n",
      "13250\n",
      "13251\n",
      "13256\n",
      "13259\n",
      "13260\n",
      "13262\n",
      "13267\n",
      "13268\n",
      "13269\n",
      "13270\n",
      "13272\n",
      "13273\n",
      "13275\n",
      "13278\n",
      "13279\n",
      "13281\n",
      "13283\n",
      "13285\n",
      "13287\n",
      "13289\n",
      "13291\n",
      "13292\n",
      "13297\n",
      "13298\n",
      "13299\n",
      "13301\n",
      "13302\n",
      "13303\n",
      "13304\n",
      "13306\n",
      "13307\n",
      "13308\n",
      "13317\n",
      "13320\n",
      "13323\n",
      "13325\n",
      "13333\n",
      "13336\n",
      "13338\n",
      "13345\n",
      "13349\n",
      "13352\n",
      "13353\n",
      "13355\n",
      "13356\n",
      "13357\n",
      "13362\n",
      "13364\n",
      "13365\n",
      "13366\n",
      "13368\n",
      "13372\n",
      "13378\n",
      "13379\n",
      "13380\n",
      "13387\n",
      "13389\n",
      "13421\n",
      "13426\n",
      "13427\n",
      "13455\n",
      "13488\n",
      "13516\n",
      "13522\n",
      "13534\n",
      "13562\n",
      "13596\n",
      "13597\n",
      "13599\n",
      "13600\n",
      "13603\n",
      "13605\n",
      "13606\n",
      "13608\n",
      "13615\n",
      "13618\n",
      "13629\n",
      "13634\n",
      "13635\n",
      "13638\n",
      "13641\n",
      "13644\n",
      "13645\n",
      "13646\n",
      "13647\n",
      "13659\n",
      "13661\n",
      "13665\n",
      "13671\n",
      "13672\n",
      "13676\n",
      "13678\n",
      "13679\n",
      "13680\n",
      "13681\n",
      "13682\n",
      "13689\n",
      "13691\n",
      "13733\n",
      "13757\n",
      "13759\n",
      "13772\n",
      "13774\n",
      "13775\n",
      "13779\n",
      "13780\n",
      "13791\n",
      "13792\n",
      "13793\n",
      "13795\n",
      "13796\n",
      "13799\n",
      "13800\n",
      "13805\n",
      "13806\n",
      "13807\n",
      "13810\n",
      "13811\n",
      "13812\n",
      "13813\n",
      "13818\n",
      "13819\n",
      "13822\n",
      "13835\n",
      "13836\n",
      "13840\n",
      "13841\n",
      "13842\n",
      "13843\n",
      "13847\n",
      "13851\n",
      "13853\n",
      "13854\n",
      "13857\n",
      "13859\n",
      "13863\n",
      "13864\n",
      "13866\n",
      "13867\n",
      "13868\n",
      "13870\n",
      "13871\n",
      "13882\n",
      "13886\n",
      "13887\n",
      "13894\n",
      "13898\n",
      "13900\n",
      "13901\n",
      "13902\n",
      "13903\n",
      "13906\n",
      "13907\n",
      "13908\n",
      "13909\n",
      "13910\n",
      "13911\n",
      "13916\n",
      "13917\n",
      "13918\n",
      "13920\n",
      "13923\n",
      "13924\n",
      "13929\n",
      "13931\n",
      "13933\n",
      "13936\n",
      "13937\n",
      "13938\n",
      "13939\n",
      "13947\n",
      "13949\n",
      "13953\n",
      "13957\n",
      "13966\n",
      "13975\n",
      "13980\n",
      "13981\n",
      "13982\n",
      "13985\n",
      "13990\n",
      "13991\n",
      "13992\n",
      "13995\n",
      "13996\n",
      "13997\n",
      "13999\n",
      "14000\n",
      "14002\n",
      "14003\n",
      "14005\n",
      "14006\n",
      "14016\n",
      "14019\n",
      "14020\n",
      "14023\n",
      "14024\n",
      "14025\n",
      "14026\n",
      "14027\n",
      "14029\n",
      "14030\n",
      "14031\n",
      "14035\n",
      "14036\n",
      "14038\n",
      "14041\n",
      "14043\n",
      "14044\n",
      "14046\n",
      "14047\n",
      "14048\n",
      "14051\n",
      "14052\n",
      "14054\n",
      "14057\n",
      "14060\n",
      "14061\n",
      "14064\n",
      "14065\n",
      "14068\n",
      "14069\n",
      "14071\n",
      "14072\n",
      "14073\n",
      "14077\n",
      "14078\n",
      "14079\n",
      "14081\n",
      "14083\n",
      "14084\n",
      "14085\n",
      "14088\n",
      "14092\n",
      "14094\n",
      "14095\n",
      "14096\n",
      "14101\n",
      "14103\n",
      "14106\n",
      "14118\n",
      "14120\n",
      "14121\n",
      "14122\n",
      "14125\n",
      "14126\n",
      "14128\n",
      "14130\n",
      "14132\n",
      "14133\n",
      "14135\n",
      "14138\n",
      "14140\n",
      "14141\n",
      "14142\n",
      "14151\n",
      "14157\n",
      "14163\n",
      "14165\n",
      "14167\n",
      "14168\n",
      "14169\n",
      "14173\n",
      "14177\n",
      "14178\n",
      "14179\n",
      "14182\n",
      "14186\n",
      "14187\n",
      "14188\n",
      "14189\n",
      "14192\n",
      "14193\n",
      "14194\n",
      "14197\n",
      "14199\n",
      "14206\n",
      "14207\n",
      "14208\n",
      "14209\n",
      "14210\n",
      "14220\n",
      "14221\n",
      "14222\n",
      "14258\n",
      "14259\n",
      "14264\n",
      "14271\n",
      "14272\n",
      "14286\n",
      "14290\n",
      "14308\n",
      "14317\n",
      "14326\n",
      "14383\n",
      "14384\n",
      "14389\n",
      "14392\n",
      "14393\n",
      "14395\n",
      "14399\n",
      "14401\n",
      "14416\n",
      "14417\n",
      "14455\n",
      "14458\n",
      "14468\n",
      "14471\n",
      "14524\n",
      "14525\n",
      "14528\n",
      "14530\n",
      "14535\n",
      "14536\n",
      "14539\n",
      "14545\n",
      "14546\n",
      "14547\n",
      "14549\n",
      "14550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14565\n",
      "14576\n",
      "14581\n",
      "14584\n",
      "14588\n",
      "14589\n",
      "14596\n",
      "14602\n",
      "14605\n",
      "14610\n",
      "14611\n",
      "14613\n",
      "14617\n",
      "14618\n",
      "14619\n",
      "14620\n",
      "14626\n",
      "14628\n",
      "14630\n",
      "14657\n",
      "14665\n",
      "14666\n",
      "14669\n",
      "14673\n",
      "14674\n",
      "14678\n",
      "14684\n",
      "14685\n",
      "14688\n",
      "14692\n",
      "14700\n",
      "14705\n",
      "14707\n",
      "14713\n",
      "14717\n",
      "14718\n",
      "14719\n",
      "14726\n",
      "14727\n",
      "14729\n",
      "14733\n",
      "14734\n",
      "14736\n",
      "14737\n",
      "14738\n",
      "14739\n",
      "14748\n",
      "14752\n",
      "14757\n",
      "14761\n",
      "14764\n",
      "14768\n",
      "14773\n",
      "14780\n",
      "14787\n",
      "14789\n",
      "14797\n",
      "14798\n",
      "14799\n",
      "14806\n",
      "14812\n",
      "14813\n",
      "14824\n",
      "14825\n",
      "14826\n",
      "14846\n",
      "14848\n",
      "14849\n",
      "14851\n",
      "14856\n",
      "14871\n",
      "14875\n",
      "14876\n",
      "14877\n",
      "14881\n",
      "14882\n",
      "14883\n",
      "14885\n",
      "14896\n",
      "14911\n",
      "14912\n",
      "14915\n",
      "14918\n",
      "14920\n",
      "14922\n",
      "14951\n",
      "14961\n",
      "14969\n",
      "14972\n",
      "14982\n",
      "14983\n",
      "14985\n",
      "15000\n",
      "15002\n",
      "15006\n",
      "15018\n",
      "15040\n",
      "15057\n",
      "15079\n",
      "15124\n",
      "15172\n",
      "15173\n",
      "15174\n",
      "15177\n",
      "15179\n",
      "15181\n",
      "15185\n",
      "15186\n",
      "15187\n",
      "15188\n",
      "15200\n",
      "15207\n",
      "15235\n",
      "15237\n",
      "15239\n",
      "15251\n",
      "15254\n",
      "15262\n",
      "15263\n",
      "15264\n",
      "15267\n",
      "15269\n",
      "15270\n",
      "15272\n",
      "15281\n",
      "15283\n",
      "15286\n",
      "15288\n",
      "15289\n",
      "15295\n",
      "15296\n",
      "15299\n",
      "15302\n",
      "15309\n",
      "15317\n",
      "15370\n",
      "15382\n",
      "15389\n",
      "15394\n",
      "15417\n",
      "15421\n",
      "15424\n",
      "15427\n",
      "15432\n",
      "15441\n",
      "15446\n",
      "15449\n",
      "15457\n",
      "15611\n",
      "15614\n",
      "15615\n",
      "15616\n",
      "15617\n",
      "15621\n",
      "15625\n",
      "15627\n",
      "15644\n",
      "15656\n",
      "15663\n",
      "15664\n",
      "15666\n",
      "15668\n",
      "15680\n",
      "15682\n",
      "15683\n",
      "15687\n",
      "15689\n",
      "15691\n",
      "15692\n",
      "15694\n",
      "15697\n",
      "15704\n",
      "15706\n",
      "15707\n",
      "15711\n",
      "15713\n",
      "15714\n",
      "15738\n",
      "15741\n",
      "15742\n",
      "15744\n",
      "15745\n",
      "15746\n",
      "15747\n",
      "15749\n",
      "15753\n",
      "15756\n",
      "15760\n",
      "15763\n",
      "15764\n",
      "15765\n",
      "15767\n",
      "15769\n",
      "15770\n",
      "15771\n",
      "15773\n",
      "15774\n",
      "15775\n",
      "15785\n",
      "15790\n",
      "15795\n",
      "15800\n",
      "15801\n",
      "15805\n",
      "15809\n",
      "15812\n",
      "15813\n",
      "15817\n",
      "15818\n",
      "15819\n",
      "15820\n",
      "15821\n",
      "15822\n",
      "15830\n",
      "15831\n",
      "15832\n",
      "15833\n",
      "15834\n",
      "15835\n",
      "15836\n",
      "15837\n",
      "15838\n",
      "15841\n",
      "15844\n",
      "15846\n",
      "15847\n",
      "15849\n",
      "15851\n",
      "15852\n",
      "15853\n",
      "15854\n",
      "15856\n",
      "15857\n",
      "15858\n",
      "15859\n",
      "15860\n",
      "15861\n",
      "15863\n",
      "15864\n",
      "15865\n",
      "15868\n",
      "15869\n",
      "15873\n",
      "15876\n",
      "15880\n",
      "15886\n",
      "15896\n",
      "15899\n",
      "15906\n",
      "15910\n",
      "15922\n",
      "15926\n",
      "15937\n",
      "15940\n",
      "15941\n",
      "15944\n",
      "15945\n",
      "15951\n",
      "15952\n",
      "15953\n",
      "15954\n",
      "15955\n",
      "15956\n",
      "15960\n",
      "15997\n",
      "16000\n",
      "16003\n",
      "16004\n",
      "16005\n",
      "16007\n",
      "16012\n",
      "16023\n",
      "16028\n",
      "16031\n",
      "16035\n",
      "16036\n",
      "16038\n",
      "16043\n",
      "16044\n",
      "16045\n",
      "16047\n",
      "16048\n",
      "16049\n",
      "16050\n",
      "16059\n",
      "16063\n",
      "16065\n",
      "16066\n",
      "16067\n",
      "16069\n",
      "16070\n",
      "16075\n",
      "16076\n",
      "16077\n",
      "16078\n",
      "16081\n",
      "16082\n",
      "16083\n",
      "16084\n",
      "16089\n",
      "16090\n",
      "16091\n",
      "16092\n",
      "16096\n",
      "16097\n",
      "16101\n",
      "16106\n",
      "16107\n",
      "16111\n",
      "16118\n",
      "16119\n",
      "16121\n",
      "16128\n",
      "16129\n",
      "16130\n",
      "16131\n",
      "16133\n",
      "16136\n",
      "16137\n",
      "16141\n",
      "16144\n",
      "16146\n",
      "16153\n",
      "16157\n",
      "16161\n",
      "16162\n",
      "16164\n",
      "16166\n",
      "16167\n",
      "16170\n",
      "16173\n",
      "16175\n",
      "16176\n",
      "16182\n",
      "16184\n",
      "16186\n",
      "16188\n",
      "16190\n",
      "16192\n",
      "16195\n",
      "16196\n",
      "16199\n",
      "16201\n",
      "16203\n",
      "16210\n",
      "16211\n",
      "16244\n",
      "16245\n",
      "16246\n",
      "16248\n",
      "16250\n",
      "16251\n",
      "16253\n",
      "16255\n",
      "16256\n",
      "16271\n",
      "16272\n",
      "16273\n",
      "16277\n",
      "16291\n",
      "16292\n",
      "16304\n",
      "16322\n",
      "16323\n",
      "16326\n",
      "16328\n",
      "16333\n",
      "16340\n",
      "16343\n",
      "16349\n",
      "16356\n",
      "16359\n",
      "16361\n",
      "16364\n",
      "16366\n",
      "16367\n",
      "16370\n",
      "16371\n",
      "16373\n",
      "16375\n",
      "16376\n",
      "16377\n",
      "16378\n",
      "16379\n",
      "16383\n",
      "16385\n",
      "16386\n",
      "16389\n",
      "16390\n",
      "16392\n",
      "16400\n",
      "16402\n",
      "16404\n",
      "16405\n",
      "16408\n",
      "16409\n",
      "16410\n",
      "16411\n",
      "16415\n",
      "16418\n",
      "16419\n",
      "16420\n",
      "16421\n",
      "16422\n",
      "16423\n",
      "16424\n",
      "16426\n",
      "16429\n",
      "16442\n",
      "16445\n",
      "16447\n",
      "16453\n",
      "16470\n",
      "16472\n",
      "16473\n",
      "16493\n",
      "16502\n",
      "16519\n",
      "16521\n",
      "16522\n",
      "16524\n",
      "16531\n",
      "16541\n",
      "16542\n",
      "16546\n",
      "16549\n",
      "16551\n",
      "16554\n",
      "16555\n",
      "16556\n",
      "16557\n",
      "16558\n",
      "16559\n",
      "16560\n",
      "16563\n",
      "16564\n",
      "16567\n",
      "16568\n",
      "16570\n",
      "16571\n",
      "16572\n",
      "16573\n",
      "16574\n",
      "16575\n",
      "16576\n",
      "16578\n",
      "16579\n",
      "16580\n",
      "16581\n",
      "16583\n",
      "16585\n",
      "16586\n",
      "16618\n",
      "16689\n",
      "16717\n",
      "16718\n",
      "16719\n",
      "16720\n",
      "16728\n",
      "16732\n",
      "16733\n",
      "16734\n",
      "16735\n",
      "16739\n",
      "16742\n",
      "16743\n",
      "16745\n",
      "16750\n",
      "16751\n",
      "16753\n",
      "16755\n",
      "16760\n",
      "16763\n",
      "16765\n",
      "16771\n",
      "16777\n",
      "16781\n",
      "16787\n",
      "16789\n",
      "16795\n",
      "16824\n",
      "16825\n",
      "16843\n",
      "16860\n",
      "16864\n",
      "16867\n",
      "16872\n",
      "16877\n",
      "16895\n",
      "16908\n",
      "16912\n",
      "16917\n",
      "16919\n",
      "16920\n",
      "16921\n",
      "16927\n",
      "16931\n",
      "16933\n",
      "16937\n",
      "16966\n",
      "16967\n",
      "16977\n",
      "16980\n",
      "16981\n",
      "16985\n",
      "16986\n",
      "16989\n",
      "16990\n",
      "17012\n",
      "17014\n",
      "17019\n",
      "17022\n",
      "17028\n",
      "17030\n",
      "17032\n",
      "17051\n",
      "17063\n",
      "17067\n",
      "17096\n",
      "17103\n",
      "17104\n",
      "17111\n",
      "17112\n",
      "17141\n",
      "17171\n",
      "17173\n",
      "17174\n",
      "17176\n",
      "17181\n",
      "17185\n",
      "17195\n",
      "17196\n",
      "17197\n",
      "17198\n",
      "17203\n",
      "17205\n",
      "17208\n",
      "17209\n",
      "17211\n",
      "17213\n",
      "17216\n",
      "17217\n",
      "17218\n",
      "17219\n",
      "17224\n",
      "17230\n",
      "17232\n",
      "17237\n",
      "17238\n",
      "17241\n",
      "17248\n",
      "17254\n",
      "17257\n",
      "17260\n",
      "17262\n",
      "17264\n",
      "17265\n",
      "17266\n",
      "17272\n",
      "17274\n",
      "17275\n",
      "17277\n",
      "17278\n",
      "17281\n",
      "17282\n",
      "17283\n",
      "17285\n",
      "17291\n",
      "17293\n",
      "17294\n",
      "17296\n",
      "17299\n",
      "17300\n",
      "17301\n",
      "17302\n",
      "17304\n",
      "17305\n",
      "17307\n",
      "17308\n",
      "17310\n",
      "17316\n",
      "17322\n",
      "17328\n",
      "17330\n",
      "17331\n",
      "17332\n",
      "17333\n",
      "17334\n",
      "17336\n",
      "17342\n",
      "17344\n",
      "17345\n",
      "17347\n",
      "17348\n",
      "17349\n",
      "17350\n",
      "17353\n",
      "17357\n",
      "17359\n",
      "17362\n",
      "17363\n",
      "17367\n",
      "17370\n",
      "17375\n",
      "17376\n",
      "17379\n",
      "17380\n",
      "17382\n",
      "17388\n",
      "17390\n",
      "17391\n",
      "17394\n",
      "17552\n",
      "17554\n",
      "17559\n",
      "17560\n",
      "17569\n",
      "17591\n",
      "17592\n",
      "17598\n",
      "17610\n",
      "17615\n",
      "17623\n",
      "17626\n",
      "17627\n",
      "17657\n",
      "17658\n",
      "17659\n",
      "17660\n",
      "17663\n",
      "17668\n",
      "17669\n",
      "17670\n",
      "17675\n",
      "17680\n",
      "17691\n",
      "17694\n",
      "17696\n",
      "17699\n",
      "17702\n",
      "17703\n",
      "17704\n",
      "17705\n",
      "17709\n",
      "17710\n",
      "17734\n",
      "17736\n",
      "17738\n",
      "17739\n",
      "17742\n",
      "17744\n",
      "17745\n",
      "17746\n",
      "17752\n",
      "17755\n",
      "17775\n",
      "17783\n",
      "17822\n",
      "17824\n",
      "17844\n",
      "17845\n",
      "17853\n",
      "17861\n",
      "17863\n",
      "17864\n",
      "17865\n",
      "17867\n",
      "17870\n",
      "17871\n",
      "17872\n",
      "17874\n",
      "17875\n",
      "17876\n",
      "17878\n",
      "17879\n",
      "17880\n",
      "17881\n",
      "17886\n",
      "17892\n",
      "17894\n",
      "17896\n",
      "17902\n",
      "17906\n",
      "17907\n",
      "17908\n",
      "17909\n",
      "17912\n",
      "17913\n",
      "17914\n",
      "17920\n",
      "17923\n",
      "17924\n",
      "17927\n",
      "17928\n",
      "17932\n",
      "17934\n",
      "17935\n",
      "17938\n",
      "17939\n",
      "17940\n",
      "17941\n",
      "17951\n",
      "17958\n",
      "17960\n",
      "17961\n",
      "17962\n",
      "17965\n",
      "17967\n",
      "17968\n",
      "17969\n",
      "17970\n",
      "17973\n",
      "17975\n",
      "17977\n",
      "17980\n",
      "17981\n",
      "17987\n",
      "17988\n",
      "17989\n",
      "17991\n",
      "17992\n",
      "17995\n",
      "17996\n",
      "17998\n",
      "18004\n",
      "18007\n",
      "18011\n",
      "18018\n",
      "18021\n",
      "18023\n",
      "18024\n",
      "18025\n",
      "18026\n",
      "18027\n",
      "18028\n",
      "18030\n",
      "18031\n",
      "18032\n",
      "18033\n",
      "18035\n",
      "18036\n",
      "18039\n",
      "18040\n",
      "18041\n",
      "18044\n",
      "18045\n",
      "18046\n",
      "18047\n",
      "18048\n",
      "18049\n",
      "18053\n",
      "18055\n",
      "18056\n",
      "18057\n",
      "18058\n",
      "18060\n",
      "18065\n",
      "18074\n",
      "18077\n",
      "18086\n",
      "18118\n",
      "18127\n",
      "18142\n",
      "18158\n",
      "18160\n",
      "18175\n",
      "18258\n",
      "18322\n",
      "18323\n",
      "18324\n",
      "18332\n",
      "18333\n",
      "18336\n",
      "18338\n",
      "18341\n",
      "18344\n",
      "18348\n",
      "18349\n",
      "18355\n",
      "18369\n",
      "18370\n",
      "18374\n",
      "18375\n",
      "18384\n",
      "18389\n",
      "18391\n",
      "18392\n",
      "18393\n",
      "18409\n",
      "18410\n",
      "18411\n",
      "18412\n",
      "18413\n",
      "18420\n",
      "18422\n",
      "18425\n",
      "18426\n",
      "18427\n",
      "18428\n",
      "18430\n",
      "18431\n",
      "18434\n",
      "18444\n",
      "18458\n",
      "18488\n",
      "18491\n",
      "18496\n",
      "18522\n",
      "18523\n",
      "18542\n",
      "18556\n",
      "18560\n",
      "18585\n",
      "18610\n",
      "18637\n",
      "18645\n",
      "18647\n",
      "18648\n",
      "18651\n",
      "18655\n",
      "18660\n",
      "18661\n",
      "18663\n",
      "18669\n",
      "18670\n",
      "18673\n",
      "18676\n",
      "18678\n",
      "18679\n",
      "18682\n",
      "18685\n",
      "18690\n",
      "18692\n",
      "18697\n",
      "18698\n",
      "18703\n",
      "18705\n",
      "18709\n",
      "18710\n",
      "18714\n",
      "18715\n",
      "18723\n",
      "18726\n",
      "18730\n",
      "18740\n",
      "18741\n",
      "18748\n",
      "18754\n",
      "18763\n",
      "18769\n",
      "18770\n",
      "18773\n",
      "18774\n",
      "18776\n",
      "18799\n",
      "18802\n",
      "18807\n",
      "18808\n",
      "18811\n",
      "18815\n",
      "18856\n",
      "18864\n",
      "18865\n",
      "18868\n",
      "18871\n",
      "18874\n",
      "18884\n",
      "18887\n",
      "18893\n",
      "18943\n",
      "18944\n",
      "18954\n",
      "18970\n",
      "18973\n",
      "19001\n",
      "19167\n",
      "19169\n",
      "19170\n",
      "19172\n",
      "19176\n",
      "19179\n",
      "19180\n",
      "19181\n",
      "19185\n",
      "19189\n",
      "19191\n",
      "19192\n",
      "19194\n",
      "19205\n",
      "19208\n",
      "19216\n",
      "19223\n",
      "19226\n",
      "19227\n",
      "19236\n",
      "19237\n",
      "19244\n",
      "19250\n",
      "19251\n",
      "19252\n",
      "19254\n",
      "19257\n",
      "19264\n",
      "19268\n",
      "19301\n",
      "19302\n",
      "19303\n",
      "19305\n",
      "19307\n",
      "19311\n",
      "19312\n",
      "19313\n",
      "19314\n",
      "19315\n",
      "19316\n",
      "19317\n",
      "19318\n",
      "19319\n",
      "19321\n",
      "19322\n",
      "19323\n",
      "19325\n",
      "19332\n",
      "19333\n",
      "19334\n",
      "19337\n",
      "19354\n",
      "19356\n",
      "19359\n",
      "19361\n",
      "19362\n",
      "19363\n",
      "19364\n",
      "19366\n",
      "19369\n",
      "19370\n",
      "19371\n",
      "19374\n",
      "19375\n",
      "19381\n",
      "19383\n",
      "19385\n",
      "19386\n",
      "19387\n",
      "19388\n",
      "19389\n",
      "19390\n",
      "19392\n",
      "19394\n",
      "19395\n",
      "19398\n",
      "19400\n",
      "19401\n",
      "19402\n",
      "19406\n",
      "19407\n",
      "19408\n",
      "19415\n",
      "19423\n",
      "19424\n",
      "19425\n",
      "19426\n",
      "19428\n",
      "19429\n",
      "19434\n",
      "19436\n",
      "19437\n",
      "19439\n",
      "19442\n",
      "19443\n",
      "19445\n",
      "19446\n",
      "19447\n",
      "19450\n",
      "19459\n",
      "19460\n",
      "19461\n",
      "19462\n",
      "19463\n",
      "19464\n",
      "19465\n",
      "19467\n",
      "19468\n",
      "19472\n",
      "19473\n",
      "19476\n",
      "19477\n",
      "19479\n",
      "19483\n",
      "19500\n",
      "19515\n",
      "19521\n",
      "19531\n",
      "19535\n",
      "19542\n",
      "19545\n",
      "19549\n",
      "19551\n",
      "19559\n",
      "19564\n",
      "19565\n",
      "19572\n",
      "19573\n",
      "19581\n",
      "19595\n",
      "19596\n",
      "19627\n",
      "19646\n",
      "19699\n",
      "19783\n",
      "19785\n",
      "19786\n",
      "19794\n",
      "19795\n",
      "19828\n",
      "19843\n",
      "19852\n",
      "19875\n",
      "19876\n",
      "19912\n",
      "19918\n",
      "19933\n",
      "20001\n",
      "20011\n",
      "20012\n",
      "20013\n",
      "20014\n",
      "20015\n",
      "20016\n",
      "20017\n",
      "20018\n",
      "20019\n",
      "20021\n",
      "20023\n",
      "20024\n",
      "20026\n",
      "20028\n",
      "20030\n",
      "20032\n",
      "20035\n",
      "20038\n",
      "20039\n",
      "20040\n",
      "20041\n",
      "20042\n",
      "20043\n",
      "20044\n",
      "20049\n",
      "20053\n",
      "20056\n",
      "20059\n",
      "20060\n",
      "20062\n",
      "20067\n",
      "20068\n",
      "20069\n",
      "20071\n",
      "20074\n",
      "20081\n",
      "20086\n",
      "20090\n",
      "20093\n",
      "20094\n",
      "20096\n",
      "20097\n",
      "20100\n",
      "20102\n",
      "20103\n",
      "20109\n",
      "20110\n",
      "20112\n",
      "20113\n",
      "20114\n",
      "20116\n",
      "20117\n",
      "20118\n",
      "20119\n",
      "20120\n",
      "20123\n",
      "20127\n",
      "20129\n",
      "20130\n",
      "20131\n",
      "20132\n",
      "20133\n",
      "20135\n",
      "20143\n",
      "20147\n",
      "20148\n",
      "20167\n",
      "20175\n",
      "20196\n",
      "20209\n",
      "20220\n",
      "20236\n",
      "20246\n",
      "20261\n",
      "20266\n",
      "20274\n",
      "20285\n",
      "20286\n",
      "20316\n",
      "20317\n",
      "20325\n",
      "20342\n",
      "20352\n",
      "20367\n",
      "20376\n",
      "20387\n",
      "20388\n",
      "20454\n",
      "20455\n",
      "20456\n",
      "20464\n",
      "20469\n",
      "20475\n",
      "20476\n",
      "20481\n",
      "20485\n",
      "20489\n",
      "20490\n",
      "20491\n",
      "20492\n",
      "20495\n",
      "20504\n",
      "20512\n",
      "20514\n",
      "20515\n",
      "20516\n",
      "20533\n",
      "20534\n",
      "20535\n",
      "20536\n",
      "20539\n",
      "20540\n",
      "20541\n",
      "20547\n",
      "20548\n",
      "20549\n",
      "20553\n",
      "20555\n",
      "20557\n",
      "20558\n",
      "20559\n",
      "20562\n",
      "20565\n",
      "20566\n",
      "20569\n",
      "20572\n",
      "20573\n",
      "20583\n",
      "20584\n",
      "20588\n",
      "20592\n",
      "20593\n",
      "20594\n",
      "20595\n",
      "20597\n",
      "20600\n",
      "20601\n",
      "20608\n",
      "20609\n",
      "20614\n",
      "20621\n",
      "20629\n",
      "20631\n",
      "20633\n",
      "20634\n",
      "20640\n",
      "20643\n",
      "20644\n",
      "20645\n",
      "20648\n",
      "20654\n",
      "20655\n",
      "20656\n",
      "20657\n",
      "20658\n",
      "20661\n",
      "20663\n",
      "20664\n",
      "20665\n",
      "20679\n",
      "20699\n",
      "20700\n",
      "20706\n",
      "20787\n",
      "20789\n",
      "20796\n",
      "20797\n",
      "20798\n",
      "20803\n",
      "20804\n",
      "20805\n",
      "20808\n",
      "20817\n",
      "20822\n",
      "20823\n",
      "20838\n",
      "20849\n",
      "20851\n",
      "20875\n",
      "20876\n",
      "20881\n",
      "20888\n",
      "20892\n",
      "20893\n",
      "20903\n",
      "20905\n",
      "20911\n",
      "20921\n",
      "20925\n",
      "20931\n",
      "20933\n",
      "20937\n",
      "20939\n",
      "20945\n",
      "20946\n",
      "20947\n",
      "20948\n",
      "20951\n",
      "20952\n",
      "20969\n",
      "20970\n",
      "20971\n",
      "20976\n",
      "20978\n",
      "20980\n",
      "20983\n",
      "20986\n",
      "20987\n",
      "20988\n",
      "20989\n",
      "20998\n",
      "21002\n",
      "21026\n",
      "21029\n",
      "21033\n",
      "21052\n",
      "21055\n",
      "21077\n",
      "21078\n",
      "21079\n",
      "21080\n",
      "21081\n",
      "21083\n",
      "21084\n",
      "21085\n",
      "21098\n",
      "21099\n",
      "21101\n",
      "21105\n",
      "21106\n",
      "21117\n",
      "21118\n",
      "21127\n",
      "21129\n",
      "21130\n",
      "21133\n",
      "21135\n",
      "21137\n",
      "21138\n",
      "21139\n",
      "21143\n",
      "21144\n",
      "21145\n",
      "21150\n",
      "21153\n",
      "21159\n",
      "21160\n",
      "21162\n",
      "21163\n",
      "21165\n",
      "21168\n",
      "21170\n",
      "21171\n",
      "21174\n",
      "21181\n",
      "21204\n",
      "21209\n",
      "21210\n",
      "21211\n",
      "21224\n",
      "21226\n",
      "21227\n",
      "21230\n",
      "21241\n",
      "21243\n",
      "21244\n",
      "21248\n",
      "21251\n",
      "21258\n",
      "21265\n",
      "21266\n",
      "21268\n",
      "21269\n",
      "21271\n",
      "21276\n",
      "21277\n",
      "21278\n",
      "21279\n",
      "21280\n",
      "21286\n",
      "21288\n",
      "21289\n",
      "21306\n",
      "21315\n",
      "21316\n",
      "21326\n",
      "21343\n",
      "21361\n",
      "21363\n",
      "21376\n",
      "21409\n",
      "21448\n",
      "21453\n",
      "21454\n",
      "21455\n",
      "21457\n",
      "21467\n",
      "21468\n",
      "21469\n",
      "21471\n",
      "21473\n",
      "21474\n",
      "21475\n",
      "21480\n",
      "21487\n",
      "21488\n",
      "21491\n",
      "21492\n",
      "21501\n",
      "21523\n",
      "21529\n",
      "21531\n",
      "21532\n",
      "21537\n",
      "21538\n",
      "21539\n",
      "21541\n",
      "21542\n",
      "21547\n",
      "21551\n",
      "21556\n",
      "21557\n",
      "21563\n",
      "21566\n",
      "21567\n",
      "21581\n",
      "21583\n",
      "21584\n",
      "21592\n",
      "21597\n",
      "21622\n",
      "21624\n",
      "21627\n",
      "21628\n",
      "21629\n",
      "21631\n",
      "21637\n",
      "21641\n",
      "21642\n",
      "21645\n",
      "21650\n",
      "21653\n",
      "21658\n",
      "21689\n",
      "21690\n",
      "21696\n",
      "21707\n",
      "21715\n",
      "21716\n",
      "21717\n",
      "21718\n",
      "21721\n",
      "21728\n",
      "21729\n",
      "21730\n",
      "21739\n",
      "21742\n",
      "21744\n",
      "21745\n",
      "21758\n",
      "21767\n",
      "21772\n",
      "21776\n",
      "21777\n",
      "21784\n",
      "21787\n",
      "21795\n",
      "21797\n",
      "21799\n",
      "21801\n",
      "21809\n",
      "21816\n",
      "21824\n",
      "21825\n",
      "21841\n",
      "21845\n",
      "21846\n",
      "21870\n",
      "21877\n",
      "21882\n",
      "21883\n",
      "21884\n",
      "21894\n",
      "21901\n",
      "21905\n",
      "21912\n",
      "21915\n",
      "21916\n",
      "21923\n",
      "21924\n",
      "21927\n",
      "21941\n",
      "21942\n",
      "21944\n",
      "21945\n",
      "21946\n",
      "21949\n",
      "21954\n",
      "21958\n",
      "21962\n",
      "21969\n",
      "21973\n",
      "21995\n",
      "21996\n",
      "21997\n",
      "21998\n",
      "22004\n",
      "22010\n",
      "22011\n",
      "22012\n",
      "22023\n",
      "22029\n",
      "22031\n",
      "22032\n",
      "22036\n",
      "22040\n",
      "22046\n",
      "22048\n",
      "22056\n",
      "22057\n",
      "22060\n",
      "22063\n",
      "22066\n",
      "22074\n",
      "22100\n",
      "22108\n",
      "22120\n",
      "22124\n",
      "22125\n",
      "22147\n",
      "22149\n",
      "22158\n",
      "22176\n",
      "22177\n",
      "22178\n",
      "22185\n",
      "22188\n",
      "22218\n",
      "22263\n",
      "22267\n",
      "22268\n",
      "22270\n",
      "22274\n",
      "22276\n",
      "22285\n",
      "22286\n",
      "22296\n",
      "22300\n",
      "22305\n",
      "22306\n",
      "22330\n",
      "22331\n",
      "22332\n",
      "22334\n",
      "22335\n",
      "22339\n",
      "22340\n",
      "22341\n",
      "22343\n",
      "22344\n",
      "22345\n",
      "22346\n",
      "22351\n",
      "22357\n",
      "22358\n",
      "22360\n",
      "22363\n",
      "22365\n",
      "22377\n",
      "22382\n",
      "22385\n",
      "22386\n",
      "22390\n",
      "22393\n",
      "22397\n",
      "22410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22414\n",
      "22418\n",
      "22421\n",
      "22422\n",
      "22423\n",
      "22424\n",
      "22428\n",
      "22429\n",
      "22442\n",
      "22469\n",
      "22489\n",
      "22490\n",
      "22491\n",
      "22509\n",
      "22513\n",
      "22515\n",
      "22517\n",
      "22535\n",
      "22537\n",
      "22547\n",
      "22550\n",
      "22565\n",
      "22578\n",
      "22579\n",
      "22581\n",
      "22582\n",
      "22585\n",
      "22586\n",
      "22588\n",
      "22589\n",
      "22600\n",
      "22601\n",
      "22602\n",
      "22603\n",
      "22604\n",
      "22605\n",
      "22607\n",
      "22609\n",
      "22610\n",
      "22624\n",
      "22631\n",
      "22632\n",
      "22634\n",
      "22643\n",
      "22646\n",
      "22648\n",
      "22650\n",
      "22652\n",
      "22665\n",
      "22668\n",
      "22670\n",
      "22672\n",
      "22673\n",
      "22674\n",
      "22676\n",
      "22677\n",
      "22678\n",
      "22679\n",
      "22682\n",
      "22686\n",
      "22688\n",
      "22692\n",
      "22693\n",
      "22696\n",
      "22700\n",
      "22702\n",
      "22703\n",
      "22704\n",
      "22706\n",
      "22707\n",
      "22709\n",
      "22712\n",
      "22714\n",
      "22717\n",
      "22719\n",
      "22721\n",
      "22722\n",
      "22723\n",
      "22725\n",
      "22727\n",
      "22728\n",
      "22729\n",
      "22730\n",
      "22731\n",
      "22738\n",
      "22741\n",
      "22744\n",
      "22746\n",
      "22749\n",
      "22750\n",
      "22752\n",
      "22753\n",
      "22754\n",
      "22761\n",
      "22763\n",
      "22768\n",
      "22769\n",
      "22770\n",
      "22771\n",
      "22777\n",
      "22794\n",
      "22800\n",
      "22813\n",
      "22825\n",
      "22829\n",
      "22839\n",
      "22841\n",
      "22849\n",
      "22850\n",
      "22854\n",
      "22855\n",
      "22856\n",
      "22860\n",
      "22864\n",
      "22869\n",
      "22873\n",
      "22875\n",
      "22876\n",
      "22878\n",
      "22880\n",
      "22881\n",
      "22882\n",
      "22883\n",
      "22894\n",
      "22914\n",
      "22915\n",
      "22920\n",
      "22922\n",
      "22946\n",
      "22950\n",
      "22951\n",
      "22952\n",
      "22954\n",
      "22961\n",
      "22964\n",
      "22966\n",
      "22970\n",
      "22971\n",
      "22973\n",
      "22978\n",
      "22979\n",
      "22980\n",
      "22981\n",
      "22985\n",
      "22994\n",
      "22998\n",
      "23004\n",
      "23005\n",
      "23007\n",
      "23011\n",
      "23012\n",
      "23014\n",
      "23015\n",
      "23025\n",
      "23026\n",
      "23031\n",
      "23037\n",
      "23039\n",
      "23041\n",
      "23045\n",
      "23048\n",
      "23049\n",
      "23052\n",
      "23053\n",
      "23054\n",
      "23060\n",
      "23061\n",
      "23065\n",
      "23074\n",
      "23076\n",
      "23078\n",
      "23080\n",
      "23084\n",
      "23085\n",
      "23087\n",
      "23088\n",
      "23090\n",
      "23106\n",
      "23107\n",
      "23112\n",
      "23115\n",
      "23116\n",
      "23119\n",
      "23121\n",
      "23122\n",
      "23123\n",
      "23124\n",
      "23125\n",
      "23127\n",
      "23133\n",
      "23134\n",
      "23135\n",
      "23136\n",
      "23138\n",
      "23141\n",
      "23142\n",
      "23147\n",
      "23148\n",
      "23153\n",
      "23155\n",
      "23157\n",
      "23158\n",
      "23160\n",
      "23162\n",
      "23166\n",
      "23167\n",
      "23170\n",
      "23172\n",
      "23173\n",
      "23174\n",
      "23178\n",
      "23186\n",
      "23206\n",
      "23216\n",
      "23240\n",
      "23263\n",
      "23266\n",
      "23268\n",
      "23269\n",
      "23277\n",
      "23284\n",
      "23290\n",
      "23297\n",
      "23300\n",
      "23309\n",
      "23311\n",
      "23319\n",
      "23320\n",
      "23324\n",
      "23332\n",
      "23335\n",
      "23341\n",
      "23344\n",
      "23350\n",
      "23351\n",
      "23352\n",
      "23355\n",
      "23356\n",
      "23357\n",
      "23359\n",
      "23364\n",
      "23365\n",
      "23366\n",
      "23368\n",
      "23377\n",
      "23378\n",
      "23379\n",
      "23380\n",
      "23401\n",
      "23402\n",
      "23426\n",
      "23437\n",
      "23440\n",
      "23442\n",
      "23445\n",
      "23473\n",
      "23474\n",
      "23494\n",
      "23499\n",
      "23501\n",
      "23502\n",
      "23503\n",
      "23517\n",
      "23523\n",
      "23524\n",
      "23526\n",
      "23530\n",
      "23533\n",
      "23534\n",
      "23537\n",
      "23540\n",
      "23547\n",
      "23552\n",
      "23558\n",
      "23559\n",
      "23561\n",
      "23562\n",
      "23564\n",
      "23565\n",
      "23567\n",
      "23584\n",
      "23585\n",
      "23588\n",
      "23593\n",
      "23595\n",
      "23602\n",
      "23603\n",
      "23605\n",
      "23606\n",
      "23607\n",
      "23608\n",
      "23609\n",
      "23610\n",
      "23615\n",
      "23617\n",
      "23620\n",
      "23621\n",
      "23623\n",
      "23633\n",
      "23638\n",
      "23643\n",
      "23644\n",
      "23647\n",
      "23651\n",
      "23652\n",
      "23653\n",
      "23662\n",
      "23663\n",
      "23666\n",
      "23669\n",
      "23676\n",
      "23680\n",
      "23688\n",
      "23689\n",
      "23698\n",
      "23712\n",
      "23735\n",
      "23740\n",
      "23743\n",
      "23746\n",
      "23749\n",
      "23750\n",
      "23753\n",
      "23755\n",
      "23756\n",
      "23757\n",
      "23760\n",
      "23766\n",
      "23767\n",
      "23768\n",
      "23775\n",
      "23779\n",
      "23783\n",
      "23784\n",
      "23788\n",
      "23795\n",
      "23797\n",
      "23814\n",
      "23815\n",
      "23819\n",
      "23822\n",
      "23833\n",
      "23834\n",
      "23874\n",
      "23888\n",
      "23898\n",
      "23926\n",
      "23927\n",
      "23937\n",
      "23942\n",
      "23943\n",
      "23944\n",
      "23945\n",
      "23953\n",
      "23968\n",
      "23982\n",
      "23983\n",
      "23984\n",
      "23987\n",
      "23989\n",
      "23991\n",
      "23998\n",
      "23999\n",
      "24000\n",
      "24006\n",
      "24008\n",
      "24011\n",
      "24015\n",
      "24016\n",
      "24017\n",
      "24022\n",
      "24023\n",
      "24025\n",
      "24028\n",
      "24029\n",
      "24041\n",
      "24051\n",
      "24055\n",
      "24056\n",
      "24057\n",
      "24060\n",
      "24061\n",
      "24069\n",
      "24072\n",
      "24081\n",
      "24082\n",
      "24102\n",
      "24103\n",
      "24115\n",
      "24116\n",
      "24122\n",
      "24123\n",
      "24136\n",
      "24142\n",
      "24144\n",
      "24146\n",
      "24148\n",
      "24150\n",
      "24152\n",
      "24159\n",
      "24162\n",
      "24163\n",
      "24168\n",
      "24169\n",
      "24170\n",
      "24171\n",
      "24177\n",
      "24180\n",
      "24181\n",
      "24182\n",
      "24185\n",
      "24187\n",
      "24189\n",
      "24190\n",
      "24196\n",
      "24198\n",
      "24199\n",
      "24202\n",
      "24203\n",
      "24205\n",
      "24211\n",
      "24214\n",
      "24219\n",
      "24223\n",
      "24227\n",
      "24234\n",
      "24239\n",
      "24244\n",
      "24260\n",
      "24266\n",
      "24273\n",
      "24274\n",
      "24276\n",
      "24280\n",
      "24283\n",
      "24285\n",
      "24287\n",
      "24302\n",
      "24316\n",
      "24318\n",
      "24339\n",
      "24348\n",
      "24349\n",
      "24350\n",
      "24351\n",
      "24362\n",
      "24367\n",
      "24371\n",
      "24375\n",
      "24376\n",
      "24377\n",
      "24379\n",
      "24380\n",
      "24385\n",
      "24386\n",
      "24388\n",
      "24389\n",
      "24393\n",
      "24395\n",
      "24397\n",
      "24401\n",
      "24402\n",
      "24405\n",
      "24406\n",
      "24407\n",
      "24408\n",
      "24410\n",
      "24564\n",
      "24568\n",
      "24570\n",
      "24595\n",
      "24615\n",
      "24646\n",
      "24647\n",
      "24650\n",
      "24763\n",
      "24765\n",
      "24766\n",
      "24768\n",
      "24773\n",
      "24774\n",
      "24776\n",
      "24777\n",
      "24784\n",
      "24796\n",
      "24797\n",
      "24799\n",
      "24814\n",
      "24815\n",
      "24817\n",
      "24820\n",
      "24826\n",
      "24837\n",
      "24840\n",
      "24841\n",
      "24845\n",
      "24854\n",
      "24862\n",
      "24863\n",
      "24864\n",
      "24867\n",
      "24880\n",
      "24881\n",
      "24884\n",
      "24889\n",
      "24914\n",
      "24918\n",
      "24924\n",
      "24934\n",
      "24965\n",
      "24967\n",
      "24968\n",
      "24970\n",
      "24973\n",
      "24975\n",
      "24976\n",
      "24977\n",
      "24984\n",
      "24985\n",
      "24990\n",
      "24991\n",
      "24993\n",
      "24998\n",
      "24999\n",
      "25002\n",
      "25018\n",
      "25022\n",
      "25041\n",
      "25051\n",
      "25053\n",
      "25084\n",
      "25086\n",
      "25087\n",
      "25089\n",
      "25090\n",
      "25091\n",
      "25093\n",
      "25095\n",
      "25098\n",
      "25110\n",
      "25118\n",
      "25125\n",
      "25128\n",
      "25132\n",
      "25133\n",
      "25134\n",
      "25137\n",
      "25138\n",
      "25139\n",
      "25142\n",
      "25143\n",
      "25144\n",
      "25149\n",
      "25151\n",
      "25163\n",
      "25167\n",
      "25175\n",
      "25178\n",
      "25191\n",
      "25193\n",
      "25194\n",
      "25195\n",
      "25199\n",
      "25200\n",
      "25201\n",
      "25202\n",
      "25205\n",
      "25206\n",
      "25207\n",
      "25209\n",
      "25223\n",
      "25226\n",
      "25228\n",
      "25232\n",
      "25236\n",
      "25238\n",
      "25259\n",
      "25269\n",
      "25274\n",
      "25289\n",
      "25313\n",
      "25315\n",
      "25320\n",
      "25347\n",
      "25376\n",
      "25377\n",
      "25380\n",
      "25381\n",
      "25382\n",
      "25385\n",
      "25387\n",
      "25388\n",
      "25389\n",
      "25392\n",
      "25393\n",
      "25394\n",
      "25395\n",
      "25398\n",
      "25399\n",
      "25400\n",
      "25411\n",
      "25413\n",
      "25414\n",
      "25415\n",
      "25423\n",
      "25425\n",
      "25426\n",
      "25427\n",
      "25429\n",
      "25431\n",
      "25432\n",
      "25433\n",
      "25434\n",
      "25435\n",
      "25436\n",
      "25437\n",
      "25438\n",
      "25440\n",
      "25442\n",
      "25443\n",
      "25445\n",
      "25447\n",
      "25451\n",
      "25452\n",
      "25462\n",
      "25463\n",
      "25464\n",
      "25465\n",
      "25466\n",
      "25467\n",
      "25468\n",
      "25472\n",
      "25473\n",
      "25474\n",
      "25475\n",
      "25477\n",
      "25479\n",
      "25480\n",
      "25481\n",
      "25483\n",
      "25485\n",
      "25486\n",
      "25489\n",
      "25490\n",
      "25491\n",
      "25492\n",
      "25512\n",
      "25513\n",
      "25516\n",
      "25518\n",
      "25519\n",
      "25520\n",
      "25521\n",
      "25523\n",
      "25525\n",
      "25526\n",
      "25528\n",
      "25532\n",
      "25533\n",
      "25534\n",
      "25535\n",
      "25539\n",
      "25542\n",
      "25544\n",
      "25547\n",
      "25548\n",
      "25550\n",
      "25559\n",
      "25560\n",
      "25561\n",
      "25562\n",
      "25565\n",
      "25574\n",
      "25584\n",
      "25598\n",
      "25624\n",
      "25626\n",
      "25627\n",
      "25631\n",
      "25632\n",
      "25642\n",
      "25645\n",
      "25646\n",
      "25651\n",
      "25666\n",
      "25668\n",
      "25670\n",
      "25700\n",
      "25701\n",
      "25702\n",
      "25707\n",
      "25708\n",
      "25709\n",
      "25711\n",
      "25722\n",
      "25723\n",
      "25724\n",
      "25725\n",
      "25730\n",
      "25732\n",
      "25733\n",
      "25745\n",
      "25746\n",
      "25751\n",
      "25752\n",
      "25766\n",
      "25771\n",
      "25772\n",
      "25774\n",
      "25775\n",
      "25787\n",
      "25794\n",
      "25802\n",
      "25806\n",
      "25811\n",
      "25820\n",
      "25828\n",
      "25834\n",
      "25835\n",
      "25836\n",
      "25839\n",
      "25840\n",
      "25841\n",
      "25843\n",
      "25844\n",
      "25845\n",
      "25848\n",
      "25850\n",
      "25851\n",
      "25852\n",
      "25860\n",
      "25861\n",
      "25862\n",
      "25868\n",
      "25870\n",
      "25876\n",
      "25877\n",
      "25878\n",
      "25881\n",
      "25883\n",
      "25884\n",
      "25887\n",
      "25889\n",
      "25890\n",
      "25892\n",
      "25893\n",
      "25894\n",
      "25895\n",
      "25896\n",
      "25898\n",
      "25901\n",
      "25913\n",
      "25917\n",
      "25918\n",
      "25919\n",
      "25920\n",
      "25921\n",
      "25924\n",
      "25927\n",
      "25929\n",
      "25931\n",
      "25933\n",
      "25938\n",
      "25939\n",
      "25940\n",
      "25941\n",
      "25942\n",
      "25943\n",
      "25944\n",
      "25945\n",
      "25950\n",
      "25952\n",
      "25953\n",
      "25982\n",
      "25984\n",
      "26007\n",
      "26018\n",
      "26019\n",
      "26021\n",
      "26022\n",
      "26026\n",
      "26029\n",
      "26031\n",
      "26033\n",
      "26035\n",
      "26036\n",
      "26037\n",
      "26040\n",
      "26046\n",
      "26049\n",
      "26051\n",
      "26053\n",
      "26055\n",
      "26056\n",
      "26057\n",
      "26068\n",
      "26069\n",
      "26078\n",
      "26080\n",
      "26083\n",
      "26084\n",
      "26092\n",
      "26094\n",
      "26095\n",
      "26096\n",
      "26098\n",
      "26099\n",
      "26100\n",
      "26102\n",
      "26105\n",
      "26107\n",
      "26108\n",
      "26110\n",
      "26115\n",
      "26116\n",
      "26117\n",
      "26118\n",
      "26124\n",
      "26125\n",
      "26128\n",
      "26130\n",
      "26134\n",
      "26137\n",
      "26139\n",
      "26143\n",
      "26148\n",
      "26150\n",
      "26152\n",
      "26154\n",
      "26157\n",
      "26158\n",
      "26159\n",
      "26160\n",
      "26162\n",
      "26163\n",
      "26164\n",
      "26168\n",
      "26176\n",
      "26178\n",
      "26180\n",
      "26181\n",
      "26184\n",
      "26186\n",
      "26187\n",
      "26197\n",
      "26200\n",
      "26201\n",
      "26205\n",
      "26206\n",
      "26208\n",
      "26213\n",
      "26214\n",
      "26215\n",
      "26216\n",
      "26220\n",
      "26227\n",
      "26228\n",
      "26232\n",
      "26233\n",
      "26243\n",
      "26247\n",
      "26250\n",
      "26253\n",
      "26254\n",
      "26255\n",
      "26256\n",
      "26257\n",
      "26258\n",
      "26259\n",
      "26261\n",
      "26262\n",
      "26264\n",
      "26267\n",
      "26268\n",
      "26271\n",
      "26272\n",
      "26282\n",
      "26284\n",
      "26285\n",
      "26287\n",
      "26293\n",
      "26294\n",
      "26295\n",
      "26296\n",
      "26297\n",
      "26298\n",
      "26299\n",
      "26301\n",
      "26302\n",
      "26303\n",
      "26306\n",
      "26307\n",
      "26310\n",
      "26313\n",
      "26315\n",
      "26316\n",
      "26323\n",
      "26340\n",
      "26342\n",
      "26350\n",
      "26358\n",
      "26359\n",
      "26362\n",
      "26363\n",
      "26365\n",
      "26367\n",
      "26387\n",
      "26388\n",
      "26399\n",
      "26404\n",
      "26406\n",
      "26407\n",
      "26410\n",
      "26415\n",
      "26423\n",
      "26424\n",
      "26431\n",
      "26436\n",
      "26438\n",
      "26439\n",
      "26440\n",
      "26441\n",
      "26442\n",
      "26443\n",
      "26454\n",
      "26455\n",
      "26457\n",
      "26458\n",
      "26459\n",
      "26461\n",
      "26462\n",
      "26463\n",
      "26471\n",
      "26473\n",
      "26474\n",
      "26475\n",
      "26478\n",
      "26480\n",
      "26483\n",
      "26487\n",
      "26490\n",
      "26493\n",
      "26495\n",
      "26497\n",
      "26499\n",
      "26501\n",
      "26508\n",
      "26509\n",
      "26511\n",
      "26512\n",
      "26513\n",
      "26517\n",
      "26519\n",
      "26522\n",
      "26527\n",
      "26547\n",
      "26550\n",
      "26551\n",
      "26552\n",
      "26554\n",
      "26558\n",
      "26559\n",
      "26561\n",
      "26565\n",
      "26571\n",
      "26572\n",
      "26574\n",
      "26575\n",
      "26587\n",
      "26591\n",
      "26594\n",
      "26595\n",
      "26596\n",
      "26600\n",
      "26602\n",
      "26605\n",
      "26609\n",
      "26614\n",
      "26619\n",
      "26629\n",
      "26634\n",
      "26637\n",
      "26639\n",
      "26640\n",
      "26641\n",
      "26644\n",
      "26648\n",
      "26649\n",
      "26650\n",
      "26651\n",
      "26652\n",
      "26658\n",
      "26661\n",
      "26663\n",
      "26665\n",
      "26666\n",
      "26670\n",
      "26676\n",
      "26682\n",
      "26683\n",
      "26684\n",
      "26685\n",
      "26687\n",
      "26689\n",
      "26691\n",
      "26696\n",
      "26697\n",
      "26700\n",
      "26701\n",
      "26704\n",
      "26731\n",
      "26735\n",
      "26742\n",
      "26743\n",
      "26755\n",
      "26759\n",
      "26763\n",
      "26785\n",
      "26801\n",
      "26816\n",
      "26820\n",
      "26833\n",
      "26845\n",
      "26846\n",
      "26850\n",
      "26854\n",
      "26855\n",
      "26869\n",
      "26883\n",
      "26885\n",
      "26922\n",
      "26932\n",
      "26935\n",
      "26940\n",
      "26942\n",
      "26945\n",
      "26946\n",
      "26949\n",
      "26952\n",
      "26962\n",
      "26964\n",
      "26967\n",
      "26972\n",
      "26973\n",
      "26975\n",
      "26976\n",
      "26979\n",
      "26980\n",
      "26982\n",
      "26984\n",
      "26986\n",
      "26990\n",
      "26992\n",
      "26996\n",
      "26997\n",
      "26998\n",
      "26999\n",
      "27002\n",
      "27008\n",
      "27009\n",
      "27012\n",
      "27015\n",
      "27018\n",
      "27019\n",
      "27020\n",
      "27021\n",
      "27023\n",
      "27027\n",
      "27029\n",
      "27037\n",
      "27040\n",
      "27041\n",
      "27044\n",
      "27056\n",
      "27057\n",
      "27059\n",
      "27063\n",
      "27064\n",
      "27065\n",
      "27066\n",
      "27067\n",
      "27068\n",
      "27070\n",
      "27071\n",
      "27073\n",
      "27077\n",
      "27078\n",
      "27085\n",
      "27086\n",
      "27087\n",
      "27088\n",
      "27095\n",
      "27096\n",
      "27098\n",
      "27099\n",
      "27100\n",
      "27104\n",
      "27105\n",
      "27106\n",
      "27107\n",
      "27108\n",
      "27109\n",
      "27111\n",
      "27114\n",
      "27116\n",
      "27117\n",
      "27120\n",
      "27122\n",
      "27124\n",
      "27142\n",
      "27145\n",
      "27146\n",
      "27148\n",
      "27153\n",
      "27154\n",
      "27156\n",
      "27157\n",
      "27161\n",
      "27162\n",
      "27165\n",
      "27169\n",
      "27170\n",
      "27171\n",
      "27175\n",
      "27204\n",
      "27206\n",
      "27211\n",
      "27212\n",
      "27213\n",
      "27215\n",
      "27217\n",
      "27221\n",
      "27223\n",
      "27224\n",
      "27228\n",
      "27234\n",
      "27235\n",
      "27242\n",
      "27244\n",
      "27245\n",
      "27246\n",
      "27254\n",
      "27261\n",
      "27265\n",
      "27266\n",
      "27267\n",
      "27268\n",
      "27271\n",
      "27277\n",
      "27281\n",
      "27283\n",
      "27291\n",
      "27299\n",
      "27320\n",
      "27350\n",
      "27361\n",
      "27380\n",
      "27390\n",
      "27430\n",
      "27434\n",
      "27436\n",
      "27444\n",
      "27445\n",
      "27446\n",
      "27459\n",
      "27468\n",
      "27469\n",
      "27470\n",
      "27526\n",
      "27527\n",
      "27528\n",
      "27530\n",
      "27549\n",
      "27554\n",
      "27555\n",
      "27579\n",
      "27584\n",
      "27585\n",
      "27587\n",
      "27589\n",
      "27593\n",
      "27594\n",
      "27595\n",
      "27639\n",
      "27642\n",
      "27643\n",
      "27645\n",
      "27646\n",
      "27649\n",
      "27650\n",
      "27670\n",
      "27678\n",
      "27690\n",
      "27694\n",
      "27695\n",
      "27717\n",
      "27719\n",
      "27728\n",
      "27746\n",
      "27747\n",
      "27748\n",
      "27755\n",
      "27758\n",
      "27788\n",
      "27834\n",
      "27840\n",
      "27857\n",
      "27884\n",
      "27887\n",
      "27909\n",
      "27910\n",
      "27911\n",
      "27912\n",
      "27921\n",
      "27939\n",
      "27944\n",
      "27950\n",
      "27958\n",
      "27959\n",
      "27971\n",
      "27975\n",
      "27976\n",
      "27977\n",
      "27978\n",
      "27979\n",
      "27994\n",
      "27995\n",
      "27997\n",
      "28025\n",
      "28026\n",
      "28027\n",
      "28031\n",
      "28036\n",
      "28037\n",
      "28038\n",
      "28040\n",
      "28044\n",
      "28051\n",
      "28052\n",
      "28053\n",
      "28055\n",
      "28056\n",
      "28057\n",
      "28060\n",
      "28061\n",
      "28063\n",
      "28066\n",
      "28067\n",
      "28068\n",
      "28072\n",
      "28073\n",
      "28077\n",
      "28082\n",
      "28084\n",
      "28086\n",
      "28093\n",
      "28094\n",
      "28097\n",
      "28098\n",
      "28099\n",
      "28100\n",
      "28101\n",
      "28102\n",
      "28104\n",
      "28105\n",
      "28106\n",
      "28107\n",
      "28108\n",
      "28109\n",
      "28112\n",
      "28116\n",
      "28123\n",
      "28129\n",
      "28130\n",
      "28131\n",
      "28133\n",
      "28134\n",
      "28137\n",
      "28138\n",
      "28143\n",
      "28144\n",
      "28145\n",
      "28148\n",
      "28149\n",
      "28150\n",
      "28151\n",
      "28156\n",
      "28157\n",
      "28160\n",
      "28173\n",
      "28174\n",
      "28178\n",
      "28179\n",
      "28180\n",
      "28181\n",
      "28185\n",
      "28189\n",
      "28191\n",
      "28192\n",
      "28195\n",
      "28197\n",
      "28201\n",
      "28202\n",
      "28204\n",
      "28205\n",
      "28206\n",
      "28208\n",
      "28209\n",
      "28220\n",
      "28224\n",
      "28225\n",
      "28232\n",
      "28236\n",
      "28238\n",
      "28241\n",
      "28244\n",
      "28245\n",
      "28246\n",
      "28247\n",
      "28248\n",
      "28250\n",
      "28252\n",
      "28254\n",
      "28255\n",
      "28256\n",
      "28257\n",
      "28258\n",
      "28259\n",
      "28260\n",
      "28266\n",
      "28268\n",
      "28275\n",
      "28294\n",
      "28295\n",
      "28296\n",
      "28297\n",
      "28302\n",
      "28306\n",
      "28308\n",
      "28309\n",
      "28310\n",
      "28312\n",
      "28317\n",
      "28318\n",
      "28320\n",
      "28329\n",
      "28333\n",
      "28334\n",
      "28335\n",
      "28337\n",
      "28338\n",
      "28339\n",
      "28345\n",
      "28352\n",
      "28353\n",
      "28360\n",
      "28361\n",
      "28364\n",
      "28371\n",
      "28384\n",
      "28393\n",
      "28395\n",
      "28406\n",
      "28421\n",
      "28422\n",
      "28432\n",
      "28435\n",
      "28440\n",
      "28442\n",
      "28444\n",
      "28446\n",
      "28450\n",
      "28453\n",
      "28454\n",
      "28455\n",
      "28456\n",
      "28458\n",
      "28468\n",
      "28496\n",
      "28497\n",
      "28499\n",
      "28500\n",
      "28501\n",
      "28502\n",
      "28505\n",
      "28508\n",
      "28510\n",
      "28511\n",
      "28517\n",
      "28519\n",
      "28521\n",
      "28527\n",
      "28530\n",
      "28535\n",
      "28536\n",
      "28538\n",
      "28540\n",
      "28545\n",
      "28556\n",
      "28557\n",
      "28558\n",
      "28560\n",
      "28562\n",
      "28563\n",
      "28564\n",
      "28573\n",
      "28574\n",
      "28576\n",
      "28579\n",
      "28581\n",
      "28589\n",
      "28591\n",
      "28601\n",
      "28603\n",
      "28604\n",
      "28609\n",
      "28610\n",
      "28618\n",
      "28621\n",
      "28622\n",
      "28720\n",
      "28721\n",
      "28724\n",
      "28743\n",
      "28744\n",
      "28745\n",
      "28747\n",
      "28750\n",
      "28757\n",
      "28758\n",
      "28759\n",
      "28760\n",
      "28767\n",
      "28771\n",
      "28775\n",
      "28785\n",
      "28786\n",
      "28799\n",
      "28806\n",
      "28810\n",
      "28815\n",
      "28818\n",
      "28823\n",
      "28824\n",
      "28826\n",
      "28831\n",
      "28861\n",
      "28863\n",
      "28864\n",
      "28866\n",
      "28874\n",
      "28879\n",
      "28884\n",
      "28889\n",
      "28892\n",
      "28903\n",
      "28905\n",
      "28906\n",
      "28907\n",
      "28908\n",
      "28909\n",
      "28911\n",
      "28913\n",
      "28914\n",
      "28916\n",
      "28917\n",
      "28918\n",
      "28925\n",
      "28926\n",
      "28930\n",
      "28931\n",
      "28934\n",
      "28935\n",
      "28937\n",
      "28940\n",
      "28946\n",
      "28947\n",
      "28950\n",
      "28953\n",
      "28954\n",
      "28966\n",
      "28967\n",
      "28970\n",
      "28979\n",
      "28980\n",
      "28981\n",
      "28985\n",
      "28986\n",
      "28987\n",
      "28990\n",
      "28991\n",
      "28992\n",
      "28993\n",
      "28994\n",
      "28996\n",
      "28997\n",
      "28998\n",
      "29000\n",
      "29009\n",
      "29010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29014\n",
      "29025\n",
      "29026\n",
      "29029\n",
      "29033\n",
      "29035\n",
      "29045\n",
      "29046\n",
      "29052\n",
      "29053\n",
      "29054\n",
      "29057\n",
      "29058\n",
      "29060\n",
      "29061\n",
      "29064\n",
      "29071\n",
      "29087\n",
      "29098\n",
      "29142\n",
      "29150\n",
      "29159\n",
      "29160\n",
      "29161\n",
      "29162\n",
      "29170\n",
      "29190\n",
      "29193\n",
      "29196\n",
      "29260\n",
      "29261\n",
      "29316\n",
      "29317\n",
      "29321\n",
      "29331\n",
      "29332\n",
      "29343\n",
      "29360\n",
      "29361\n",
      "29379\n",
      "29416\n",
      "29418\n",
      "29419\n",
      "29421\n",
      "29429\n",
      "29431\n",
      "29467\n",
      "29470\n",
      "29486\n",
      "29487\n",
      "29491\n",
      "29493\n",
      "29494\n",
      "29497\n",
      "29498\n",
      "29499\n",
      "29500\n",
      "29501\n",
      "29515\n",
      "29518\n",
      "29522\n",
      "29527\n",
      "29531\n",
      "29532\n",
      "29538\n",
      "29543\n",
      "29544\n",
      "29546\n",
      "29552\n",
      "29553\n",
      "29554\n",
      "29558\n",
      "29559\n",
      "29560\n",
      "29568\n",
      "29569\n",
      "29582\n",
      "29584\n",
      "29589\n",
      "29591\n",
      "29598\n",
      "29599\n",
      "29602\n",
      "29608\n",
      "29611\n",
      "29621\n",
      "29630\n",
      "29632\n",
      "29633\n",
      "29634\n",
      "29635\n",
      "29636\n",
      "29641\n",
      "29648\n",
      "29650\n",
      "29653\n",
      "29654\n",
      "29656\n",
      "29657\n",
      "29658\n",
      "29659\n",
      "29660\n",
      "29667\n",
      "29668\n",
      "29670\n",
      "29671\n",
      "29672\n",
      "29673\n",
      "29674\n",
      "29675\n",
      "29683\n",
      "29688\n",
      "29701\n",
      "29704\n",
      "29709\n",
      "29710\n",
      "29716\n",
      "29718\n",
      "29720\n",
      "29735\n",
      "29737\n",
      "29745\n",
      "29746\n",
      "29749\n",
      "29755\n",
      "29768\n",
      "29794\n",
      "29804\n",
      "29810\n",
      "29815\n",
      "29817\n",
      "29818\n",
      "29834\n",
      "29848\n",
      "29858\n",
      "29863\n",
      "29875\n",
      "29881\n",
      "29897\n",
      "29898\n",
      "29904\n",
      "29907\n",
      "29913\n",
      "29917\n",
      "29918\n",
      "29920\n",
      "29924\n",
      "29927\n",
      "29930\n",
      "29932\n",
      "29939\n",
      "29945\n",
      "29946\n",
      "29950\n",
      "29952\n",
      "29954\n",
      "29970\n",
      "29972\n",
      "29975\n",
      "29978\n",
      "29982\n",
      "29984\n",
      "29985\n",
      "29987\n",
      "29988\n",
      "29993\n",
      "29997\n",
      "30009\n",
      "30012\n",
      "30028\n",
      "30039\n",
      "30055\n",
      "30074\n",
      "30075\n",
      "30077\n",
      "30088\n",
      "30091\n",
      "30097\n",
      "30103\n",
      "30118\n",
      "30124\n",
      "30127\n",
      "30129\n",
      "30133\n",
      "30141\n",
      "30152\n",
      "30153\n",
      "30154\n",
      "30158\n",
      "30183\n",
      "30185\n",
      "30187\n",
      "30189\n",
      "30193\n",
      "30196\n",
      "30199\n",
      "30213\n",
      "30229\n",
      "30244\n",
      "30247\n",
      "30249\n",
      "30250\n",
      "30262\n",
      "30263\n",
      "30276\n",
      "30278\n",
      "30285\n",
      "30300\n",
      "30302\n",
      "30303\n",
      "30304\n",
      "30306\n",
      "30308\n",
      "30309\n",
      "30316\n",
      "30317\n",
      "30328\n",
      "30329\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(jsonlist_inf)):\n",
    "    if len(jsonlist_inf[i]['relations'])>0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['*', 'Drug'],\n",
       " ['*', 'Reason'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Reason'],\n",
       " ['*', 'Drug'],\n",
       " ['*[1]', 'Strength[1]'],\n",
       " ['*[1]', 'Strength[1]'],\n",
       " ['*', 'Frequency'],\n",
       " ['*[2]', 'Duration[2]'],\n",
       " ['*[2]', 'Duration[2]'],\n",
       " ['*[2]', 'Duration[2]'],\n",
       " ['*[2]', 'Duration[2]'],\n",
       " ['*[3]', 'Duration[3]'],\n",
       " ['*[3]', 'Duration[3]'],\n",
       " ['*[4]', 'Drug[4]'],\n",
       " ['*[4]', 'Drug[4]'],\n",
       " ['*[5]', 'Drug[5]'],\n",
       " ['*[5]', 'Drug[5]'],\n",
       " ['*[6]', 'Drug[6]'],\n",
       " ['*[6]', 'Drug[6]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[7]', 'Dosage[7]'],\n",
       " ['*[7]', 'Dosage[7]'],\n",
       " ['*[8]', 'Duration[8]'],\n",
       " ['*[8]', 'Duration[8]'],\n",
       " ['*', 'Drug'],\n",
       " ['*[9]', 'Reason[9]'],\n",
       " ['*[9]', 'Reason[9]'],\n",
       " ['*[10]', 'Drug[10]'],\n",
       " ['*[10]', 'Drug[10]'],\n",
       " ['*[11]', 'Reason[11]'],\n",
       " ['*[11]', 'Reason[11]'],\n",
       " ['*[11]', 'Reason[11]'],\n",
       " ['*[11]', 'Reason[11]'],\n",
       " ['*[11]', 'Reason[11]'],\n",
       " ['*[11]', 'Reason[11]'],\n",
       " ['*[11]', 'Reason[11]'],\n",
       " ['*[12]', 'Reason[12]'],\n",
       " ['*[12]', 'Reason[12]'],\n",
       " ['*[12]', 'Reason[12]'],\n",
       " ['*[12]', 'Reason[12]'],\n",
       " ['*', 'Reason'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'death'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[13]', 'Reason[13]'],\n",
       " ['*[13]', 'Reason[13]'],\n",
       " ['*[14]', 'functional\\\\_class[14]'],\n",
       " ['*[14]', 'functional\\\\_class[14]'],\n",
       " ['*[14]', 'functional\\\\_class[14]'],\n",
       " ['*[15]', 'Drug[15]'],\n",
       " ['*[15]', 'Drug[15]'],\n",
       " ['*[16]', 'Reason[16]'],\n",
       " ['*[16]', 'Reason[16]'],\n",
       " ['*[17]', 'Drug[17]'],\n",
       " ['*[17]', 'Drug[17]'],\n",
       " ['*[18]', 'Drug[18]'],\n",
       " ['*[18]', 'Drug[18]'],\n",
       " ['*[19]', 'Drug[19]'],\n",
       " ['*[19]', 'Drug[19]'],\n",
       " ['*[20]', 'Drug[20]'],\n",
       " ['*[20]', 'Drug[20]'],\n",
       " ['*[21]', 'Drug[21]'],\n",
       " ['*[21]', 'Drug[21]'],\n",
       " ['*[22]', 'Drug[22]'],\n",
       " ['*[22]', 'Drug[22]'],\n",
       " ['*[23]', 'Drug[23]'],\n",
       " ['*[23]', 'Drug[23]'],\n",
       " ['*[24]', 'Drug[24]'],\n",
       " ['*[24]', 'Drug[24]'],\n",
       " ['*[24]', 'Drug[24]'],\n",
       " ['*[25]', 'Duration[25]'],\n",
       " ['*[25]', 'Duration[25]'],\n",
       " ['*', 'functional\\\\_class'],\n",
       " ['*', 'Drug'],\n",
       " ['*[26]', 'Drug[26]'],\n",
       " ['*[26]', 'Drug[26]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Duration'],\n",
       " ['*[27]', 'Strength[27]'],\n",
       " ['*[27]', 'Strength[27]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[28]', 'Frequency[28]'],\n",
       " ['*[28]', 'Frequency[28]'],\n",
       " ['*', 'Duration'],\n",
       " ['*[29]', 'Strength[29]'],\n",
       " ['*[29]', 'Strength[29]'],\n",
       " ['*', 'Dosage'],\n",
       " ['*', 'Form'],\n",
       " ['*', 'Form'],\n",
       " ['*[30]', 'Frequency[30]'],\n",
       " ['*[30]', 'Frequency[30]'],\n",
       " ['*[31]', 'ADE[31]'],\n",
       " ['*[31]', 'ADE[31]'],\n",
       " ['*[31]', 'ADE[31]'],\n",
       " ['*[31]', 'ADE[31]'],\n",
       " ['*[31]', 'ADE[31]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Route'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'functional\\\\_class'],\n",
       " ['*', 'Drug'],\n",
       " ['*[32]', 'ADE[32]'],\n",
       " ['*[32]', 'ADE[32]'],\n",
       " ['*[32]', 'ADE[32]'],\n",
       " ['*[32]', 'ADE[32]'],\n",
       " ['*[32]', 'ADE[32]'],\n",
       " ['*[32]', 'ADE[32]'],\n",
       " ['*[32]', 'ADE[32]'],\n",
       " ['*[33]', 'ADE[33]'],\n",
       " ['*[33]', 'ADE[33]'],\n",
       " ['*[33]', 'ADE[33]'],\n",
       " ['*[33]', 'ADE[33]'],\n",
       " ['*[33]', 'ADE[33]'],\n",
       " ['*', 'Drug'],\n",
       " ['*[34]', 'ADE[34]'],\n",
       " ['*[34]', 'ADE[34]'],\n",
       " ['*[34]', 'ADE[34]'],\n",
       " ['*[34]', 'ADE[34]'],\n",
       " ['*[34]', 'ADE[34]'],\n",
       " ['*[34]', 'ADE[34]'],\n",
       " ['*', 'Drug'],\n",
       " ['*[35]', 'ADE[35]'],\n",
       " ['*[35]', 'ADE[35]'],\n",
       " ['*[35]', 'ADE[35]'],\n",
       " ['*[35]', 'ADE[35]'],\n",
       " ['*[35]', 'ADE[35]'],\n",
       " ['*[35]', 'ADE[35]'],\n",
       " ['*[36]', 'ADE[36]'],\n",
       " ['*[36]', 'ADE[36]'],\n",
       " ['*[36]', 'ADE[36]'],\n",
       " ['*[36]', 'ADE[36]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'functional\\\\_class'],\n",
       " ['*[37]', 'Drug[37]'],\n",
       " ['*[37]', 'Drug[37]'],\n",
       " ['*[38]', 'Strength[38]'],\n",
       " ['*[38]', 'Strength[38]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[39]', 'Drug[39]'],\n",
       " ['*[39]', 'Drug[39]'],\n",
       " ['*', 'Drug'],\n",
       " ['*[40]', 'Drug[40]'],\n",
       " ['*[40]', 'Drug[40]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[41]', 'ADE[41]'],\n",
       " ['*[41]', 'ADE[41]'],\n",
       " ['*[41]', 'ADE[41]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'ADE'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[42]', 'ADE[42]'],\n",
       " ['*[42]', 'ADE[42]'],\n",
       " ['*[42]', 'ADE[42]'],\n",
       " ['*[42]', 'ADE[42]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[43]', 'Drug[43]'],\n",
       " ['*[43]', 'Drug[43]'],\n",
       " ['*[44]', 'ADE[44]'],\n",
       " ['*[44]', 'ADE[44]'],\n",
       " ['*[44]', 'ADE[44]'],\n",
       " ['*[44]', 'ADE[44]'],\n",
       " ['*[44]', 'ADE[44]'],\n",
       " ['*[44]', 'ADE[44]'],\n",
       " ['*', 'Drug'],\n",
       " ['*[45]', 'Drug[45]'],\n",
       " ['*[45]', 'Drug[45]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'death'],\n",
       " ['*', 'Reason'],\n",
       " ['*', 'Drug'],\n",
       " ['*[46]', 'Reason[46]'],\n",
       " ['*[46]', 'Reason[46]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'functional\\\\_class'],\n",
       " ['*[47]', 'Duration[47]'],\n",
       " ['*[47]', 'Duration[47]'],\n",
       " ['*[47]', 'Duration[47]'],\n",
       " ['*[47]', 'Duration[47]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[48]', 'ADE[48]'],\n",
       " ['*[48]', 'ADE[48]'],\n",
       " ['*[48]', 'ADE[48]'],\n",
       " ['*[48]', 'ADE[48]'],\n",
       " ['*[48]', 'ADE[48]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[49]', 'Reason[49]'],\n",
       " ['*[49]', 'Reason[49]'],\n",
       " ['*[50]', 'Reason[50]'],\n",
       " ['*[50]', 'Reason[50]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Route'],\n",
       " ['*', 'Drug'],\n",
       " ['*[51]', 'Reason[51]'],\n",
       " ['*[51]', 'Reason[51]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Duration'],\n",
       " ['*[52]', 'Reason[52]'],\n",
       " ['*[52]', 'Reason[52]'],\n",
       " ['*', 'Drug'],\n",
       " ['*[53]', 'Reason[53]'],\n",
       " ['*[53]', 'Reason[53]'],\n",
       " ['*[53]', 'Reason[53]'],\n",
       " ['*[54]', 'Duration[54]'],\n",
       " ['*[54]', 'Duration[54]'],\n",
       " ['*', 'Drug'],\n",
       " ['*[55]', 'Reason[55]'],\n",
       " ['*[55]', 'Reason[55]'],\n",
       " ['*[55]', 'Reason[55]'],\n",
       " ['*', 'Drug'],\n",
       " ['*[56]', 'ADE[56]'],\n",
       " ['*[56]', 'ADE[56]'],\n",
       " ['*[57]', 'Reason[57]'],\n",
       " ['*[57]', 'Reason[57]'],\n",
       " ['*[57]', 'Reason[57]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Duration'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[58]', 'Strength[58]'],\n",
       " ['*[58]', 'Strength[58]'],\n",
       " ['*[58]', 'Strength[58]'],\n",
       " ['*[59]', 'Reason[59]'],\n",
       " ['*[59]', 'Reason[59]'],\n",
       " ['*[59]', 'Reason[59]'],\n",
       " ['*[59]', 'Reason[59]'],\n",
       " ['*[59]', 'Reason[59]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Drug'],\n",
       " ['*[60]', 'Duration[60]'],\n",
       " ['*[60]', 'Duration[60]'],\n",
       " ['*[60]', 'Duration[60]'],\n",
       " ['*[60]', 'Duration[60]'],\n",
       " ['*[61]', 'Reason[61]'],\n",
       " ['*[61]', 'Reason[61]'],\n",
       " ['*[61]', 'Reason[61]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'death'],\n",
       " ['*[62]', 'Duration[62]'],\n",
       " ['*[62]', 'Duration[62]'],\n",
       " ['*', 'Drug'],\n",
       " ['*', 'Reason'],\n",
       " ['*', 'Reason'],\n",
       " ['*', 'Reason'],\n",
       " ['*[63]', 'Drug[63]'],\n",
       " ['*[63]', 'Drug[63]'],\n",
       " ['*', 'Drug']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['For',\n",
       "  'the',\n",
       "  'first',\n",
       "  'month',\n",
       "  ',',\n",
       "  'the',\n",
       "  'patients',\n",
       "  'received',\n",
       "  'either',\n",
       "  '62.5',\n",
       "  'mg',\n",
       "  'bosentan',\n",
       "  'or',\n",
       "  'placebo',\n",
       "  'bid',\n",
       "  '.'],\n",
       " 'entities': [{'type': 'duration', 'start': 3, 'end': 4},\n",
       "  {'type': 'strength', 'start': 9, 'end': 11},\n",
       "  {'type': 'drug', 'start': 11, 'end': 12},\n",
       "  {'type': 'drug', 'start': 13, 'end': 14},\n",
       "  {'type': 'frequency', 'start': 14, 'end': 16}],\n",
       " 'relations': [{'type': 'NotRel', 'head': 0, 'tail': 1},\n",
       "  {'type': 'NotRel', 'head': 0, 'tail': 2},\n",
       "  {'type': 'NotRel', 'head': 0, 'tail': 3},\n",
       "  {'type': 'NotRel', 'head': 0, 'tail': 4},\n",
       "  {'type': 'NotRel', 'head': 1, 'tail': 2},\n",
       "  {'type': 'NotRel', 'head': 1, 'tail': 3},\n",
       "  {'type': 'NotRel', 'head': 1, 'tail': 4},\n",
       "  {'type': 'NotRel', 'head': 2, 'tail': 4},\n",
       "  {'type': 'NotRel', 'head': 3, 'tail': 4}],\n",
       " 'orig_id': 19}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonlist[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.9999583  0.97070044 0.9993794  0.9991804 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.99691296 0.999587   0.9636258  0.9999801  0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.9997768  0.88127637]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.99073833 0.99993026 0.89592886 0.9260644  0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                                 []\n",
       "1      [0.9999583  0.97070044 0.9993794  0.9991804 ]\n",
       "2  [0.99691296 0.999587   0.9636258  0.9999801  0...\n",
       "3                            [0.9997768  0.88127637]\n",
       "4  [0.99073833 0.99993026 0.89592886 0.9260644  0..."
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = json.load(open('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/new/predictions_valid_epoch_150.json'))\n",
    "\n",
    "pred_prob = pd.read_csv('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/new/predictions_entvalid_epoch_150.csv', header = None)\n",
    "pred_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58262"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57733"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62844"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jsons_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Consequently',\n",
       "  ',',\n",
       "  'we',\n",
       "  'investigated',\n",
       "  'the',\n",
       "  'hemodynamic',\n",
       "  'effects',\n",
       "  'of',\n",
       "  'inhaled',\n",
       "  'milrinone',\n",
       "  'or',\n",
       "  'the',\n",
       "  'combination',\n",
       "  'iPGI2',\n",
       "  'inhaled',\n",
       "  'milrinone',\n",
       "  'in',\n",
       "  'cardiac',\n",
       "  'surgical',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'postoperative',\n",
       "  'mean',\n",
       "  'pulmonary',\n",
       "  'arterial',\n",
       "  'pressure',\n",
       "  '(',\n",
       "  'MPAP',\n",
       "  ')',\n",
       "  '25',\n",
       "  'mm',\n",
       "  'Hg',\n",
       "  'and',\n",
       "  'PVR',\n",
       "  '200',\n",
       "  'dynes',\n",
       "  's',\n",
       "  '1',\n",
       "  'cm',\n",
       "  '5',\n",
       "  '.'],\n",
       " 'entities': [{'type': 'drug', 'start': 9, 'end': 10},\n",
       "  {'type': 'drug', 'start': 15, 'end': 16}],\n",
       " 'relations': []}"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for SEMVEAL\n",
    "\n",
    "json_neg = []\n",
    "cnt = 0\n",
    "for p in pred:\n",
    "    if len(p['entities'])>1:\n",
    "        for e1 in p['entities']:\n",
    "            for e2 in p['entities']:\n",
    "                if e1!=e2:\n",
    "                    json_neg.append({\n",
    "                       'h': [[p['tokens'][e1['start']:e1['end']]], e1['type'].capitalize(), [str(y) for y in range(int(e1['start']),int(e1['end']))]],\n",
    "                       't': [[p['tokens'][e2['start']:e2['end']]], e2['type'].capitalize(), [str(y) for y in range(int(e2['start']),int(e2['end']))]],\n",
    "                        'tokens': p['tokens'],\n",
    "                        'h_id': cnt+p['entities'].index(e1),\n",
    "                        't_id': cnt+p['entities'].index(e2)\n",
    "                    })\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['iPGI2']], 'Drug', ['18']]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pred[4]\n",
    "e1 = p['entities'][0]\n",
    "e2 = p['entities'][1]\n",
    "[[p['tokens'][e1['start']:e1['end']]], e1['type'].capitalize(), [str(y) for y in range(int(e1['start']),int(e1['end']))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': [['bosentan'], 'Drug', [['10']]],\n",
       " 't': [['for', '16', 'weeks', '.'], 'Duration', [['21', '22', '23', '24']]],\n",
       " 'tokens': ['The',\n",
       "  'cohort',\n",
       "  'was',\n",
       "  'randomized',\n",
       "  'two',\n",
       "  'to',\n",
       "  'one',\n",
       "  'to',\n",
       "  'receive',\n",
       "  'bosentan',\n",
       "  'at',\n",
       "  'a',\n",
       "  'maximal',\n",
       "  'dose',\n",
       "  'of',\n",
       "  '125',\n",
       "  'mg',\n",
       "  'or',\n",
       "  'placebo',\n",
       "  'bid',\n",
       "  'for',\n",
       "  '16',\n",
       "  'weeks',\n",
       "  '.'],\n",
       " 'h_id': 5,\n",
       " 't_id': 88,\n",
       " 'filename': 'ann_chest.13-1766.tsv'}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons_neg[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = []\n",
    "l1 = ''\n",
    "for line in pred_prob[0]:\n",
    "    if str(line).find(']')>0 :\n",
    "        line = l1+line\n",
    "        prob.append(line)\n",
    "        l1 = ''\n",
    "    else:\n",
    "        l1 = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57733"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61687</th>\n",
       "      <td>61688</td>\n",
       "      <td>s00246-007-9139-2.txt</td>\n",
       "      <td>Our data reveal that a single dose of oral sil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id               filename  \\\n",
       "61687        61688  s00246-007-9139-2.txt   \n",
       "\n",
       "                                                sentence  \n",
       "61687  Our data reveal that a single dose of oral sil...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ' '.join(pred[65]['tokens'][:-1])+'.'\n",
    "sent[sent['sentence']==s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443,)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent['filename'].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "All rights reserved.                                                                                                                                                                                                                                                                                                                23\n",
       "Circulation.                                                                                                                                                                                                                                                                                                                        22\n",
       "Written informed consent was obtained from all patients.                                                                                                                                                                                                                                                                            17\n",
       "Statistical analysis.                                                                                                                                                                                                                                                                                                               17\n",
       "Journal of the American College of Cardiology.                                                                                                                                                                                                                                                                                      15\n",
       "                                                                                                                                                                                                                                                                                                                                    ..\n",
       "Correlation analysis was used to analyse the ... relationship of TAPSE, TAPSE/PASP, FAC, and FAC/PASP with PV loop ... parameters and with the Swan-Ganz catheter-derived vascular.                                                                                                                                                  1\n",
       "Therefore, future recommendations evaluation of death is subject to this limitation.                                                                                                                                                                                                                                                 1\n",
       "Unfortunately, both equitable listing for PAH patients and the long-term survival for this procedure both remain problematic.17,18 Therefore, the need for better alternative therapies and rational, data-driven combination approaches with current therapeutics are needed to improve QOL and survival for patients with PAH.     1\n",
       "Because of this, we can patients with COPD.                                                                                                                                                                                                                                                                                          1\n",
       "Vital status and treatment at 12 and 24 months were summarised as the proportions of patients who entered the 12-month and 24month periods, respectively.                                                                                                                                                                            1\n",
       "Name: sentence, Length: 61936, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent['len_sent'] = sent['sentence'].str.len()\n",
    "sent = sent[sent['len_sent']>5]\n",
    "sent['sentence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio = []\n",
    "error = 0\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    s = ' '.join(pred[i]['tokens'][:-1])+'.'\n",
    "    starts = []\n",
    "    strt = 0\n",
    "    for tkn in pred[i]['tokens'][:-1]:\n",
    "        starts.append(strt)\n",
    "        strt+=len(tkn)+1\n",
    "    starts.append(strt-1) #добавляем точку\n",
    "    probs = prob[i].replace('[','').replace(']','').split(' ')\n",
    "    while len(probs)<len(pred[i]['entities']):\n",
    "        probs.append('')\n",
    "        error+=1\n",
    "    for j in range(len(pred[i]['tokens'])):\n",
    "        in_ = 0\n",
    "        for k in range(len(pred[i]['entities'])):\n",
    "            if j==pred[i]['entities'][k]['start']:\n",
    "                bio.append([i, jsonlist_f[i]['file'], jsonlist_f[i]['sentence'], pred[i]['tokens'][j], 'B', pred[i]['entities'][k]['type'], starts[j], probs[k]])\n",
    "                in_+=1\n",
    "                break\n",
    "            elif j > pred[i]['entities'][k]['start'] and j < pred[i]['entities'][k]['end']:\n",
    "                bio.append([i, jsonlist_f[i]['file'], jsonlist_f[i]['sentence'], pred[i]['tokens'][j], 'I', pred[i]['entities'][k]['type'], starts[j], probs[k]])\n",
    "                in_+=1\n",
    "                break\n",
    "        if in_==0:\n",
    "            bio.append([i, jsonlist_f[i]['file'], jsonlist_f[i]['sentence'], pred[i]['tokens'][j], 'O', '', starts[j], ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>bio</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>tag_start</th>\n",
       "      <th>tag_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Selective</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>vasodilation</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>33</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>an</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>36</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                      filename  sentence_id          word bio tag_name  \\\n",
       "0   0  00000539-200112000-00018.txt            1     Selective   O            \n",
       "1   0  00000539-200112000-00018.txt            1     pulmonary   O            \n",
       "2   0  00000539-200112000-00018.txt            1  vasodilation   O            \n",
       "3   0  00000539-200112000-00018.txt            1            is   O            \n",
       "4   0  00000539-200112000-00018.txt            1            an   O            \n",
       "\n",
       "   tag_start tag_prob  \n",
       "0          0           \n",
       "1         10           \n",
       "2         20           \n",
       "3         33           \n",
       "4         36           "
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbio1 = pd.DataFrame(bio).drop_duplicates()\n",
    "dfbio1.columns = ['id','filename','sentence_id','word','bio','tag_name','tag_start', 'tag_prob']\n",
    "dfbio1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbio1.drop_duplicates().to_csv('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/pred_entities_joint1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbio = dfbio.drop_duplicates()\n",
    "dfbio1 = dfbio1.drop_duplicates()\n",
    "# dfbio['tag_start'] = dfbio['tag_start']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>score</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>selective</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.062703</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.062780</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>vasodilation</td>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.062780</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>0.062871</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>an</td>\n",
       "      <td>O</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>0.062871</td>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        tokens tags  start  end     score  seq_id  \\\n",
       "0           0     selective    O      0    9  0.062703       1   \n",
       "1           1     pulmonary    O     10   32  0.062780       1   \n",
       "2           2  vasodilation    O     10   32  0.062780       1   \n",
       "3           3            is    O     33   63  0.062871       1   \n",
       "4           4            an    O     33   63  0.062871       1   \n",
       "\n",
       "                       document  \n",
       "0  00000539-200112000-00018.txt  \n",
       "1  00000539-200112000-00018.txt  \n",
       "2  00000539-200112000-00018.txt  \n",
       "3  00000539-200112000-00018.txt  \n",
       "4  00000539-200112000-00018.txt  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denisbio = pd.read_csv('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/Ner на всех файлах со скором 443 0.2.csv', delimiter = '\\t')\n",
    "denisbio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>bio</th>\n",
       "      <th>tag_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Selective</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>vasodilation</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>an</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      filename  sentence_id          word bio  \\\n",
       "0           0  00000539-200112000-00018.txt            1     Selective   O   \n",
       "1           1  00000539-200112000-00018.txt            1     pulmonary   O   \n",
       "2           2  00000539-200112000-00018.txt            1  vasodilation   O   \n",
       "3           3  00000539-200112000-00018.txt            1            is   O   \n",
       "4           4  00000539-200112000-00018.txt            1            an   O   \n",
       "\n",
       "  tag_name  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sashabio = pd.read_csv('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/corrected_bio_new.csv', delimiter = '\\t')\n",
    "sashabio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1883808, 6), (2072623, 8))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbio.shape, denisbio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69374, 69374)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denisbio.seq_id.max(), dfbio.sentence_id.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres = denisbio[denisbio['tags']!='O'].merge(dfbio1[dfbio1['bio']!='O'], how = 'outer', left_on = ['seq_id', 'tokens'], \n",
    "                                              right_on = ['sentence_id','word'], indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>bio</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>tag_start</th>\n",
       "      <th>tag_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13524</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13525</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>mean</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>differences</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13527</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13528</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>VE/VCO2</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13529</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>32</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13530</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>VE/VO2</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>36</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13531</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>were</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>43</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13532</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>вЂ</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>48</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13533</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>“</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>51</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13534</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>13.3</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>53</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13535</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>58</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13536</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>95</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>60</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13537</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>%</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>63</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13538</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>CI</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13539</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>вЂ</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>68</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13540</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>“</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>71</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13541</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>36.5</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>73</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13542</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>78</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13543</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>вЂ</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>81</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13544</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>“</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>84</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13545</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>2.7</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>86</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13546</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>;</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13547</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>p</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>92</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13548</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>=</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>94</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13549</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>0.002</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>96</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13550</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>)</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>102</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13551</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>104</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13552</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>вЂ</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>108</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13553</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>“</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>111</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13554</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>15.0</td>\n",
       "      <td>B</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>113</td>\n",
       "      <td>0.92332095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13555</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>118</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13556</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>95</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>120</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13557</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>%</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>123</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>CI</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>125</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13559</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>вЂ</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>128</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13560</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>“</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>131</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13561</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>36.7</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>133</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13562</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>138</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13563</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>вЂ</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>141</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13564</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>“</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>144</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13565</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>0.4</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>146</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13566</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>;</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>150</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13567</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>p</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>152</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13568</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>=</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>154</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13569</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>0.02</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>156</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13570</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>)</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>161</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13571</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>respectively</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>163</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13572</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>176</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13573</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>fig</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>178</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13574</th>\n",
       "      <td>538</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td>181</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       filename  sentence_id          word bio tag_name  tag_start  \\\n",
       "13524  538  000242498.txt          586           The   O                   0   \n",
       "13525  538  000242498.txt          586          mean   O                   4   \n",
       "13526  538  000242498.txt          586   differences   O                   9   \n",
       "13527  538  000242498.txt          586            in   O                  21   \n",
       "13528  538  000242498.txt          586       VE/VCO2   O                  24   \n",
       "13529  538  000242498.txt          586           and   O                  32   \n",
       "13530  538  000242498.txt          586        VE/VO2   O                  36   \n",
       "13531  538  000242498.txt          586          were   O                  43   \n",
       "13532  538  000242498.txt          586            вЂ   O                  48   \n",
       "13533  538  000242498.txt          586             “   O                  51   \n",
       "13534  538  000242498.txt          586          13.3   O                  53   \n",
       "13535  538  000242498.txt          586             (   O                  58   \n",
       "13536  538  000242498.txt          586            95   O                  60   \n",
       "13537  538  000242498.txt          586             %   O                  63   \n",
       "13538  538  000242498.txt          586            CI   O                  65   \n",
       "13539  538  000242498.txt          586            вЂ   O                  68   \n",
       "13540  538  000242498.txt          586             “   O                  71   \n",
       "13541  538  000242498.txt          586          36.5   O                  73   \n",
       "13542  538  000242498.txt          586            to   O                  78   \n",
       "13543  538  000242498.txt          586            вЂ   O                  81   \n",
       "13544  538  000242498.txt          586             “   O                  84   \n",
       "13545  538  000242498.txt          586           2.7   O                  86   \n",
       "13546  538  000242498.txt          586             ;   O                  90   \n",
       "13547  538  000242498.txt          586             p   O                  92   \n",
       "13548  538  000242498.txt          586             =   O                  94   \n",
       "13549  538  000242498.txt          586         0.002   O                  96   \n",
       "13550  538  000242498.txt          586             )   O                 102   \n",
       "13551  538  000242498.txt          586           and   O                 104   \n",
       "13552  538  000242498.txt          586            вЂ   O                 108   \n",
       "13553  538  000242498.txt          586             “   O                 111   \n",
       "13554  538  000242498.txt          586          15.0   B     6mwd        113   \n",
       "13555  538  000242498.txt          586             (   O                 118   \n",
       "13556  538  000242498.txt          586            95   O                 120   \n",
       "13557  538  000242498.txt          586             %   O                 123   \n",
       "13558  538  000242498.txt          586            CI   O                 125   \n",
       "13559  538  000242498.txt          586            вЂ   O                 128   \n",
       "13560  538  000242498.txt          586             “   O                 131   \n",
       "13561  538  000242498.txt          586          36.7   O                 133   \n",
       "13562  538  000242498.txt          586            to   O                 138   \n",
       "13563  538  000242498.txt          586            вЂ   O                 141   \n",
       "13564  538  000242498.txt          586             “   O                 144   \n",
       "13565  538  000242498.txt          586           0.4   O                 146   \n",
       "13566  538  000242498.txt          586             ;   O                 150   \n",
       "13567  538  000242498.txt          586             p   O                 152   \n",
       "13568  538  000242498.txt          586             =   O                 154   \n",
       "13569  538  000242498.txt          586          0.02   O                 156   \n",
       "13570  538  000242498.txt          586             )   O                 161   \n",
       "13571  538  000242498.txt          586  respectively   O                 163   \n",
       "13572  538  000242498.txt          586             (   O                 176   \n",
       "13573  538  000242498.txt          586           fig   O                 178   \n",
       "13574  538  000242498.txt          586             .   O                 181   \n",
       "\n",
       "         tag_prob  \n",
       "13524              \n",
       "13525              \n",
       "13526              \n",
       "13527              \n",
       "13528              \n",
       "13529              \n",
       "13530              \n",
       "13531              \n",
       "13532              \n",
       "13533              \n",
       "13534              \n",
       "13535              \n",
       "13536              \n",
       "13537              \n",
       "13538              \n",
       "13539              \n",
       "13540              \n",
       "13541              \n",
       "13542              \n",
       "13543              \n",
       "13544              \n",
       "13545              \n",
       "13546              \n",
       "13547              \n",
       "13548              \n",
       "13549              \n",
       "13550              \n",
       "13551              \n",
       "13552              \n",
       "13553              \n",
       "13554  0.92332095  \n",
       "13555              \n",
       "13556              \n",
       "13557              \n",
       "13558              \n",
       "13559              \n",
       "13560              \n",
       "13561              \n",
       "13562              \n",
       "13563              \n",
       "13564              \n",
       "13565              \n",
       "13566              \n",
       "13567              \n",
       "13568              \n",
       "13569              \n",
       "13570              \n",
       "13571              \n",
       "13572              \n",
       "13573              \n",
       "13574              "
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbio1[dfbio1['sentence_id']==586]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>score</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>768476</th>\n",
       "      <td>768476</td>\n",
       "      <td>patients</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.907970</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768477</th>\n",
       "      <td>768477</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.907970</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768478</th>\n",
       "      <td>768478</td>\n",
       "      <td>baseline</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.907970</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768479</th>\n",
       "      <td>768479</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>ADE</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>0.434897</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768480</th>\n",
       "      <td>768480</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>0.824960</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768481</th>\n",
       "      <td>768481</td>\n",
       "      <td>325</td>\n",
       "      <td>O</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>0.824960</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768482</th>\n",
       "      <td>768482</td>\n",
       "      <td>m</td>\n",
       "      <td>O</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>0.824960</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768483</th>\n",
       "      <td>768483</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>0.824960</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768484</th>\n",
       "      <td>768484</td>\n",
       "      <td>super</td>\n",
       "      <td>O</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>0.824960</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768485</th>\n",
       "      <td>768485</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>0.067540</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768486</th>\n",
       "      <td>768486</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>0.071367</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768487</th>\n",
       "      <td>768487</td>\n",
       "      <td>who</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768488</th>\n",
       "      <td>768488</td>\n",
       "      <td>lacked</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768489</th>\n",
       "      <td>768489</td>\n",
       "      <td>improvement</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768490</th>\n",
       "      <td>768490</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768491</th>\n",
       "      <td>768491</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768492</th>\n",
       "      <td>768492</td>\n",
       "      <td>during</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768493</th>\n",
       "      <td>768493</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768494</th>\n",
       "      <td>768494</td>\n",
       "      <td>first</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768495</th>\n",
       "      <td>768495</td>\n",
       "      <td>12</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768496</th>\n",
       "      <td>768496</td>\n",
       "      <td>weeks</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768497</th>\n",
       "      <td>768497</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>47</td>\n",
       "      <td>106</td>\n",
       "      <td>0.907762</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768498</th>\n",
       "      <td>768498</td>\n",
       "      <td>sildenafil</td>\n",
       "      <td>Drug</td>\n",
       "      <td>107</td>\n",
       "      <td>117</td>\n",
       "      <td>0.947102</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768499</th>\n",
       "      <td>768499</td>\n",
       "      <td>treatment</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768500</th>\n",
       "      <td>768500</td>\n",
       "      <td>had</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768501</th>\n",
       "      <td>768501</td>\n",
       "      <td>poor</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768502</th>\n",
       "      <td>768502</td>\n",
       "      <td>survival</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768503</th>\n",
       "      <td>768503</td>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768504</th>\n",
       "      <td>768504</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768505</th>\n",
       "      <td>768505</td>\n",
       "      <td>35</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768506</th>\n",
       "      <td>768506</td>\n",
       "      <td>%</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768507</th>\n",
       "      <td>768507</td>\n",
       "      <td>)</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768508</th>\n",
       "      <td>768508</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>118</td>\n",
       "      <td>154</td>\n",
       "      <td>0.907993</td>\n",
       "      <td>24590</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       tokens  tags  start  end     score  seq_id  \\\n",
       "768476      768476     patients     O      0   22  0.907970   24590   \n",
       "768477      768477         with     O      0   22  0.907970   24590   \n",
       "768478      768478     baseline     O      0   22  0.907970   24590   \n",
       "768479      768479         6mwd   ADE     23   27  0.434897   24590   \n",
       "768480      768480            ,     O     28   44  0.824960   24590   \n",
       "768481      768481          325     O     28   44  0.824960   24590   \n",
       "768482      768482            m     O     28   44  0.824960   24590   \n",
       "768483      768483           in     O     28   44  0.824960   24590   \n",
       "768484      768484        super     O     28   44  0.824960   24590   \n",
       "768485      768485            -     O     44   45  0.067540   24590   \n",
       "768486      768486            1     O     45   46  0.071367   24590   \n",
       "768487      768487          who     O     47  106  0.907762   24590   \n",
       "768488      768488       lacked     O     47  106  0.907762   24590   \n",
       "768489      768489  improvement     O     47  106  0.907762   24590   \n",
       "768490      768490           in     O     47  106  0.907762   24590   \n",
       "768491      768491         6mwd     O     47  106  0.907762   24590   \n",
       "768492      768492       during     O     47  106  0.907762   24590   \n",
       "768493      768493          the     O     47  106  0.907762   24590   \n",
       "768494      768494        first     O     47  106  0.907762   24590   \n",
       "768495      768495           12     O     47  106  0.907762   24590   \n",
       "768496      768496        weeks     O     47  106  0.907762   24590   \n",
       "768497      768497           of     O     47  106  0.907762   24590   \n",
       "768498      768498   sildenafil  Drug    107  117  0.947102   24590   \n",
       "768499      768499    treatment     O    118  154  0.907993   24590   \n",
       "768500      768500          had     O    118  154  0.907993   24590   \n",
       "768501      768501         poor     O    118  154  0.907993   24590   \n",
       "768502      768502     survival     O    118  154  0.907993   24590   \n",
       "768503      768503            (     O    118  154  0.907993   24590   \n",
       "768504      768504            ,     O    118  154  0.907993   24590   \n",
       "768505      768505           35     O    118  154  0.907993   24590   \n",
       "768506      768506            %     O    118  154  0.907993   24590   \n",
       "768507      768507            )     O    118  154  0.907993   24590   \n",
       "768508      768508            .     O    118  154  0.907993   24590   \n",
       "\n",
       "                 document  \n",
       "768476  chest.10-0969.txt  \n",
       "768477  chest.10-0969.txt  \n",
       "768478  chest.10-0969.txt  \n",
       "768479  chest.10-0969.txt  \n",
       "768480  chest.10-0969.txt  \n",
       "768481  chest.10-0969.txt  \n",
       "768482  chest.10-0969.txt  \n",
       "768483  chest.10-0969.txt  \n",
       "768484  chest.10-0969.txt  \n",
       "768485  chest.10-0969.txt  \n",
       "768486  chest.10-0969.txt  \n",
       "768487  chest.10-0969.txt  \n",
       "768488  chest.10-0969.txt  \n",
       "768489  chest.10-0969.txt  \n",
       "768490  chest.10-0969.txt  \n",
       "768491  chest.10-0969.txt  \n",
       "768492  chest.10-0969.txt  \n",
       "768493  chest.10-0969.txt  \n",
       "768494  chest.10-0969.txt  \n",
       "768495  chest.10-0969.txt  \n",
       "768496  chest.10-0969.txt  \n",
       "768497  chest.10-0969.txt  \n",
       "768498  chest.10-0969.txt  \n",
       "768499  chest.10-0969.txt  \n",
       "768500  chest.10-0969.txt  \n",
       "768501  chest.10-0969.txt  \n",
       "768502  chest.10-0969.txt  \n",
       "768503  chest.10-0969.txt  \n",
       "768504  chest.10-0969.txt  \n",
       "768505  chest.10-0969.txt  \n",
       "768506  chest.10-0969.txt  \n",
       "768507  chest.10-0969.txt  \n",
       "768508  chest.10-0969.txt  "
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "denisbio[denisbio['seq_id']==24590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>bio</th>\n",
       "      <th>tag_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>713667</th>\n",
       "      <td>713667</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>Patients</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713668</th>\n",
       "      <td>713668</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713669</th>\n",
       "      <td>713669</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>baseline</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713670</th>\n",
       "      <td>713670</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>6MWD</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713671</th>\n",
       "      <td>713671</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713672</th>\n",
       "      <td>713672</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>325</td>\n",
       "      <td>B</td>\n",
       "      <td>6MWD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713673</th>\n",
       "      <td>713673</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>m</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713674</th>\n",
       "      <td>713674</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713675</th>\n",
       "      <td>713675</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>SUPER-1</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713676</th>\n",
       "      <td>713676</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>who</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713677</th>\n",
       "      <td>713677</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>lacked</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713678</th>\n",
       "      <td>713678</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>improvement</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713679</th>\n",
       "      <td>713679</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713680</th>\n",
       "      <td>713680</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>6MWD</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713681</th>\n",
       "      <td>713681</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>during</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713682</th>\n",
       "      <td>713682</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713683</th>\n",
       "      <td>713683</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>first</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713684</th>\n",
       "      <td>713684</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>12</td>\n",
       "      <td>B</td>\n",
       "      <td>duration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713685</th>\n",
       "      <td>713685</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>weeks</td>\n",
       "      <td>I</td>\n",
       "      <td>duration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713686</th>\n",
       "      <td>713686</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713687</th>\n",
       "      <td>713687</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>sildenafil</td>\n",
       "      <td>B</td>\n",
       "      <td>prev_treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713688</th>\n",
       "      <td>713688</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>treatment</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713689</th>\n",
       "      <td>713689</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>had</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713690</th>\n",
       "      <td>713690</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>poor</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713691</th>\n",
       "      <td>713691</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>survival</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713692</th>\n",
       "      <td>713692</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713693</th>\n",
       "      <td>713693</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713694</th>\n",
       "      <td>713694</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>35</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713695</th>\n",
       "      <td>713695</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>%</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713696</th>\n",
       "      <td>713696</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>)</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713697</th>\n",
       "      <td>713697</td>\n",
       "      <td>chest.10-0969.txt</td>\n",
       "      <td>24590</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0           filename  sentence_id         word bio  \\\n",
       "713667      713667  chest.10-0969.txt        24590     Patients   O   \n",
       "713668      713668  chest.10-0969.txt        24590         with   O   \n",
       "713669      713669  chest.10-0969.txt        24590     baseline   O   \n",
       "713670      713670  chest.10-0969.txt        24590         6MWD   O   \n",
       "713671      713671  chest.10-0969.txt        24590            ,   O   \n",
       "713672      713672  chest.10-0969.txt        24590          325   B   \n",
       "713673      713673  chest.10-0969.txt        24590            m   O   \n",
       "713674      713674  chest.10-0969.txt        24590           in   O   \n",
       "713675      713675  chest.10-0969.txt        24590      SUPER-1   O   \n",
       "713676      713676  chest.10-0969.txt        24590          who   O   \n",
       "713677      713677  chest.10-0969.txt        24590       lacked   O   \n",
       "713678      713678  chest.10-0969.txt        24590  improvement   O   \n",
       "713679      713679  chest.10-0969.txt        24590           in   O   \n",
       "713680      713680  chest.10-0969.txt        24590         6MWD   O   \n",
       "713681      713681  chest.10-0969.txt        24590       during   O   \n",
       "713682      713682  chest.10-0969.txt        24590          the   O   \n",
       "713683      713683  chest.10-0969.txt        24590        first   O   \n",
       "713684      713684  chest.10-0969.txt        24590           12   B   \n",
       "713685      713685  chest.10-0969.txt        24590        weeks   I   \n",
       "713686      713686  chest.10-0969.txt        24590           of   O   \n",
       "713687      713687  chest.10-0969.txt        24590   sildenafil   B   \n",
       "713688      713688  chest.10-0969.txt        24590    treatment   O   \n",
       "713689      713689  chest.10-0969.txt        24590          had   O   \n",
       "713690      713690  chest.10-0969.txt        24590         poor   O   \n",
       "713691      713691  chest.10-0969.txt        24590     survival   O   \n",
       "713692      713692  chest.10-0969.txt        24590            (   O   \n",
       "713693      713693  chest.10-0969.txt        24590            ,   O   \n",
       "713694      713694  chest.10-0969.txt        24590           35   O   \n",
       "713695      713695  chest.10-0969.txt        24590            %   O   \n",
       "713696      713696  chest.10-0969.txt        24590            )   O   \n",
       "713697      713697  chest.10-0969.txt        24590            .   O   \n",
       "\n",
       "          tag_name  \n",
       "713667         NaN  \n",
       "713668         NaN  \n",
       "713669         NaN  \n",
       "713670         NaN  \n",
       "713671         NaN  \n",
       "713672        6MWD  \n",
       "713673         NaN  \n",
       "713674         NaN  \n",
       "713675         NaN  \n",
       "713676         NaN  \n",
       "713677         NaN  \n",
       "713678         NaN  \n",
       "713679         NaN  \n",
       "713680         NaN  \n",
       "713681         NaN  \n",
       "713682         NaN  \n",
       "713683         NaN  \n",
       "713684    duration  \n",
       "713685    duration  \n",
       "713686         NaN  \n",
       "713687  prev_treat  \n",
       "713688         NaN  \n",
       "713689         NaN  \n",
       "713690         NaN  \n",
       "713691         NaN  \n",
       "713692         NaN  \n",
       "713693         NaN  \n",
       "713694         NaN  \n",
       "713695         NaN  \n",
       "713696         NaN  \n",
       "713697         NaN  "
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sashabio[sashabio['sentence_id']==24590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>score</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>document</th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>bio</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>tag_start</th>\n",
       "      <th>tag_prob</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142562</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>B</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>89.0</td>\n",
       "      <td></td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142563</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>00000539-200112000-00018.txt</td>\n",
       "      <td>34.0</td>\n",
       "      <td>m</td>\n",
       "      <td>I</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>93.0</td>\n",
       "      <td></td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142716</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166.0</td>\n",
       "      <td>00003088-200847090-00004.txt</td>\n",
       "      <td>172.0</td>\n",
       "      <td>50вЂ</td>\n",
       "      <td>B</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.9414474</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142723</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166.0</td>\n",
       "      <td>00003088-200847090-00004.txt</td>\n",
       "      <td>172.0</td>\n",
       "      <td>30вЂ</td>\n",
       "      <td>B</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>69.0</td>\n",
       "      <td></td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143045</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>538.0</td>\n",
       "      <td>000242498.txt</td>\n",
       "      <td>586.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>B</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.92332095</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203461</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57656.0</td>\n",
       "      <td>v10007-012-0027-9.txt</td>\n",
       "      <td>69281.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>B</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.5026451</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203469</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57669.0</td>\n",
       "      <td>v10007-012-0027-9.txt</td>\n",
       "      <td>69298.0</td>\n",
       "      <td>44.7</td>\n",
       "      <td>B</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>208.0</td>\n",
       "      <td></td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57669.0</td>\n",
       "      <td>v10007-012-0027-9.txt</td>\n",
       "      <td>69298.0</td>\n",
       "      <td>N</td>\n",
       "      <td>I</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>213.0</td>\n",
       "      <td></td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203494</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57697.0</td>\n",
       "      <td>v10007-012-0027-9.txt</td>\n",
       "      <td>69328.0</td>\n",
       "      <td>20</td>\n",
       "      <td>B</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.5100748</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203495</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57697.0</td>\n",
       "      <td>v10007-012-0027-9.txt</td>\n",
       "      <td>69328.0</td>\n",
       "      <td>s</td>\n",
       "      <td>I</td>\n",
       "      <td>6mwd</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.5100748</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1520 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 tokens tags  start  end  score  seq_id document       id  \\\n",
       "142562         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN     31.0   \n",
       "142563         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN     31.0   \n",
       "142716         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN    166.0   \n",
       "142723         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN    166.0   \n",
       "143045         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN    538.0   \n",
       "...            ...    ...  ...    ...  ...    ...     ...      ...      ...   \n",
       "203461         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN  57656.0   \n",
       "203469         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN  57669.0   \n",
       "203470         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN  57669.0   \n",
       "203494         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN  57697.0   \n",
       "203495         NaN    NaN  NaN    NaN  NaN    NaN     NaN      NaN  57697.0   \n",
       "\n",
       "                            filename  sentence_id  word bio tag_name  \\\n",
       "142562  00000539-200112000-00018.txt         34.0   2.1   B     6mwd   \n",
       "142563  00000539-200112000-00018.txt         34.0     m   I     6mwd   \n",
       "142716  00003088-200847090-00004.txt        172.0  50вЂ   B     6mwd   \n",
       "142723  00003088-200847090-00004.txt        172.0  30вЂ   B     6mwd   \n",
       "143045                 000242498.txt        586.0  15.0   B     6mwd   \n",
       "...                              ...          ...   ...  ..      ...   \n",
       "203461         v10007-012-0027-9.txt      69281.0  14.4   B     6mwd   \n",
       "203469         v10007-012-0027-9.txt      69298.0  44.7   B     6mwd   \n",
       "203470         v10007-012-0027-9.txt      69298.0     N   I     6mwd   \n",
       "203494         v10007-012-0027-9.txt      69328.0    20   B     6mwd   \n",
       "203495         v10007-012-0027-9.txt      69328.0     s   I     6mwd   \n",
       "\n",
       "        tag_start    tag_prob      _merge  \n",
       "142562       89.0              right_only  \n",
       "142563       93.0              right_only  \n",
       "142716       20.0   0.9414474  right_only  \n",
       "142723       69.0              right_only  \n",
       "143045      113.0  0.92332095  right_only  \n",
       "...           ...         ...         ...  \n",
       "203461       69.0   0.5026451  right_only  \n",
       "203469      208.0              right_only  \n",
       "203470      213.0              right_only  \n",
       "203494      155.0   0.5100748  right_only  \n",
       "203495      158.0   0.5100748  right_only  \n",
       "\n",
       "[1520 rows x 17 columns]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfres[(dfres['_merge']=='right_only')&(dfres['tag_name']=='6mwd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>score</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>document</th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>bio</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>tag_start</th>\n",
       "      <th>tag_prob</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>5047.0</td>\n",
       "      <td>“</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.784557</td>\n",
       "      <td>172.0</td>\n",
       "      <td>00003088-200847090-00004.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>5048.0</td>\n",
       "      <td>450</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.784557</td>\n",
       "      <td>172.0</td>\n",
       "      <td>00003088-200847090-00004.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>5049.0</td>\n",
       "      <td>m</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.784557</td>\n",
       "      <td>172.0</td>\n",
       "      <td>00003088-200847090-00004.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>5064.0</td>\n",
       "      <td>m</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>70.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.956073</td>\n",
       "      <td>172.0</td>\n",
       "      <td>00003088-200847090-00004.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>6460.0</td>\n",
       "      <td>+</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>81.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.958149</td>\n",
       "      <td>213.0</td>\n",
       "      <td>00003088-200847090-00004.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139833</th>\n",
       "      <td>2060002.0</td>\n",
       "      <td>40</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>127.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.953244</td>\n",
       "      <td>68813.0</td>\n",
       "      <td>thx.2005.041954.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139835</th>\n",
       "      <td>2060050.0</td>\n",
       "      <td>43</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>120.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.956802</td>\n",
       "      <td>68814.0</td>\n",
       "      <td>thx.2005.041954.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139836</th>\n",
       "      <td>2060053.0</td>\n",
       "      <td>m</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>125.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.947535</td>\n",
       "      <td>68814.0</td>\n",
       "      <td>thx.2005.041954.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140114</th>\n",
       "      <td>2063685.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>58.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.814918</td>\n",
       "      <td>69073.0</td>\n",
       "      <td>v058p00797.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140115</th>\n",
       "      <td>2063686.0</td>\n",
       "      <td>mm</td>\n",
       "      <td>6mwd_rate</td>\n",
       "      <td>58.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.814918</td>\n",
       "      <td>69073.0</td>\n",
       "      <td>v058p00797.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4768 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 tokens       tags  start    end     score   seq_id  \\\n",
       "506         5047.0      “  6mwd_rate   24.0   30.0  0.784557    172.0   \n",
       "507         5048.0    450  6mwd_rate   24.0   30.0  0.784557    172.0   \n",
       "508         5049.0      m  6mwd_rate   24.0   30.0  0.784557    172.0   \n",
       "509         5064.0      m  6mwd_rate   70.0   75.0  0.956073    172.0   \n",
       "677         6460.0      +  6mwd_rate   81.0   86.0  0.958149    213.0   \n",
       "...            ...    ...        ...    ...    ...       ...      ...   \n",
       "139833   2060002.0     40  6mwd_rate  127.0  129.0  0.953244  68813.0   \n",
       "139835   2060050.0     43  6mwd_rate  120.0  122.0  0.956802  68814.0   \n",
       "139836   2060053.0      m  6mwd_rate  125.0  126.0  0.947535  68814.0   \n",
       "140114   2063685.0      8  6mwd_rate   58.0   62.0  0.814918  69073.0   \n",
       "140115   2063686.0     mm  6mwd_rate   58.0   62.0  0.814918  69073.0   \n",
       "\n",
       "                            document  id filename  sentence_id word  bio  \\\n",
       "506     00003088-200847090-00004.txt NaN      NaN          NaN  NaN  NaN   \n",
       "507     00003088-200847090-00004.txt NaN      NaN          NaN  NaN  NaN   \n",
       "508     00003088-200847090-00004.txt NaN      NaN          NaN  NaN  NaN   \n",
       "509     00003088-200847090-00004.txt NaN      NaN          NaN  NaN  NaN   \n",
       "677     00003088-200847090-00004.txt NaN      NaN          NaN  NaN  NaN   \n",
       "...                              ...  ..      ...          ...  ...  ...   \n",
       "139833           thx.2005.041954.txt NaN      NaN          NaN  NaN  NaN   \n",
       "139835           thx.2005.041954.txt NaN      NaN          NaN  NaN  NaN   \n",
       "139836           thx.2005.041954.txt NaN      NaN          NaN  NaN  NaN   \n",
       "140114                v058p00797.txt NaN      NaN          NaN  NaN  NaN   \n",
       "140115                v058p00797.txt NaN      NaN          NaN  NaN  NaN   \n",
       "\n",
       "       tag_name  tag_start tag_prob     _merge  \n",
       "506         NaN        NaN      NaN  left_only  \n",
       "507         NaN        NaN      NaN  left_only  \n",
       "508         NaN        NaN      NaN  left_only  \n",
       "509         NaN        NaN      NaN  left_only  \n",
       "677         NaN        NaN      NaN  left_only  \n",
       "...         ...        ...      ...        ...  \n",
       "139833      NaN        NaN      NaN  left_only  \n",
       "139835      NaN        NaN      NaN  left_only  \n",
       "139836      NaN        NaN      NaN  left_only  \n",
       "140114      NaN        NaN      NaN  left_only  \n",
       "140115      NaN        NaN      NaN  left_only  \n",
       "\n",
       "[4768 rows x 17 columns]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfres[(dfres['_merge']=='left_only')&(dfres['tags']=='6mwd_rate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'reason', 'start': 0, 'end': 1},\n",
       " {'type': 'reason', 'start': 8, 'end': 9},\n",
       " {'type': '6mwd', 'start': 10, 'end': 11},\n",
       " {'type': '6mwd', 'start': 10, 'end': 12},\n",
       " {'type': '6mwd', 'start': 11, 'end': 12},\n",
       " {'type': '6mwd', 'start': 16, 'end': 17},\n",
       " {'type': '6mwd', 'start': 16, 'end': 18},\n",
       " {'type': '6mwd', 'start': 17, 'end': 18},\n",
       " {'type': 'reason', 'start': 27, 'end': 28},\n",
       " {'type': '6mwd', 'start': 29, 'end': 30},\n",
       " {'type': '6mwd', 'start': 29, 'end': 31},\n",
       " {'type': '6mwd', 'start': 30, 'end': 31},\n",
       " {'type': '6mwd', 'start': 35, 'end': 36},\n",
       " {'type': '6mwd', 'start': 35, 'end': 37},\n",
       " {'type': '6mwd', 'start': 36, 'end': 37}]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[i]['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "#корректировка на номер\n",
    "for j in jsonlist:\n",
    "    for e in j['entities']:\n",
    "        if e['start'] == e['end']:\n",
    "            e['end']+=1\n",
    "#текстовая корректировка          \n",
    "for j in jsonlist:\n",
    "    for i in range(len(j['tokens'])):\n",
    "        j['tokens'][i] = j['tokens'][i].replace('\"','')\n",
    "\n",
    "#корректировка на длину строки\n",
    "for j in jsonlist:\n",
    "    s = j['tokens']\n",
    "    m = len(s)\n",
    "    for e in j['entities']:\n",
    "        if e['start']> m or e['end']> m:\n",
    "            m = e['end']\n",
    "    if m>len(s):\n",
    "        for e in j['entities']:\n",
    "            if e['start']> len(s) or e['end']> len(s):\n",
    "                e['start'] = e['start'] - (m-len(s))\n",
    "                e['end'] = e['end'] - (m-len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/aelitta/Documents/DMMatrix/PAH/CRE/spert/articles_inf.json', 'w') as f:\n",
    "    f.write(json.dumps(jsonlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_annotation=[]\n",
    "with open('Downloads/zlj1236.tsv') as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        # print(line)\n",
    "        if len(line)<5:\n",
    "            continue\n",
    "        if len(line[2])>0:\n",
    "            bio_annotation.append(line[2] + '\\t' + 'O')\n",
    "            \n",
    "f=open('test.tsv','w')\n",
    "cnt = 0\n",
    "for ele in bio_annotation:\n",
    "#     print(ele)\n",
    "    if cnt<=50:\n",
    "        f.write(ele+'\\n')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        cnt = 0\n",
    "        f.write(ele+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Cardiopulmonary',\n",
       "  'exercise',\n",
       "  'testing',\n",
       "  'was',\n",
       "  'performed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'time',\n",
       "  'points',\n",
       "  'baseline',\n",
       "  ',',\n",
       "  'inhaled',\n",
       "  'iloprost',\n",
       "  'and',\n",
       "  'chronic',\n",
       "  'intravenous',\n",
       "  'iloprost',\n",
       "  '.'],\n",
       " 'entities': [{'type': 'route', 'start': 13, 'end': 14},\n",
       "  {'type': 'drug', 'start': 14, 'end': 16},\n",
       "  {'type': 'drug', 'start': 16, 'end': 18},\n",
       "  {'type': 'route', 'start': 15, 'end': 16}],\n",
       " 'relations': [{'type': 'NotRel', 'head': 0, 'tail': 1},\n",
       "  {'type': 'NotRel', 'head': 0, 'tail': 2},\n",
       "  {'type': 'NotRel', 'head': 1, 'tail': 3},\n",
       "  {'type': 'NotRel', 'head': 3, 'tail': 2}],\n",
       " 'orig_id': 8917}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonlist[4050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intravenous']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonlist[4050]['tokens'][15:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Downloads/ner_outputs/NER_result_conll-2.txt\",) as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        if len(line)>0:\n",
    "            if line[0].split(' ')[1]=='B-MISC' or line[0].split(' ')[1]=='I-MISC':\n",
    "                print(line[0].split(' ')[0], line[0].split(' ')[1], line[0].split(' ')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups_oak, and subgroup1 (with date), subgroup2 (no date)\n",
    "\n",
    "import csv\n",
    "txt = []\n",
    "tst = []\n",
    "apply = []\n",
    "txt_groups = []\n",
    "tst_groups = []\n",
    "txt_subgroups = []\n",
    "tst_subgroups = []\n",
    "txt_subgroups1 = []\n",
    "tst_subgroups1 = []\n",
    "set_of_ann = []\n",
    "n = 0\n",
    "ng = 0\n",
    "nsg = 0\n",
    "with open(\"Documents/Myeloma/mm_kazan.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        word = line[2:3]\n",
    "        #print(\"word\", word)\n",
    "        ann = line[4:5]\n",
    "        #print(\"ann\", ann)\n",
    "        ann2 = line[8:9]\n",
    "        #print(\"ann2\", ann2)\n",
    "#        print(line)\n",
    "        if len(word) == 0 or len(ann) == 0 or word[0][:1] == '\\t' or len(word[0]+ ' '+'O')==2 :\n",
    "            continue\n",
    "        if word[0].lower() == 'эпикриз':\n",
    "            n+=1\n",
    "            ng=0\n",
    "            nsg=0\n",
    "        if len(word[0]+ ' '+'O')==2:\n",
    "            print(2, word[0])\n",
    "        if n<6:\n",
    "            set_of_ann.append(ann2[0])\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                txt.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0]):\n",
    "                txt.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            if ann2[0].split('[')[0] == 'surgery\\_date':\n",
    "                if ng==0:\n",
    "                    txt_groups.append(word[0] + ' ' + 'B-surgery_date')\n",
    "                    ng+=1\n",
    "                else:\n",
    "                    txt_groups.append(word[0] + ' ' + 'I-surgery_date')\n",
    "            else:\n",
    "                txt_groups.append(word[0] + ' ' + 'O')\n",
    "                ng=0\n",
    "            if len(ann2[0].split('|')) == 2:\n",
    "                    if ann2[0].split('|')[0].split('[')[0] == 'subgroup1':\n",
    "                        if nsg==0:\n",
    "                            txt_subgroups1.append(word[0] + ' ' + 'B-subgroup1')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            txt_subgroups1.append(word[0] + ' ' + 'I-subgroup1')\n",
    "                    if ann2[0].split('|')[1].split('[')[0] == 'subgroup2':\n",
    "                        if nsg==0:\n",
    "                            txt_subgroups.append(word[0] + ' ' + 'B-subgroup2')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            txt_subgroups.append(word[0] + ' ' + 'I-subgroup2')\n",
    "            else:\n",
    "                txt_subgroups.append(word[0] + ' ' + 'O')\n",
    "                nsg=0\n",
    "            \n",
    "        if n==6:\n",
    "            if ann[0] == '_' and len(word[0])>0:\n",
    "                tst.append(word[0] + ' ' + 'O')\n",
    "            elif (ann[0][0] == 'B' or ann[0][0] == 'I') and len(word[0])>0:\n",
    "                tst.append(word[0] + ' ' + ann[0].split('[')[0])\n",
    "            if ann2[0].split('[')[0] == 'group\\_oak' and len(word[0])>0:\n",
    "                if ng==0:\n",
    "                    tst_groups.append(word[0] + ' ' + 'B-group_oak')\n",
    "                    ng+=1\n",
    "                else:\n",
    "                    tst_groups.append(word[0] + ' ' + 'I-group_oak')\n",
    "            else:\n",
    "                tst_groups.append(word[0] + ' ' + 'O')\n",
    "            if len(ann2[0].split('|')) == 2 and len(word[0])>0:\n",
    "                    if ann2[0].split('|')[0].split('[')[0] == 'subgroup1':\n",
    "                        if nsg==0:\n",
    "                            tst_subgroups1.append(word[0] + ' ' + 'B-subgroup1')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            tst_subgroups1.append(word[0] + ' ' + 'I-subgroup1')\n",
    "                    if ann2[0].split('|')[1].split('[')[0] == 'subgroup2':\n",
    "                        if nsg==0:\n",
    "                            tst_subgroups.append(word[0] + ' ' + 'B-subgroup2')\n",
    "                            nsg+=1\n",
    "                        else:\n",
    "                            tst_subgroups.append(word[0] + ' ' + 'I-subgroup2')\n",
    "            elif len(word[0])>0:\n",
    "                    tst_subgroups.append(word[0] + ' ' + 'O')\n",
    "                    txt_subgroups1.append(word[0] + ' ' + 'O')\n",
    "        else:\n",
    "            apply.append(word[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chisl = pd.read_excel('Численность больше 1000 ФНС.xlsx')\n",
    "inns = chisl['inn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl --location --request POST 'https://api.bo.nalog.ru/oauth/token' \\\n",
    "--header 'Authorization: Basic YXBpOjEyMzQ1Njc4OTA='  --form 'username=dharitonov@vtb.ru' --form 'password=DKABfns20@!' --form 'grant_type=password' -k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer = '275f9ec4-d6ba-4454-ae08-03b5ba9a88b2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inn1 in inns:\n",
    "    res = !curl --location --request GET 'https://api.bo.nalog.ru/api/v1/files/?period=2020' \\\n",
    "    --header 'Authorization: Bearer '\"$bearer\"'' \\\n",
    "    --form 'inn = '\"$inn1\"'' -k \n",
    "    try:\n",
    "        token = res[5].split('token')[1].split('}')[0].replace('\"','').replace(':','')\n",
    "    except:\n",
    "        try:\n",
    "            token = res[6].split('token')[1].split('}')[0].replace('\"','').replace(':','')\n",
    "        except:\n",
    "            try:\n",
    "                token = res[7].split('token')[1].split('}')[0].replace('\"','').replace(':','')\n",
    "            except:\n",
    "                continue\n",
    "    !curl -o '/Users/aelitta/Downloads/REST_API/'\"$token\"''  --location --request GET 'https://api.bo.nalog.ru/api/v1/files/'\"$token\"'' --header 'Authorization: Bearer '\"$bearer\"'' -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = res[7].split('token')[1].split('}')[0].replace('\"','').replace(':','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o '/Users/aelitta/Downloads/REST_API/'+token  --location --request GET 'https://api.bo.nalog.ru/api/v1/files/' --header 'Authorization: Bearer '\"$bearer\"'' -k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o '/Users/aelitta/Downloads/REST_API/'\"$token\"''  --location --request GET 'https://api.bo.nalog.ru/api/v1/files/'\"$token\"'' --header 'Authorization: Bearer '\"$bearer\"'' -k\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
